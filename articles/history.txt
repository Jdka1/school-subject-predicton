
Those who cannot remember the past are condemned to repeat it.[1]
—George Santayana
History (from Ancient Greek: ἱστορία, romanized: historíā, lit. 'inquiry; knowledge acquired by investigation')[2] is the study and the documentation of the past.[3][4] Events before the invention of writing systems are considered prehistory. "History" is an umbrella term comprising past events as well as the memory, discovery, collection, organization, presentation, and interpretation of these events. Historians seek knowledge of the past using historical sources such as written documents, oral accounts, art and material artifacts, and ecological markers.[5]
History is also an academic discipline which uses narrative to describe, examine, question, and analyze past events, and investigate their patterns of cause and effect.[6][7] Historians often debate which narrative best explains an event, as well as the significance of different causes and effects. Historians also debate the nature of history as an end in itself, as well as its usefulness to give perspective on the problems of the present.[6][8][9][10]
Stories common to a particular culture, but not supported by external sources (such as the tales surrounding King Arthur), are usually classified as cultural heritage or legends.[11][12] History differs from myth in that it is supported by verifiable evidence. However, ancient cultural influences have helped spawn variant interpretations of the nature of history which have evolved over the centuries and continue to change today. The modern study of history is wide-ranging, and includes the study of specific regions and the study of certain topical or thematic elements of historical investigation. History is often taught as a part of primary and secondary education, and the academic study of history is a major discipline in university studies.
Herodotus, a 5th-century BC Greek historian, is often considered the "father of history" in the Western tradition,[13] although he has also been criticized as the "father of lies".[14][15] Along with his contemporary Thucydides, he helped form the foundations for the modern study of past events and societies.[16] Their works continue to be read today, and the gap between the culture-focused Herodotus and the military-focused Thucydides remains a point of contention or approach in modern historical writing. In East Asia, a state chronicle, the Spring and Autumn Annals, was reputed to date from as early as 722 BC, although only 2nd-century BC texts have survived.
The word history comes from historía (Ancient Greek: ἱστορία, romanized: historíā, lit. 'inquiry, knowledge from inquiry, or judge'[17]). It was in that sense that Aristotle used the word in his History of Animals.[18] The ancestor word ἵστωρ is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boeotic inscriptions (in a legal sense, either "judge" or "witness", or similar). The Greek word was borrowed into Classical Latin as historia, meaning "investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative". History was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as stær ("history, narrative, story"), but this word fell out of use in the late Old English period.[19] Meanwhile, as Latin became Old French (and Anglo-Norman), historia developed into forms such as istorie, estoire, and historie, with new developments in the meaning: "account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)".[19]
It was from Anglo-Norman that history was borrowed into Middle English, and this time the loan stuck. It appears in the 13th-century Ancrene Wisse, but seems to have become a common word in the late 14th century, with an early attestation appearing in John Gower's Confessio Amantis of the 1390s (VI.1383): "I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire". In Middle English, the meaning of history was "story" in general. The restriction to the meaning "the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs" arose in the mid-15th century.[19] With the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late 16th century, when he wrote about natural history. For him, historia was "the knowledge of objects determined by space and time", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).[20]
In an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both "history" and "story". Historian in the sense of a "researcher of history" is attested from 1531. In all European languages, the substantive history is still used to mean both "what happened with men", and "the scholarly study of the happened", the latter sense sometimes distinguished with a capital letter, or the word historiography.[18] The adjective historical is attested from 1661, and historic from 1669.[21]
Historians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, "All history is contemporary history". History is facilitated by the formation of a "true discourse of past" through the production of narrative and analysis of past events relating to the human race.[22] The modern discipline of history is dedicated to the institutional production of this discourse.
All events that are remembered and preserved in some authentic form constitute the historical record.[23] The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the "true past"). Part of the historian's role is to skillfully and objectively utilize the vast amount of sources from the past, most often found in the archives. The process of creating a narrative inevitably generates a silence as historians remember or emphasize different events of the past.[24][clarification needed]
The study of history has sometimes been classified as part of the humanities and at other times as part of the social sciences.[25] It can also be seen as a bridge between those two broad areas, incorporating methodologies from both. Some individual historians strongly support one or the other classification.[26] In the 20th century the Annales school revolutionized the study of history, by using such outside disciplines as economics, sociology, and geography in the study of global history.[27]
Traditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and have attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have also used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three.[28] But writing is the marker that separates history from what comes before.
Archeology is especially helpful in unearthing buried sites and objects, which contribute to the study of history. Archeological finds rarely stand alone, with narrative sources complementing its discoveries. Archeology's methodologies and approaches are independent from the field of history. "Historical archaeology" is a specific branch of archeology which often contrasts its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, US, has sought to understand the contradiction between textual documents idealizing "liberty" and the material record, demonstrating the possession of slaves and the inequalities of wealth made apparent by the study of the total historical environment.
There are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant intersections are often present. It is possible for historians to concern themselves with both the very specific and the very general, although the modern trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but also may be studied out of simple intellectual curiosity.[29]
The history of the world is the memory of the past experience of Homo sapiens sapiens around the world, as that experience has been preserved, largely in written records. By "prehistory", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of Sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world.[30] In 1961, British historian E. H. Carr wrote:
The line of demarcation between prehistoric and historical times is crossed when people cease to live only in the present, and become consciously interested both in their past and in their future. History begins with the handing down of tradition; and tradition means the carrying of the habits and lessons of the past into the future. Records of the past begin to be kept for the benefit of future generations.[31]This definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization.
Historiography has a number of related meanings.[32] Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative toward long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, "medieval historiography during the 1960s" means "Works of medieval history written during the 1960s").[32] Thirdly, it may refer to why history is produced: the philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Professional historians also debate the question of whether history can be taught as a single coherent narrative or a series of competing narratives.[33][34]
The following questions are used by historians in modern work.
The first four are known as historical criticism; the fifth, textual criticism; and, together, external criticism. The sixth and final inquiry about a source is called internal criticism.
The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history.
Herodotus of Halicarnassus (484 BC–c. 425 BC)[35] has generally been acclaimed as the "father of history". However, his contemporary Thucydides (c. 460 BC–c. 400 BC) is credited with having first approached history with a well-developed historical method in his work the History of the Peloponnesian War. Thucydides, unlike Herodotus, regarded history as being the product of the choices and actions of human beings, and looked at cause and effect, rather than as the result of divine intervention (though Herodotus was not wholly committed to this idea himself).[35] In his historical method, Thucydides emphasized chronology, a nominally neutral point of view, and that the human world was the result of the actions of human beings. Greek historians also viewed history as cyclical, with events regularly recurring.[36]
There were historical traditions and sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by the Han dynasty court historian known as Sima Qian (145–90 BC), author of the Records of the Grand Historian (Shiji). For the quality of his written work, Sima Qian is posthumously known as the Father of Chinese historiography. Chinese historians of subsequent dynastic periods in China used his Shiji as the official format for historical texts, as well as for biographical literature.[citation needed]
Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.[29]
In the preface to his book, the Muqaddimah (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of seven mistakes that he thought that historians regularly committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and lastly, to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun often criticized "idle superstition and uncritical acceptance of historical data". As a result, he introduced a scientific method to the study of history, and he often referred to it as his "new science".[37] His historical method also laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history,[38] and he is thus considered to be the "father of historiography"[39][40] or the "father of the philosophy of history".[41]
In the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. In 1851, Herbert Spencer summarized these methods:

From the successive strata of our historical deposits, they [Historians] diligently gather all the highly colored fragments, pounce upon everything that is curious and sparkling and chuckle like children over their glittering acquisitions; meanwhile the rich veins of wisdom that ramify amidst this worthless debris, lie utterly neglected. Cumbrous volumes of rubbish are greedily accumulated, while those masses of rich ore, that should have been dug out, and from which golden truths might have been smelted, are left untaught and unsought[42]By the "rich ore" Spencer meant scientific theory of history. Meanwhile, Henry Thomas Buckle expressed a dream of history becoming one day science:

In regard to nature, events apparently the most irregular and capricious have been explained and have been shown to be in accordance with certain fixed and universal laws. This have been done because men of ability and, above all, men of patient, untiring thought have studied events with the view of discovering their regularity, and if human events were subject to a similar treatment, we have every right to expect similar results[43]Contrary to Buckle's dream, the 19th-century historian with greatest influence on methods became Leopold von Ranke in Germany. He limited history to "what really happened" and by this directed the field further away from science. For Ranke, historical data should be collected carefully, examined objectively and put together with critical rigor. But these procedures "are merely the prerequisites and preliminaries of science. The heart of science is searching out order and regularity in the data being examined and in formulating generalizations or laws about them."[44]

As Historians like Ranke and many who followed him have pursued it, no, history is not a science. Thus if Historians tell us that, given the manner in which he practices his craft, it cannot be considered a science, we must take him at his word. If he is not doing science, then, whatever else he is doing, he is not doing science. The traditional Historian is thus no scientist and history, as conventionally practiced, is not a science.[45]In the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was a tendency to treat history more as a social science rather than as an art, which traditionally had been the case. Some of the leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre, and Lawrence Stone. Many of the advocates of history as a social science were or are noted for their multidisciplinary approach. Braudel combined history with geography, Bracher history with political science, Fogel history with economics, Gay history with psychology, Trigger history with archeology, while Wehler, Bloch, Fischer, Stone, Febvre, and Le Roy Ladurie have in varying and differing ways amalgamated history with sociology, geography, anthropology, and economics. Nevertheless, these multidisciplinary approaches failed to produce a theory of history. So far only one theory of history came from the pen of a professional Historian.[46] Whatever other theories of history we have, they were written by experts from other fields (for example, Marxian theory of history). More recently, the field of digital history has begun to address ways of using computer technology to pose new questions to historical data and generate digital scholarship.
In sincere opposition to the claims of history as a social science, historians such as Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb, and Gerhard Ritter argued that the key to the historians' work was the power of the imagination, and hence contended that history should be understood as an art. French historians associated with the Annales School introduced quantitative history, using raw data to track the lives of typical individuals, and were prominent in the establishment of cultural history (cf. histoire des mentalités). Intellectual historians such as Herbert Butterfield, Ernst Nolte and George Mosse have argued for the significance of ideas in history. American historians, motivated by the civil rights era, focused on formerly overlooked ethnic, racial, and socioeconomic groups. Another genre of social history to emerge in the post-WWII era was Alltagsgeschichte (History of Everyday Life). Scholars such as Martin Broszat, Ian Kershaw and Detlev Peukert sought to examine what everyday life was like for ordinary people in 20th-century Germany, especially in the Nazi period.
Marxist historians such as Eric Hobsbawm, E. P. Thompson, Rodney Hilton, Georges Lefebvre, Eugene Genovese, Isaac Deutscher, C. L. R. James, Timothy Mason, Herbert Aptheker, Arno J. Mayer, and Christopher Hill have sought to validate Karl Marx's theories by analyzing history from a Marxist perspective. In response to the Marxist interpretation of history, historians such as François Furet, Richard Pipes, J. C. D. Clark, Roland Mousnier, Henry Ashby Turner, and Robert Conquest have offered anti-Marxist interpretations of history. Feminist historians such as Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese, and Lynn Hunt have argued for the importance of studying the experience of women in the past. In recent years, postmodernists have challenged the validity and need for the study of history on the basis that all history is based on the personal interpretation of sources. In his 1997 book In Defence of History, Richard J. Evans defended the worth of history. Another defense of history from postmodernist criticism was the Australian historian Keith Windschuttle's 1994 book, The Killing of History.
Today, most historians begin their research process in the archives, on either a physical or digital platform. They often propose an argument and use their research to support it. John H. Arnold proposed that history is an argument, which creates the possibility of creating change.[5] Digital information companies, such as Google, have sparked controversy over the role of internet censorship in information access.[47]
The Marxist theory of historical materialism theorises that society is fundamentally determined by the material conditions at any given time – in other words, the relationships which people have with each other in order to fulfill basic needs such as feeding, clothing and housing themselves and their families.[48] Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe.[49] Marxist historiography was once orthodoxy in the Soviet Union, but since the collapse of communism there in 1991, Mikhail Krom says it has been reduced to the margins of scholarship.[50]
Many historians believe that the production of history is embedded with bias because events and known facts in history can be interpreted in a variety of ways. Constantin Fasolt suggested that history is linked to politics by the practice of silence itself.[51] He also said: "A second common view of the link between history and politics rests on the elementary observation that historians are often influenced by politics."[51] According to Michel-Rolph Trouillot, the historical process is rooted in the archives, therefore silences, or parts of history that are forgotten, may be an intentional part of a narrative strategy that dictates how areas of history are remembered.[24] Historical omissions can occur in many ways and can have a profound effect on historical records. Information can also purposely be excluded or left out accidentally. Historians have coined multiple terms that describe the act of omitting historical information, including: "silencing",[24] "selective memory",[52] and erasures.[53] Gerda Lerner, a twentieth century historian who focused much of her work on historical omissions involving women and their accomplishments, explained the negative impact that these omissions had on minority groups.[52]
Environmental historian William Cronon proposed three ways to combat bias and ensure authentic and accurate narratives: narratives must not contradict known fact, they must make ecological sense (specifically for environmental history), and published work must be reviewed by scholarly community and other historians to ensure accountability.[53]
These are approaches to history; not listed are histories of other fields, such as history of science, history of mathematics and history of philosophy.
Historical study often focuses on events and developments that occur in particular blocks of time. Historians give these periods of time names in order to allow "organising ideas and classificatory generalisations" to be used by historians.[54] The names given to a period can vary with geographical location, as can the dates of the beginning and end of a particular period. Centuries and decades are commonly used periods and the time they represent depends on the dating system used. Most periods are constructed retrospectively and so reflect value judgments made about the past. The way periods are constructed and the names given to them can affect the way they are viewed and studied.[55]
The field of history generally leaves prehistory to archeologists, who have entirely different sets of tools and theories. In archeology, the usual method for periodization of the distant prehistoric past is to rely on changes in material culture and technology, such as the Stone Age, Bronze Age, and Iron Age, with subdivisions that are also based on different styles of material remains. Here prehistory is divided into a series of "chapters" so that periods in history could unfold not only in a relative chronology but also narrative chronology.[56] This narrative content could be in the form of functional-economic interpretation. There are periodizations, however, that do not have this narrative aspect, relying largely on relative chronology, and that are thus devoid of any specific meaning.
Despite the development over recent decades of the ability through radiocarbon dating and other scientific methods to give actual dates for many sites or artefacts, these long-established schemes seem likely to remain in use. In many cases neighboring cultures with writing have left some history of cultures without it, which may be used. Periodization, however, is not viewed as a perfect framework, with one account explaining that "cultural changes do not conveniently start and stop (combinedly) at periodization boundaries" and that different trajectories of change need to be studied in their own right before they get intertwined with cultural phenomena.[57]
Particular geographical locations can form the basis of historical study, for example, continents, countries, and cities. Understanding why historic events took place is important. To do this, historians often turn to geography. According to Jules Michelet in his book Histoire de France (1833), "without geographical basis, the people, the makers of history, seem to be walking on air".[58] Weather patterns, the water supply, and the landscape of a place all affect the lives of the people who live there. For example, to explain why the ancient Egyptians developed a successful civilization, studying the geography of Egypt is essential. Egyptian civilization was built on the banks of the Nile River, which flooded each year, depositing soil on its banks. The rich soil could help farmers grow enough crops to feed the people in the cities. That meant everyone did not have to farm, so some people could perform other jobs that helped develop the civilization. There is also the case of climate, which historians like Ellsworth Huntington and Ellen Churchill Semple cited as a crucial influence on the course of history. Huntington and Semple further argued that climate has an impact on racial temperament.[59]
Military history concerns warfare, strategies, battles, weapons, and the psychology of combat.[60] The "new military history" since the 1970s has been concerned with soldiers more than generals, with psychology more than tactics, and with the broader impact of warfare on society and culture.[61]
The history of religion has been a main theme for both secular and religious historians for centuries, and continues to be taught in seminaries and academe. Leading journals include Church History, The Catholic Historical Review, and History of Religions. Topics range widely from political and cultural and artistic dimensions, to theology and liturgy.[62] This subject studies religions from all regions and areas of the world where humans have lived.[63]
Social history, sometimes called the new social history, is the field that includes history of ordinary people and their strategies and institutions for coping with life.[64] In its "golden age" it was a major growth field in the 1960s and 1970s among scholars, and still is well represented in history departments. In two decades from 1975 to 1995, the proportion of professors of history in American universities identifying with social history rose from 31% to 41%, while the proportion of political historians fell from 40% to 30%.[65] In the history departments of British universities in 2007, of the 5723 faculty members, 1644 (29%) identified themselves with social history while political history came next with 1425 (25%).[66] The "old" social history before the 1960s was a hodgepodge of topics without a central theme, and it often included political movements, like Populism, that were "social" in the sense of being outside the elite system. Social history was contrasted with political history, intellectual history and the history of great men. English historian G. M. Trevelyan saw it as the bridging point between economic and political history, reflecting that, "Without social history, economic history is barren and political history unintelligible."[67] While the field has often been viewed negatively as history with the politics left out, it has also been defended as "history with the people put back in".[68]
The chief subfields of social history include:
Cultural history replaced social history as the dominant form in the 1980s and 1990s. It typically combines the approaches of anthropology and history to look at language, popular cultural traditions and cultural interpretations of historical experience. It examines the records and narrative descriptions of past knowledge, customs, and arts of a group of people. How peoples constructed their memory of the past is a major topic. Cultural history includes the study of art in society as well is the study of images and human visual production (iconography).[69]
Diplomatic history focuses on the relationships between nations, primarily regarding diplomacy and the causes of wars.[70] More recently it looks at the causes of peace and human rights. It typically presents the viewpoints of the foreign office, and long-term strategic values, as the driving force of continuity and change in history. This type of political history is the study of the conduct of international relations between states or across state boundaries over time. Historian Muriel Chamberlain notes that after the First World War, "diplomatic history replaced constitutional history as the flagship of historical investigation, at once the most important, most exact and most sophisticated of historical studies".[71] She adds that after 1945, the trend reversed, allowing social history to replace it.
Although economic history has been well established since the late 19th century, in recent years academic studies have shifted more and more toward economics departments and away from traditional history departments.[72] Business history deals with the history of individual business organizations, business methods, government regulation, labour relations, and impact on society. It also includes biographies of individual companies, executives, and entrepreneurs. It is related to economic history. Business history is most often taught in business schools.[73]
Environmental history is a new field that emerged in the 1980s to look at the history of the environment, especially in the long run, and the impact of human activities upon it.[74] It is an offshoot of the environmental movement, which was kickstarted by Rachel Carson's Silent Spring in the 1960s.
World history is the study of major civilizations over the last 3000 years or so. World history is primarily a teaching field, rather than a research field. It gained popularity in the United States,[75] Japan[76] and other countries after the 1980s with the realization that students need a broader exposure to the world as globalization proceeds.
It has led to highly controversial interpretations by Oswald Spengler and Arnold J. Toynbee, among others.
The World History Association publishes the Journal of World History every quarter since 1990.[77] The H-World discussion list[78] serves as a network of communication among practitioners of world history, with discussions among scholars, announcements, syllabi, bibliographies and book reviews.
A people's history is a type of historical work which attempts to account for historical events from the perspective of common people. A people's history is the history of the world that is the story of mass movements and of the outsiders. Individuals or groups not included in the past in other types of writing about history are the primary focus, which includes the disenfranchised, the oppressed, the poor, the nonconformists, and the otherwise forgotten people. The authors are typically on the left and have a socialist model in mind, as in the approach of the History Workshop movement in Britain in the 1960s.[79]
Intellectual history and the history of ideas emerged in the mid-20th century, with the focus on the intellectuals and their books on the one hand, and on the other the study of ideas as disembodied objects with a career of their own.[80][81]
Gender history is a subfield of History and Gender studies, which looks at the past from the perspective of gender. The outgrowth of gender history from women's history stemmed from many non-feminist historians dismissing the importance of women in history. According to Joan W. Scott, "Gender is a constitutive element of social relationships based on perceived differences between the sexes, and gender is a primary way of signifying relations of power",[82] meaning that gender historians study the social effects of perceived differences between the sexes and how all genders utilize allotted power in societal and political structures. Despite being a relatively new field, gender history has had a significant effect on the general study of history. Gender history traditionally differs from women's history in its inclusion of all aspects of gender such as masculinity and femininity, and today's gender history extends to include people who identify outside of that binary. 
LGBT history deals with the first recorded instances of same-sex love and sexuality of ancient civilizations, and involves the history of lesbian, gay, bisexual and transgender (LGBT) peoples and cultures around the world.[83]
Public history describes the broad range of activities undertaken by people with some training in the discipline of history who are generally working outside of specialized academic settings. Public history practice has quite deep roots in the areas of historic preservation, archival science, oral history, museum curatorship, and other related fields. The term itself began to be used in the U.S. and Canada in the late 1970s, and the field has become increasingly professionalized since that time. Some of the most common settings for public history are museums, historic homes and historic sites, parks, battlefields, archives, film and television companies, and all levels of government.[84]
Professional and amateur historians discover, collect, organize, and present information about past events. They discover this information through archeological evidence, written primary sources, verbal stories or oral histories, and other archival material. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included.
Since the 20th century, Western historians have disavowed the aspiration to provide the "judgement of history".[85] The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final.[86] A related issue to that of the judgement of history is that of collective memory.
Pseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions. It is closely related to deceptive historical revisionism. Works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.
A major intellectual battle took place in Britain in the early twentieth century regarding the place of history teaching in the universities. At Oxford and Cambridge, scholarship was downplayed. Professor Charles Harding Firth, Oxford's Regius Professor of history in 1904 ridiculed the system as best suited to produce superficial journalists. The Oxford tutors, who had more votes than the professors, fought back in defense of their system saying that it successfully produced Britain's outstanding statesmen, administrators, prelates, and diplomats, and that mission was as valuable as training scholars. The tutors dominated the debate until after the Second World War. It forced aspiring young scholars to teach at outlying schools, such as Manchester University, where Thomas Frederick Tout was professionalizing the History undergraduate programme by introducing the study of original sources and requiring the writing of a thesis.[87][88]
In the United States, scholarship was concentrated at the major PhD-producing universities, while the large number of other colleges and universities focused on undergraduate teaching. A tendency in the 21st century was for the latter schools to increasingly demand scholarly productivity of their younger tenure-track faculty. Furthermore, universities have increasingly relied on inexpensive part-time adjuncts to do most of the classroom teaching.[89]
From the origins of national school systems in the 19th century, the teaching of history to promote national sentiment has been a high priority. In the United States after World War I, a strong movement emerged at the university level to teach courses in Western Civilization, so as to give students a common heritage with Europe. In the U.S. after 1980, attention increasingly moved toward teaching world history or requiring students to take courses in non-western cultures, to prepare students for life in a globalized economy.[90]
At the university level, historians debate the question of whether history belongs more to social science or to the humanities. Many view the field from both perspectives.
The teaching of history in French schools was influenced by the Nouvelle histoire as disseminated after the 1960s by Cahiers pédagogiques and Enseignement and other journals for teachers. Also influential was the Institut national de recherche et de documentation pédagogique, (INRDP). Joseph Leif, the Inspector-general of teacher training, said pupils children should learn about historians' approaches as well as facts and dates. Louis François, Dean of the History/Geography group in the Inspectorate of National Education advised that teachers should provide historic documents and promote "active methods" which would give pupils "the immense happiness of discovery." Proponents said it was a reaction against the memorization of names and dates that characterized teaching and left the students bored. Traditionalists protested loudly it was a postmodern innovation that threatened to leave the youth ignorant of French patriotism and national identity.[91]
In several countries history textbooks are tools to foster nationalism and patriotism, and give students the official narrative about national enemies.[92]
In many countries, history textbooks are sponsored by the national government and are written to put the national heritage in the most favorable light. For example, in Japan, mention of the Nanking Massacre has been removed from textbooks and the entire Second World War is given cursory treatment. Other countries have complained.[93] Another example includes Turkey, where there is no mention of the Armenian Genocide in Turkish textbooks as a result of the denial of the genocide.[94]
It was standard policy in communist countries to present only a rigid Marxist historiography.[95][96]
In the United States, textbooks published by the same company often differ in content from state to state.[97] An example of content that is represented different in different regions of the country is the history of the Southern states, where slavery and the American Civil War are treated as controversial topics. McGraw-Hill Education for example, was criticized for describing Africans brought to American plantations as "workers" instead of slaves in a textbook.[98]
Academic historians have often fought against the politicization of the textbooks, sometimes with success.[99][100]
In 21st-century Germany, the history curriculum is controlled by the 16 states, and is characterized not by superpatriotism but rather by an "almost pacifistic and deliberately unpatriotic undertone" and reflects "principles formulated by international organizations such as UNESCO or the Council of Europe, thus oriented towards human rights, democracy and peace." The result is that "German textbooks usually downplay national pride and ambitions and aim to develop an understanding of citizenship centered on democracy, progress, human rights, peace, tolerance and Europeanness."[101]


According to consensus in modern genetics, anatomically modern humans first arrived on the Indian subcontinent from Africa between 73,000 and 55,000 years ago.[1] However, the earliest known human remains in South Asia date to 30,000 years ago. Settled life, which involves the transition from foraging to farming and pastoralism, began in South Asia around 7000 BCE. At the site of Mehrgarh presence can be documented of the domestication of wheat and barley, rapidly followed by that of goats, sheep, and cattle.[2] By 4500 BCE, settled life had spread more widely,[2] and began to gradually evolve into the Indus Valley civilisation, an early civilisation of the Old World, which was contemporaneous with Ancient Egypt and Mesopotamia. This civilisation flourished between 2500 BCE and 1900 BCE in what today is Pakistan and north-western India, and was noted for its urban planning, baked brick houses, elaborate drainage, and water supply.[3]
Early on in the second millennium BCE, persistent drought caused the population of the Indus Valley to scatter from large urban centres to villages. Around the same time, Indo-Aryan tribes moved into the Punjab from Central Asia in several waves of migration. Their Vedic Period (1500–500 BCE) was marked by the composition of the Vedas, large collections of hymns of these tribes. Their varna system, which evolved into the caste system, consisted of a hierarchy of priests, warriors, and free peasants. The pastoral and nomadic Indo-Aryans spread from the Punjab into the Gangetic plain, large swaths of which they deforested for agriculture usage. The composition of Vedic texts ended around 600 BCE, when a new, interregional culture arose. Small chieftaincies, or janapadas, were consolidated into larger states, or mahajanapadas, and a second urbanisation took place. This urbanisation was accompanied by the rise of new ascetic movements in Greater Magadha, including Jainism and Buddhism, which opposed the growing influence of Brahmanism and the primacy of rituals, presided by Brahmin priests, that had come to be associated with Vedic religion,[4] and gave rise to new religious concepts.[5] In response to the success of these movements, Vedic Brahmanism was synthesised with the preexisting religious cultures of the subcontinent, giving rise to Hinduism.
Most of the Indian subcontinent was conquered by the Maurya Empire during the 4th and 3rd centuries BCE. From the 3rd century BCE onwards, Prakrit and Pali literature in the north and Tamil Sangam literature in southern India started to flourish.[6][7] Wootz steel originated in south India in the 3rd century BCE and was exported.[8][9][10] The Maurya Empire would collapse in 185 BCE, on the assassination of the then-Emperor Brihadratha by his General Pushyamitra Shunga. Shunga would go on to form the Shunga Empire in the North and Northeast of the subcontinent, while the Greco-Bactrian Kingdom would claim the Northwest, and found the Indo-Greek Kingdom. During this Classical period, various parts of India were ruled by numerous dynasties, including the Gupta Empire in the 4-6th centuries CE. This period, witnessing a Hindu religious and intellectual resurgence, is known as the classical or "Golden Age of India". During this period, aspects of Indian civilisation, administration, culture, and religion (Hinduism and Buddhism) spread to much of Asia, while kingdoms in southern India had maritime business links with the Middle East and the Mediterranean. Indian cultural influence spread over many parts of Southeast Asia, which led to the establishment of Indianised kingdoms in Southeast Asia (Greater India).[11][12]
The most significant event between the 7th and 11th century was the Tripartite struggle centred on Kannauj that lasted for more than two centuries between the Pala Empire, Rashtrakuta Empire, and Gurjara-Pratihara Empire. Southern India saw the rise of multiple imperial powers from the middle of the fifth century, most notably the Chalukya, Chola, Pallava, Chera, Pandyan, and Western Chalukya Empires. The Chola dynasty conquered southern India and successfully invaded parts of Southeast Asia, Sri Lanka, the Maldives, and Bengal[13] in the 11th century.[14][15] In the early medieval period Indian mathematics, including Hindu numerals, influenced the development of mathematics and astronomy in the Arab world.[16]
Islamic conquests made limited inroads into modern Afghanistan and Sindh as early as the 8th century,[17] followed by the invasions of Mahmud Ghazni.[18]
The Delhi Sultanate was founded in 1206 CE by Central Asian Turks who ruled a major part of the northern Indian subcontinent in the early 14th century, but declined in the late 14th century,[19] and saw the advent of the Deccan sultanates.[20] The wealthy Bengal Sultanate also emerged as a major power, lasting over three centuries.[21]
This period also saw the emergence of several powerful Hindu states, notably Vijayanagara and Rajput states, such as Mewar. The 15th century saw the advent of Sikhism. The early modern period began in the 16th century, when the Mughal Empire conquered most of the Indian subcontinent,[22] signalling the proto-industrialization, becoming the biggest global economy and manufacturing power,[23] with a nominal GDP that valued a quarter of world GDP, superior than the combination of Europe's GDP.[24][25] The Mughals suffered a gradual decline in the early 18th century, which provided opportunities for the Marathas, Sikhs, Mysoreans, Nizams, and Nawabs of Bengal to exercise control over large regions of the Indian subcontinent.[26][27]
From the mid-18th century to the mid-19th century, large regions of India were gradually annexed by the East India Company, a chartered company acting as a sovereign power on behalf of the British government. Dissatisfaction with company rule in India led to the Indian Rebellion of 1857, which rocked parts of north and central India, and led to the dissolution of the company. India was afterwards ruled directly by the British Crown, in the British Raj. After World War I, a nationwide struggle for independence was launched by the Indian National Congress, led by Mahatma Gandhi, and noted for nonviolence. Later, the All-India Muslim League would advocate for a separate Muslim-majority nation state. The British Indian Empire was partitioned in August 1947 into the Dominion of India and Dominion of Pakistan, each gaining its independence.
Hominin expansion from Africa is estimated to have reached the Indian subcontinent approximately two million years ago, and possibly as early as 2.2 million years before the present.[32][33][34] This dating is based on the known presence of Homo erectus in Indonesia by 1.8 million years before the present and in East Asia by 1.36 million years before present, as well as the discovery of stone tools at Riwat in the Soan River valley of the Pabbi Hills region, Pakistan.[33][35] Although some older discoveries have been claimed, the suggested dates, based on the dating of fluvial sediments, have not been independently verified.[34][36]
The oldest hominin fossil remains in the Indian subcontinent are those of Homo erectus or Homo heidelbergensis, from the Narmada Valley in central India, and are dated to approximately half a million years ago.[33][36] Older fossil finds have been claimed, but are considered unreliable.[36] Reviews of archaeological evidence have suggested that occupation of the Indian subcontinent by hominins was sporadic until approximately 700,000 years ago, and was geographically widespread by approximately 250,000 years before the present, from which point onward, archaeological evidence of proto-human presence is widely mentioned.[36][34]

According to a historical demographer of South Asia, Tim Dyson: Modern human beings—Homo sapiens—originated in Africa. Then, intermittently, sometime between 60,000 and 80,000 years ago, tiny groups of them began to enter the north-west of the Indian subcontinent. It seems likely that initially, they came by way of the coast. ... it is virtually certain that there were Homo sapiens in the subcontinent 55,000 years ago, even though the earliest fossils that have been found of them date to only about 30,000 years before the present.[37]
According to Michael D. Petraglia and Bridget Allchin: Y-Chromosome and Mt-DNA data support the colonization of South Asia by modern humans originating in Africa. ... Coalescence dates for most non-European populations average to between 73–55 ka.[38]
And according to historian of South Asia, Michael H. Fisher: Scholars estimate that the first successful expansion of the Homo sapiens range beyond Africa and across the Arabian Peninsula occurred from as early as 80,000 years ago to as late as 40,000 years ago, although there may have been prior unsuccessful emigrations. Some of their descendants extended the human range ever further in each generation, spreading into each habitable land they encountered. One human channel was along the warm and productive coastal lands of the Persian Gulf and northern Indian Ocean. Eventually, various bands entered India between 75,000 years ago and 35,000 years ago.[39]Archaeological evidence has been interpreted to suggest the presence of anatomically modern humans in the Indian subcontinent 78,000–74,000 years ago,[40] although this interpretation is disputed.[41][42] The occupation of South Asia by modern humans, over a long time, initially in varying forms of isolation as hunter-gatherers, has turned it into a highly diverse one, second only to Africa in human genetic diversity.[43]

According to Tim Dyson: Genetic research has contributed to knowledge of the prehistory of the subcontinent's people in other respects. In particular, the level of genetic diversity in the region is extremely high. Indeed, only Africa's population is genetically more diverse. Related to this, there is strong evidence of ‘founder’ events in the subcontinent. By this is meant circumstances where a subgroup—such as a tribe—derives from a tiny number of ‘original’ individuals. Further, compared to most world regions, the subcontinent's people are relatively distinct in having practised comparatively high levels of endogamy.[43]Settled life emerged on the subcontinent in the western margins of the Indus River alluvium approximately 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[2][44] According to Tim Dyson: "By 7,000 years ago agriculture was firmly established in Baluchistan. And, over the next 2,000 years, the practice of farming slowly spread eastwards into the Indus valley." And according to Michael Fisher:[45]  "The earliest discovered instance ... of well-established, settled agricultural society is at Mehrgarh in the hills between the Bolan Pass and the Indus plain (today in Pakistan) (see Map 3.1). From as early as 7000 BCE, communities there started investing increased labor in preparing the land and selecting, planting, tending, and harvesting particular grain-producing plants. They also domesticated animals, including sheep, goats, pigs, and oxen (both humped zebu [Bos indicus] and unhumped [Bos taurus]). Castrating oxen, for instance, turned them from mainly meat sources into domesticated draft-animals as well."[45]The Bronze Age in the Indian subcontinent began around 3300 BCE. Along with Ancient Egypt and Mesopotamia, the Indus Valley region was one of three early cradles of civilization of the Old World. Of the three, the Indus Valley civilisation was the most expansive,[46] and at its peak, may have had a population of over five million.[47]
The civilisation was primarily centered in modern-day Pakistan, in the Indus river basin, and secondarily in the Ghaggar-Hakra River basin in eastern Pakistan and northwestern India. The mature Indus civilisation flourished from about 2600 to 1900 BCE, marking the beginning of urban civilisation on the Indian subcontinent. The civilisation included cities such as Harappa, Ganeriwala, and Mohenjo-daro in modern-day Pakistan, and Dholavira, Kalibangan, Rakhigarhi, and Lothal in modern-day India.
Inhabitants of the ancient Indus river valley, the Harappans, developed new techniques in metallurgy and handicraft (carneol[further explanation needed] products, seal carving), and produced copper, bronze, lead, and tin. The civilisation is noted for its cities built of brick, roadside drainage system, and multi-storeyed houses and is thought to have had some kind of municipal organisation. Civilization also developed a Indus script, which is presently undeciphered.[49] This is the reason why Harappan language is not directly attested, and its affiliation uncertain. [50] A relationship or membership of the Dravidian or Elamo-Dravidian language family is proposed by some scholars.[51][52]
After the collapse of Indus Valley civilisation, the inhabitants of the Indus Valley civilisation migrated from the river valleys of Indus and Ghaggar-Hakra, towards the Himalayan foothills of Ganga-Yamuna basin.[53]
During 2nd millennium BCE, Ochre Coloured Pottery culture was in Ganga Yamuna Doab region. These were rural settlement with agriculture and hunting. They were using copper tools such as axes, spears, arrows, and swords. The people had domesticated cattle, goats, sheep, horses, pigs and dogs.[55] The site gained attention for its Bronze Age solid-disk wheel carts, found in 2018,[56] which were interpreted by some as horse-pulled "chariots".[57][58][note 1]
Starting ca. 1900 BCE, Indo-Aryan tribes moved into the Punjab from Central Asia in several waves of migration.[60][61] The Vedic period is the period when the Vedas were composed, the liturgical hymns from the Indo-Aryan people. The Vedic culture was located in part of north-west India, while other parts of India had a distinct cultural identity during this period. Many regions of the Indian subcontinent transitioned from the Chalcolithic to the Iron Age in this period.[62]
The Vedic culture is described in the texts of Vedas, still sacred to Hindus, which were orally composed and transmitted in Vedic Sanskrit. The Vedas are some of the oldest extant texts in India.[63] The Vedic period, lasting from about 1500 to 500 BCE,[64][65] contributed the foundations of several cultural aspects of the Indian subcontinent.
Historians have analysed the Vedas to posit a Vedic culture in the Punjab region and the upper Gangetic Plain.[62] The peepal tree and cow were sanctified by the time of the Atharva Veda.[67] Many of the concepts of Indian philosophy espoused later, like dharma, trace their roots to Vedic antecedents.[68]
Early Vedic society is described in the Rigveda, the oldest Vedic text, believed to have been compiled during 2nd millennium BCE,[69][70] in the northwestern region of the Indian subcontinent.[71] At this time, Aryan society consisted of largely tribal and pastoral groups, distinct from the Harappan urbanisation which had been abandoned.[72] The early Indo-Aryan presence probably corresponds, in part, to the Ochre Coloured Pottery culture in archaeological contexts.[73][74]
At the end of the Rigvedic period, the Aryan society began to expand from the northwestern region of the Indian subcontinent, into the western Ganges plain. It became increasingly agricultural and was socially organised around the hierarchy of the four varnas, or social classes. This social structure was characterized both by syncretising with the native cultures of northern India,[75] but also eventually by the excluding of some indigenous peoples by labeling their occupations impure.[76] During this period, many of the previous small tribal units and chiefdoms began to coalesce into Janapadas (monarchical, state-level polities).[77]
The Iron Age in the Indian subcontinent from about 1200 BCE to the 6th century BCE is defined by the rise of Janapadas, which are realms, republics and kingdoms—notably the Iron Age Kingdoms of Kuru, Panchala, Kosala, Videha.[78][79]
The Kuru Kingdom (c. 1200–450 BCE) was the first state-level society of the Vedic period, corresponding to the beginning of the Iron Age in northwestern India, around 1200–800 BCE,[80] as well as with the composition of the Atharvaveda (the first Indian text to mention iron, as śyāma ayas, literally "black metal").[81] The Kuru state organised the Vedic hymns into collections, and developed the srauta ritual to uphold the social order.[81] Two key figures of the Kuru state were king Parikshit and his successor Janamejaya, transforming this realm into the dominant political, social, and cultural power of northern Iron Age India.[81] When the Kuru kingdom declined, the centre of Vedic culture shifted to their eastern neighbours, the Panchala kingdom.[81] The archaeological PGW (Painted Grey Ware) culture, which flourished in the Haryana and western Uttar Pradesh regions of northern India from about 1100 to 600 BCE,[73] is believed to correspond to the Kuru and Panchala kingdoms.[81][82]
During the Late Vedic Period, the kingdom of Videha emerged as a new centre of Vedic culture, situated even farther to the East (in what is today Nepal and Bihar state in India);[74] reaching its prominence under the king Janaka, whose court provided patronage for Brahmin sages and philosophers such as Yajnavalkya, Aruni, and Gārgī Vāchaknavī.[83] The later part of this period corresponds with a consolidation of increasingly large states and kingdoms, called Mahajanapadas, all across Northern India.
Sometime between 800 and 200 BCE the Śramaṇa movement formed, from which originated Jainism and Buddhism. In the same period, the first Upanishads were written. After 500 BCE, the so-called "second urbanisation" started, with new urban settlements arising at the Ganges plain, especially the Central Ganges plain.[84] The foundations for the "second urbanisation" were laid prior to 600 BCE, in the Painted Grey Ware culture of the Ghaggar-Hakra and Upper Ganges Plain; although most PGW sites were small farming villages, "several dozen" PGW sites eventually emerged as relatively large settlements that can be characterized as towns, the largest of which were fortified by ditches or moats and embankments made of piled earth with wooden palisades, albeit smaller and simpler than the elaborately fortified large cities which grew after 600 BCE in the Northern Black Polished Ware culture.[85]
The Central Ganges Plain, where Magadha gained prominence, forming the base of the Maurya Empire, was a distinct cultural area,[86] with new states arising after 500 BCE[87] during the so-called "second urbanisation".[88][note 2] It was influenced by the Vedic culture,[89] but differed markedly from the Kuru-Panchala region.[86] It "was the area of the earliest known cultivation of rice in South Asia and by 1800 BCE was the location of an advanced Neolithic population associated with the sites of Chirand and Chechar".[90] In this region, the Śramaṇic movements flourished, and Jainism and Buddhism originated.[84]
The time between 800 BCE and 400 BCE witnessed the composition of the earliest Upanishads.[4][91][92] The Upanishads form the theoretical basis of classical Hinduism, and are also known as Vedanta (conclusion of the Vedas).[93]
Increasing urbanisation of India in 7th and 6th centuries BCE led to the rise of new ascetic or "Śramaṇa movements" which challenged the orthodoxy of rituals.[4] Mahavira (c. 549–477 BCE), proponent of Jainism, and Gautama Buddha (c. 563–483 BCE), founder of Buddhism, were the most prominent icons of this movement. Śramaṇa gave rise to the concept of the cycle of birth and death, the concept of samsara, and the concept of liberation.[94] Buddha found a Middle Way that ameliorated the extreme asceticism found in the Śramaṇa religions.[95]
Around the same time, Mahavira (the 24th Tirthankara in Jainism) propagated a theology that was to later become Jainism.[96] However, Jain orthodoxy believes the teachings of the Tirthankaras predates all known time and scholars believe Parshvanatha (c. 872 – c. 772 BCE), accorded status as the 23rd Tirthankara, was a historical figure. The Vedas are believed to have documented a few Tirthankaras and an ascetic order similar to the Śramaṇa movement.[97]
The Sanskrit epics Ramayana and Mahabharata were composed during this period.[98] The Mahabharata remains, till this day, the longest single poem in the world.[99] Historians formerly postulated an "epic age" as the milieu of these two epic poems, but now recognize that the texts (which are both familiar with each other) went through multiple stages of development over centuries. For instance, the Mahabharata may have been based on a small-scale conflict (possibly about 1000 BCE) which was eventually "transformed into a gigantic epic war by bards and poets". There is no conclusive proof from archaeology as to whether the specific events of the Mahabharata have any historical basis.[100] The existing texts of these epics are believed to belong to the post-Vedic age, between c. 400 BCE and 400 CE.[100][101]
The period from c. 600 BCE to c. 300 BCE witnessed the rise of the Mahajanapadas, sixteen powerful and vast kingdoms and oligarchic republics. These Mahajanapadas evolved and flourished in a belt stretching from Gandhara in the northwest to Bengal in the eastern part of the Indian subcontinent and included parts of the trans-Vindhyan region.[102] Ancient Buddhist texts, like the Aṅguttara Nikāya,[103] make frequent reference to these sixteen great kingdoms and republics—Anga, Assaka, Avanti, Chedi, Gandhara, Kashi, Kamboja, Kosala, Kuru, Magadha, Malla, Matsya (or Machcha), Panchala, Surasena, Vṛji, and Vatsa. This period saw the second major rise of urbanism in India after the Indus Valley Civilisation.[104]
Early "republics" or gaṇasaṅgha,[105] such as Shakyas, Koliyas, Mallakas, and Licchavis had republican governments. Gaṇasaṅghas,[105] such as the Mallakas, centered in the city of Kusinagara, and the Vajjika League, centered in the city of Vaishali, existed as early as the 6th century BCE and persisted in some areas until the 4th century CE.[106] The most famous clan amongst the ruling confederate clans of the Vajji Mahajanapada were the Licchavis.[107]
This period corresponds in an archaeological context to the Northern Black Polished Ware culture. Especially focused in the Central Ganges plain but also spreading across vast areas of the northern and central Indian subcontinent, this culture is characterized by the emergence of large cities with massive fortifications, significant population growth, increased social stratification, wide-ranging trade networks, construction of public architecture and water channels, specialized craft industries (e.g., ivory and carnelian carving), a system of weights, punch-marked coins, and the introduction of writing in the form of Brahmi and Kharosthi scripts.[108][109] The language of the gentry at that time was Sanskrit, while the languages of the general population of northern India are referred to as Prakrits.
Many of the sixteen kingdoms had coalesced into four major ones by 500/400 BCE, by the time of Gautama Buddha. These four were Vatsa, Avanti, Kosala, and Magadha. The life of Gautama Buddha was mainly associated with these four kingdoms.[104]
Magadha formed one of the sixteen Mahajanapadas (Sanskrit: "Great Realms") or kingdoms in ancient India. The core of the kingdom was the area of Bihar south of the Ganges; its first capital was Rajagriha (modern Rajgir) then Pataliputra (modern Patna). Magadha expanded to include most of Bihar and Bengal with the conquest of Licchavi and Anga respectively,[110] followed by much of eastern Uttar Pradesh and Orissa. The ancient kingdom of Magadha is heavily mentioned in Jain and Buddhist texts. It is also mentioned in the Ramayana, Mahabharata and Puranas.[111] The earliest reference to the Magadha people occurs in the Atharva-Veda where they are found listed along with the Angas, Gandharis, and Mujavats. Magadha played an important role in the development of Jainism and Buddhism. The Magadha kingdom included republican communities such as the community of Rajakumara. Villages had their own assemblies under their local chiefs called Gramakas. Their administrations were divided into executive, judicial, and military functions.
Early sources, from the Buddhist Pāli Canon, the Jain Agamas and the Hindu Puranas, mention Magadha being ruled by the Pradyota dynasty and Haryanka dynasty (c. 544–413 BCE) for some 200 years, c. 600–413 BCE. King Bimbisara of the Haryanka dynasty led an active and expansive policy, conquering Anga in what is now eastern Bihar and West Bengal. King Bimbisara was overthrown and killed by his son, Prince Ajatashatru, who continued the expansionist policy of Magadha. During this period, Gautama Buddha, the founder of Buddhism, lived much of his life in Magadha kingdom. He attained enlightenment in Bodh Gaya, gave his first sermon in Sarnath and the first Buddhist council was held in Rajgriha.[112] The Haryanka dynasty was overthrown by the Shaishunaga dynasty (c. 413–345 BCE). The last Shishunaga ruler, Kalasoka, was assassinated by Mahapadma Nanda in 345 BCE, the first of the so-called Nine Nandas, which were Mahapadma and his eight sons.
The Nanda Empire (c. 345–322 BCE), at its greatest extent, extended from Bengal in the east, to the Punjab region in the west and as far south as the Vindhya Range.[113] The Nanda dynasty was famed for their great wealth. The Nanda dynasty built on the foundations laid by their Haryanka and Shishunaga predecessors to create the first great empire of north India.[114] To achieve this objective they built a vast army, consisting of 200,000 infantry, 20,000 cavalry, 2,000 war chariots and 3,000 war elephants (at the lowest estimates).[115][116][117] According to the Greek historian Plutarch, the size of the Nanda army was even larger, numbering 200,000 infantry, 80,000 cavalry, 8,000 war chariots, and 6,000 war elephants.[116][118] However, the Nanda Empire did not have the opportunity to see their army face Alexander the Great, who invaded north-western India at the time of Dhana Nanda, since Alexander was forced to confine his campaign to the plains of Punjab and Sindh, for his forces mutinied at the river Beas and refused to go any further upon encountering Nanda and Gangaridai forces.[116]
The Maurya Empire (322–185 BCE) unified most of the Indian subcontinent into one state, and was the largest empire ever to exist on the Indian subcontinent.[119] At its greatest extent, the Mauryan Empire stretched to the north up to the natural boundaries of the Himalayas and to the east into what is now Assam. To the west, it reached beyond modern Pakistan, to the Hindu Kush mountains in what is now Afghanistan. The empire was established by Chandragupta Maurya assisted by Chanakya (Kautilya) in Magadha (in modern Bihar) when he overthrew the Nanda Empire.[120]
Chandragupta rapidly expanded his power westwards across central and western India, and by 317 BCE the empire had fully occupied Northwestern India. The Mauryan Empire then defeated Seleucus I, a diadochus and founder of the Seleucid Empire, during the Seleucid–Mauryan war, thus gained additional territory west of the Indus River. Chandragupta's son Bindusara succeeded to the throne around 297 BCE. By the time he died in c. 272 BCE, a large part of the Indian subcontinent was under Mauryan suzerainty. However, the region of Kalinga (around modern day Odisha) remained outside Mauryan control, perhaps interfering with their trade with the south.[121]
Bindusara was succeeded by Ashoka, whose reign lasted for around 37 years until his death in about 232 BCE.[122] His campaign against the Kalingans in about 260 BCE, though successful, led to immense loss of life and misery. This filled Ashoka with remorse and led him to shun violence, and subsequently to embrace Buddhism.[121] The empire began to decline after his death and the last Mauryan ruler, Brihadratha, was assassinated by Pushyamitra Shunga to establish the Shunga Empire.[122]
Under Chandragupta Maurya and his successors, internal and external trade, agriculture, and economic activities all thrived and expanded across India thanks to the creation of a single efficient system of finance, administration, and security. The Mauryans built the Grand Trunk Road, one of Asia's oldest and longest major roads connecting the Indian subcontinent with Central Asia.[123] After the Kalinga War, the Empire experienced nearly half a century of peace and security under Ashoka. Mauryan India also enjoyed an era of social harmony, religious transformation, and expansion of the sciences and of knowledge. Chandragupta Maurya's embrace of Jainism increased social and religious renewal and reform across his society, while Ashoka's embrace of Buddhism has been said to have been the foundation of the reign of social and political peace and non-violence across all of India. Ashoka sponsored the spreading of Buddhist missionaries into Sri Lanka, Southeast Asia, West Asia, North Africa, and Mediterranean Europe.[124]
The Arthashastra wrote by Chanakya and the Edicts of Ashoka are the primary written records of the Mauryan times. Archaeologically, this period falls into the era of Northern Black Polished Ware. The Mauryan Empire was based on a modern and efficient economy and society. However, the sale of merchandise was closely regulated by the government.[125] Although there was no banking in the Mauryan society, usury was customary. A significant amount of written records on slavery are found, suggesting a prevalence thereof.[126] During this period, a high quality steel called Wootz steel was developed in south India and was later exported to China and Arabia.[8]
During the Sangam period Tamil literature flourished from the 3rd century BCE to the 4th century CE. During this period, three Tamil dynasties, collectively known as the Three Crowned Kings of Tamilakam: Chera dynasty, Chola dynasty, and the Pandya dynasty ruled parts of southern India.[128]
The Sangam literature deals with the history, politics, wars, and culture of the Tamil people of this period.[129] The scholars of the Sangam period rose from among the common people who sought the patronage of the Tamil Kings, but who mainly wrote about the common people and their concerns.[130] Unlike Sanskrit writers who were mostly Brahmins, Sangam writers came from diverse classes and social backgrounds and were mostly non-Brahmins. They belonged to different faiths and professions such as farmers, artisans, merchants, monks, and priests, including also royalty and women.[130]
Around c. 300 BCE – c. 200 CE, Pathupattu, an anthology of ten mid-length books collection, which is considered part of Sangam Literature, were composed; the composition of eight anthologies of poetic works Ettuthogai as well as the composition of eighteen minor poetic works Patiṉeṇkīḻkaṇakku; while Tolkāppiyam, the earliest grammarian work in the Tamil language was developed.[131] Also, during Sangam period, two of the Five Great Epics of Tamil Literature were composed. Ilango Adigal composed Silappatikaram, which is a non-religious work, that revolves around Kannagi, who having lost her husband to a miscarriage of justice at the court of the Pandyan dynasty, wreaks her revenge on his kingdom,[132] and Manimekalai, composed by Chithalai Chathanar, is a sequel to Silappatikaram, and tells the story of the daughter of Kovalan and Madhavi, who became a Buddhist Bikkuni.[133][134]
Ancient India during the rise of the Shungas from the North, Satavahanas from the Deccan, and Pandyas and Cholas from the southern tip of India.
The Great Chaitya in the Karla Caves. The shrines were developed over the period from 2nd century BCE to the 5th century CE.
Udayagiri and Khandagiri Caves is home to the Hathigumpha inscription, which was inscribed under Kharavela, the then Emperor of Kalinga of the Mahameghavahana dynasty.
Relief of a multi-storied temple, 2nd century CE, Ghantasala Stupa.[135][136]
The time between the Maurya Empire in the 3rd century BCE and the end of the Gupta Empire in the 6th century CE is referred to as the "Classical" period of India.[137] It can be divided in various sub-periods, depending on the chosen periodisation. Classical period begins after the decline of the Maurya Empire, and the corresponding rise of the Shunga Empire and Satavahana dynasty. The Gupta Empire (4th–6th century) is regarded as the "Golden Age" of Hinduism, although a host of kingdoms ruled over India in these centuries. Also, the Sangam literature flourished from the 3rd century BCE to the 3rd century CE in southern India.[7] During this period, India's economy is estimated to have been the largest in the world, having between one-third and one-quarter of the world's wealth, from 1 CE to 1000 CE.[138][139]
The Shungas originated from Magadha, and controlled large areas of the central and eastern Indian subcontinent from around 187 to 78 BCE. The dynasty was established by Pushyamitra Shunga, who overthrew the last Maurya emperor. Its capital was Pataliputra, but later emperors, such as Bhagabhadra, also held court at Vidisha, modern Besnagar in Eastern Malwa.[140]
Pushyamitra Shunga ruled for 36 years and was succeeded by his son Agnimitra. There were ten Shunga rulers. However, after the death of Agnimitra, the empire rapidly disintegrated;[141] inscriptions and coins indicate that much of northern and central India consisted of small kingdoms and city-states that were independent of any Shunga hegemony.[142] The empire is noted for its numerous wars with both foreign and indigenous powers. They fought battles with the Mahameghavahana dynasty of Kalinga, Satavahana dynasty of Deccan, the Indo-Greeks, and possibly the Panchalas and Mitras of Mathura.
Art, education, philosophy, and other forms of learning flowered during this period including small terracotta images, larger stone sculptures, and architectural monuments such as the Stupa at Bharhut, and the renowned Great Stupa at Sanchi. The Shunga rulers helped to establish the tradition of royal sponsorship of learning and art. The script used by the empire was a variant of Brahmi and was used to write the Sanskrit language. The Shunga Empire played an imperative role in patronising Indian culture at a time when some of the most important developments in Hindu thought were taking place. This helped the empire flourish and gain power.
The Śātavāhanas were based from Amaravati in Andhra Pradesh as well as Junnar (Pune) and Prathisthan (Paithan) in Maharashtra. The territory of the empire covered large parts of India from the 1st century BCE onward. The Sātavāhanas started out as feudatories to the Mauryan dynasty, but declared independence with its decline.
The Sātavāhanas are known for their patronage of Hinduism and Buddhism, which resulted in Buddhist monuments from Ellora (a UNESCO World Heritage Site) to Amaravati. They were one of the first Indian states to issue coins struck with their rulers embossed. They formed a cultural bridge and played a vital role in trade as well as the transfer of ideas and culture to and from the Indo-Gangetic Plain to the southern tip of India.
They had to compete with the Shunga Empire and then the Kanva dynasty of Magadha to establish their rule. Later, they played a crucial role to protect large part of India against foreign invaders like the Sakas, Yavanas and Pahlavas. In particular, their struggles with the Western Kshatrapas went on for a long time. The notable rulers of the Satavahana Dynasty Gautamiputra Satakarni and Sri Yajna Sātakarni were able to defeat the foreign invaders like the Western Kshatrapas and to stop their expansion. In the 3rd century CE the empire was split into smaller states.[143]
The Kushan Empire expanded out of what is now Afghanistan into the northwest of the Indian subcontinent under the leadership of their first emperor, Kujula Kadphises, about the middle of the 1st century CE. The Kushans were possibly of Tocharian speaking tribe;[150] one of five branches of the Yuezhi confederation.[151][152] By the time of his grandson, Kanishka the Great, the empire spread to encompass much of Afghanistan,[153] and then the northern parts of the Indian subcontinent at least as far as Saketa and Sarnath near Varanasi (Banaras).[154]
Emperor Kanishka was a great patron of Buddhism; however, as Kushans expanded southward, the deities of their later coinage came to reflect its new Hindu majority.[155][156] They played an important role in the establishment of Buddhism in India and its spread to Central Asia and China.
Historian Vincent Smith said about Kanishka:
He played the part of a second Ashoka in the history of Buddhism.[157]The empire linked the Indian Ocean maritime trade with the commerce of the Silk Road through the Indus valley, encouraging long-distance trade, particularly between China and Rome. The Kushans brought new trends to the budding and blossoming Gandhara art and Mathura art, which reached its peak during Kushan rule.[158]
H.G. Rowlinson commented:
The Kushan period is a fitting prelude to the Age of the Guptas.[159]By the 3rd century, their empire in India was disintegrating and their last known great emperor was Vasudeva I.[160][161]
The Gupta period was noted for cultural creativity, especially in literature, architecture, sculpture, and painting.[162] The Gupta period produced scholars such as Kalidasa, Aryabhata, Varahamihira, Vishnu Sharma, and Vatsyayana who made great advancements in many academic fields. The Gupta period marked a watershed of Indian culture: the Guptas performed Vedic sacrifices to legitimise their rule, but they also patronised Buddhism, which continued to provide an alternative to Brahmanical orthodoxy. The military exploits of the first three rulers – Chandragupta I, Samudragupta, and Chandragupta II – brought much of India under their leadership.[163] Science and political administration reached new heights during the Gupta era. Strong trade ties also made the region an important cultural centre and established it as a base that would influence nearby kingdoms and regions in: Sri Lanka; Maritime Southeast Asia (Brunei, Indonesia, Malaysia, Philippines, Singapore, and Timor-Leste);[164] as well as Indochina (Cambodia, Laos, Myanmar, Thailand, and Vietnam).[165]
The latter Guptas successfully resisted the northwestern kingdoms until the arrival of the Alchon Huns, who established themselves in Afghanistan by the first half of the 5th century CE, with their capital at Bamiyan.[166] However, much of the southern India including Deccan were largely unaffected by these events in the north.[167][168]
The Vākāṭaka Empire originated from the Deccan in the mid-third century CE. Their state is believed to have extended from the southern edges of Malwa and Gujarat in the north to the Tungabhadra River in the south as well as from the Arabian Sea in the western to the edges of Chhattisgarh in the east. They were the most important successors of the Satavahanas in the Deccan, contemporaneous with the Guptas in northern India and succeeded by the Vishnukundina dynasty.
The Vakatakas are noted for having been patrons of the arts, architecture and literature. They led public works and their monuments are a visible legacy. The rock-cut Buddhist viharas and chaityas of Ajanta Caves (a UNESCO World Heritage Site) were built under the patronage of Vakataka emperor, Harishena.[169][170]
The Ajanta Caves are 30 rock-cut Buddhist cave monument built under the Vakatakas.
Buddhist monks praying in front of the Dagoba of Chaitya Cave 26 of the Ajanta Caves.
Buddhist "Chaitya Griha" or prayer hall, with a seated Buddha, Cave 26 of the Ajanta Caves.
Many foreign ambassadors, representatives, and travelers are included as devotees attending the Buddha's descent from Trayastrimsa Heaven; painting from Cave 17 of the Ajanta Caves.
Samudragupta's 4th-century Allahabad pillar inscription mentions Kamarupa (Western Assam)[171] and Davaka (Central Assam)[172] as frontier kingdoms of the Gupta Empire. Davaka was later absorbed by Kamarupa, which grew into a large kingdom that spanned from Karatoya river to near present Sadiya and covered the entire Brahmaputra valley, North Bengal, parts of Bangladesh and, at times Purnea and parts of West Bengal.[173]
Ruled by three dynasties Varmanas (c. 350–650 CE), Mlechchha dynasty (c. 655–900 CE) and Kamarupa-Palas (c. 900–1100 CE), from their capitals in present-day Guwahati (Pragjyotishpura), Tezpur (Haruppeswara) and North Gauhati (Durjaya) respectively. All three dynasties claimed their descent from Narakasura, an immigrant from Aryavarta.[174] In the reign of the Varman king, Bhaskar Varman (c. 600–650 CE), the Chinese traveller Xuanzang visited the region and recorded his travels. Later, after weakening and disintegration (after the Kamarupa-Palas), the Kamarupa tradition was somewhat extended until c. 1255 CE by the Lunar I (c. 1120–1185 CE) and Lunar II (c. 1155–1255 CE) dynasties.[175] The Kamarupa kingdom came to an end in the middle of the 13th century when the Khen dynasty under Sandhya of Kamarupanagara (North Guwahati), moved his capital to Kamatapur (North Bengal) after the invasion of Muslim Turks, and established the Kamata kingdom.[176]
The Pallavas, during the 4th to 9th centuries were, alongside the Guptas of the North, great patronisers of Sanskrit development in the South of the Indian subcontinent. The Pallava reign saw the first Sanskrit inscriptions in a script called Grantha.[177] Early Pallavas had different connexions to Southeast Asian countries. The Pallavas used Dravidian architecture to build some very important Hindu temples and academies in Mamallapuram, Kanchipuram and other places; their rule saw the rise of great poets. The practice of dedicating temples to different deities came into vogue followed by fine artistic temple architecture and sculpture style of Vastu Shastra.[178]
Pallavas reached the height of power during the reign of Mahendravarman I (571–630 CE) and Narasimhavarman I (630–668 CE) and dominated the Telugu and northern parts of the Tamil region for about six hundred years until the end of the 9th century.[179]
Kadambas originated from Karnataka, was founded by Mayurasharma in 345 CE which at later times showed the potential of developing into imperial proportions, an indication to which is provided by the titles and epithets assumed by its rulers. King Mayurasharma defeated the armies of Pallavas of Kanchi possibly with help of some native tribes. The Kadamba fame reached its peak during the rule of Kakusthavarma, a notable ruler with whom even the kings of Gupta Dynasty of northern India cultivated marital alliances. The Kadambas were contemporaries of the Western Ganga Dynasty and together they formed the earliest native kingdoms to rule the land with absolute autonomy. The dynasty later continued to rule as a feudatory of larger Kannada empires, the Chalukya and the Rashtrakuta empires, for over five hundred years during which time they branched into minor dynasties known as the Kadambas of Goa, Kadambas of Halasi and Kadambas of Hangal.
Harsha ruled northern India from 606 to 647 CE. He was the son of Prabhakarvardhana and the younger brother of Rajyavardhana, who were members of the Vardhana dynasty and ruled Thanesar, in present-day Haryana.
After the downfall of the prior Gupta Empire in the middle of the 6th century, North India reverted to smaller republics and monarchical states. The power vacuum resulted in the rise of the Vardhanas of Thanesar, who began uniting the republics and monarchies from the Punjab to central India. After the death of Harsha's father and brother, representatives of the empire crowned Harsha emperor at an assembly in April 606 CE, giving him the title of Maharaja when he was merely 16 years old.[181] At the height of his power, his Empire covered much of North and Northwestern India, extended East until Kamarupa, and South until Narmada River; and eventually made Kannauj (in present Uttar Pradesh state) his capital, and ruled until 647 CE.[182]
The peace and prosperity that prevailed made his court a centre of cosmopolitanism, attracting scholars, artists and religious visitors from far and wide.[182] During this time, Harsha converted to Buddhism from Surya worship.[183] The Chinese traveller Xuanzang visited the court of Harsha and wrote a very favourable account of him, praising his justice and generosity.[182] His biography Harshacharita ("Deeds of Harsha") written by Sanskrit poet Banabhatta, describes his association with Thanesar, besides mentioning the defence wall, a moat and the palace with a two-storied Dhavalagriha (White Mansion).[184][185]
Early medieval India began after the end of the Gupta Empire in the 6th century CE.[137] This period also covers the "Late Classical Age" of Hinduism,[186] which began after the end of the Gupta Empire,[186] and the collapse of the Empire of Harsha in the 7th century CE;[186] the beginning of Imperial Kannauj, leading to the Tripartite struggle; and ended in the 13th century with the rise of the Delhi Sultanate in Northern India[187] and the end of the Later Cholas with the death of Rajendra Chola III in 1279 in Southern India; however some aspects of the Classical period continued until the fall of the Vijayanagara Empire in the south around the 17th century.
From the fifth century to the thirteenth, Śrauta sacrifices declined, and initiatory traditions of Buddhism, Jainism or more commonly Shaivism, Vaishnavism and Shaktism expanded in royal courts.[188] This period produced some of India's finest art, considered the epitome of classical development, and the development of the main spiritual and philosophical systems which continued to be in Hinduism, Buddhism and Jainism.
In the 7th century CE, Kumārila Bhaṭṭa formulated his school of Mimamsa philosophy and defended the position on Vedic rituals against Buddhist attacks. Scholars note Bhaṭṭa's contribution to the decline of Buddhism in India.[189] In the 8th century, Adi Shankara travelled across the Indian subcontinent to propagate and spread the doctrine of Advaita Vedanta, which he consolidated; and is credited with unifying the main characteristics of the current thoughts in Hinduism.[190][191][192] He was a critic of both Buddhism and Minamsa school of Hinduism;[193][194][195][196] and founded mathas (monasteries), in the four corners of the Indian subcontinent for the spread and development of Advaita Vedanta.[197] While, Muhammad bin Qasim's invasion of Sindh (modern Pakistan) in 711 CE witnessed further decline of Buddhism. The Chach Nama records many instances of conversion of stupas to mosques such as at Nerun.[198]
From the 8th to the 10th century, three dynasties contested for control of northern India: the Gurjara Pratiharas of Malwa, the Palas of Bengal, and the Rashtrakutas of the Deccan. The Sena dynasty would later assume control of the Pala Empire; the Gurjara Pratiharas fragmented into various states, notably the Paramaras of Malwa, the Chandelas of Bundelkhand, the Kalachuris of Mahakoshal, the Tomaras of Haryana, and the Chauhans of Rajputana, these states were some of the earliest Rajput kingdoms;[199] while the Rashtrakutas were annexed by the Western Chalukyas.[200] During this period, the Chaulukya dynasty emerged; the Chaulukyas constructed the Dilwara Temples, Modhera Sun Temple, Rani ki vav[201] in the style of Māru-Gurjara architecture, and their capital Anhilwara (modern Patan, Gujarat) was one of the largest cities in the Indian subcontinent, with the population estimated at 100,000 in 1000 CE.
The Chola Empire emerged as a major power during the reign of Raja Raja Chola I and Rajendra Chola I who successfully invaded parts of Southeast Asia and Sri Lanka in the 11th century.[202] Lalitaditya Muktapida (r. 724–760 CE) was an emperor of the Kashmiri Karkoṭa dynasty, which exercised influence in northwestern India from 625 CE until 1003, and was followed by Lohara dynasty. Kalhana in his Rajatarangini credits king Lalitaditya with leading an aggressive military campaign in Northern India and Central Asia.[203][204][205]
The Hindu Shahi dynasty ruled portions of eastern Afghanistan, northern Pakistan, and Kashmir from the mid-7th century to the early 11th century. While in Odisha, the Eastern Ganga Empire rose to power; noted for the advancement of Hindu architecture, most notable being Jagannath Temple and Konark Sun Temple, as well as being patrons of art and literature.
Martand Sun Temple Central shrine, dedicated to the deity Surya, and built by the third ruler of the Karkota dynasty, Lalitaditya Muktapida, in the 8th century CE.
Konark Sun Temple at Konark, Orissa, built by Narasimhadeva I (1238–1264 CE) of the Eastern Ganga dynasty.
Kandariya Mahadeva Temple in the Khajuraho complex was built by the Chandelas.
Jagannath Temple at Puri, built by Anantavarman Chodaganga Deva of the Eastern Ganga dynasty.
The Chalukya Empire ruled large parts of southern and central India between the 6th and the 12th centuries. During this period, they ruled as three related yet individual dynasties. The earliest dynasty, known as the "Badami Chalukyas", ruled from Vatapi (modern Badami) from the middle of the 6th century. The Badami Chalukyas began to assert their independence at the decline of the Kadamba kingdom of Banavasi and rapidly rose to prominence during the reign of Pulakeshin II. The rule of the Chalukyas marks an important milestone in the history of South India and a golden age in the history of Karnataka. The political atmosphere in South India shifted from smaller kingdoms to large empires with the ascendancy of Badami Chalukyas. A Southern India-based kingdom took control and consolidated the entire region between the Kaveri and the Narmada rivers. The rise of this empire saw the birth of efficient administration, overseas trade and commerce and the development of new style of architecture called "Chalukyan architecture". The Chalukya dynasty ruled parts of southern and central India from Badami in Karnataka between 550 and 750, and then again from Kalyani between 970 and 1190.
Galaganatha Temple at Pattadakal complex (UNESCO World Heritage) is an example of Badami Chalukya architecture.
Bhutanatha temple complex at Badami, next to a waterfall, during the monsoon.
Vishnu image inside the Badami Cave Temple Complex. The complex is an example of Indian rock-cut architecture.
8th century Durga temple exterior view at Aihole complex. Aihole complex includes Hindu, Buddhist and Jain temples and monuments.
Founded by Dantidurga around 753,[206] the Rashtrakuta Empire ruled from its capital at Manyakheta for almost two centuries.[207] At its peak, the Rashtrakutas ruled from the Ganges River and Yamuna River doab in the north to Cape Comorin in the south, a fruitful time of political expansion, architectural achievements and famous literary contributions.[208][209]
The early rulers of this dynasty were Hindu, but the later rulers were strongly influenced by Jainism.[210] Govinda III and Amoghavarsha were the most famous of the long line of able administrators produced by the dynasty. Amoghavarsha, who ruled for 64 years, was also an author and wrote Kavirajamarga, the earliest known Kannada work on poetics.[207][211] Architecture reached a milestone in the Dravidian style, the finest example of which is seen in the Kailasanath Temple at Ellora. Other important contributions are the Kashivishvanatha temple and the Jain Narayana temple at Pattadakal in Karnataka.
The Arab traveller Suleiman described the Rashtrakuta Empire as one of the four great Empires of the world.[212] The Rashtrakuta period marked the beginning of the golden age of southern Indian mathematics. The great south Indian mathematician Mahāvīra lived in the Rashtrakuta Empire and his text had a huge impact on the medieval south Indian mathematicians who lived after him.[213] The Rashtrakuta rulers also patronised men of letters, who wrote in a variety of languages from Sanskrit to the Apabhraṃśas.[207]
Kailasa temple, is one of the largest rock-cut ancient Hindu temples located in Ellora.
Shikhara of Indra Sabha at Ellora Caves.
Statue of the Buddha seated. A part of the Carpenter's cave (Buddhist Cave 10).
Jain Tirthankara Mahavira with Yaksha Matanga and Yakshi Siddhaiki at Ellora Caves.
The Gurjara-Pratiharas were instrumental in containing Arab armies moving east of the Indus River.[214] Nagabhata I defeated the Arab army under Junaid and Tamin during the Caliphate campaigns in India. Under Nagabhata II, the Gurjara-Pratiharas became the most powerful dynasty in northern India. He was succeeded by his son Ramabhadra, who ruled briefly before being succeeded by his son, Mihira Bhoja. Under Bhoja and his successor Mahendrapala I, the Pratihara Empire reached its peak of prosperity and power. By the time of Mahendrapala, the extent of its territory rivalled that of the Gupta Empire stretching from the border of Sindh in the west to Bihar in the east and from the Himalayas in the north to areas past the Narmada in the south.[215][verification needed] The expansion triggered a tripartite power struggle with the Rashtrakuta and Pala empires for control of the Indian subcontinent. During this period, Imperial Pratihara took the title of Maharajadhiraja of Āryāvarta (Great King of Kings of India).[citation needed]
By the 10th century, several feudatories of the empire took advantage of the temporary weakness of the Gurjara-Pratiharas to declare their independence, notably the Paramaras of Malwa, the Chandelas of Bundelkhand, the Kalachuris of Mahakoshal, the Tomaras of Haryana, and the Chauhans of Rajputana.
One of the four entrances of the Teli ka Mandir. This Hindu temple was built by the Pratihara emperor Mihira Bhoja.[216]
Sculptures near Teli ka Mandir, Gwalior Fort.
Jainism-related cave monuments and statues carved into the rock face inside Siddhachal Caves, Gwalior Fort.
Ghateshwara Mahadeva temple at Baroli Temples complex. The complex of eight temples, built by the Gurjara-Pratiharas, is situated within a walled enclosure.
Gahadavala dynasty ruled parts of the present-day Indian states of Uttar Pradesh and Bihar, during 11th and 12th centuries. Their capital was located at Varanasi in the Gangetic plains.[217]

The Khayaravala dynasty, ruled parts of the present-day Indian states of Bihar and Jharkhand, during 11th and 12th centuries. Their capital was located at Khayaragarh in Shahabad district. Pratapdhavala and Shri Pratapa were king of the dynasty according to inscription of Rohtas.[218]The Pala Empire was founded by Gopala I.[219][220][221] It was ruled by a Buddhist dynasty from Bengal in the eastern region of the Indian subcontinent. The Palas reunified Bengal after the fall of Shashanka's Gauda Kingdom.[222]
The Palas were followers of the Mahayana and Tantric schools of Buddhism,[223] they also patronised Shaivism and Vaishnavism.[224] The morpheme Pala, meaning "protector", was used as an ending for the names of all the Pala monarchs. The empire reached its peak under Dharmapala and Devapala. Dharmapala is believed to have conquered Kanauj and extended his sway up to the farthest limits of India in the northwest.[224]
The Pala Empire can be considered as the golden era of Bengal in many ways.[225] Dharmapala founded the Vikramashila and revived Nalanda,[224] considered one of the first great universities in recorded history. Nalanda reached its height under the patronage of the Pala Empire.[225][226] The Palas also built many viharas. They maintained close cultural and commercial ties with countries of Southeast Asia and Tibet. Sea trade added greatly to the prosperity of the Pala Empire. The Arab merchant Suleiman notes the enormity of the Pala army in his memoirs.[224]
Medieval Cholas rose to prominence during the middle of the 9th century CE and established the greatest empire South India had seen.[227] They successfully united the South India under their rule and through their naval strength extended their influence in the Southeast Asian countries such as Srivijaya.[202] Under Rajaraja Chola I and his successors Rajendra Chola I, Rajadhiraja Chola, Virarajendra Chola and Kulothunga Chola I the dynasty became a military, economic and cultural power in South Asia and South-East Asia.[228][229] Rajendra Chola I's navies went even further, occupying the sea coasts from Burma to Vietnam,[230] the Andaman and Nicobar Islands, the Lakshadweep (Laccadive) islands, Sumatra, and the Malay Peninsula in Southeast Asia and the Pegu islands. The power of the new empire was proclaimed to the eastern world by the expedition to the Ganges which Rajendra Chola I undertook and by the occupation of cities of the maritime empire of Srivijaya in Southeast Asia, as well as by the repeated embassies to China.[231]
They dominated the political affairs of Sri Lanka for over two centuries through repeated invasions and occupation. They also had continuing trade contacts with the Arabs in the west and with the Chinese empire in the east.[232] Rajaraja Chola I and his equally distinguished son Rajendra Chola I gave political unity to the whole of Southern India and established the Chola Empire as a respected sea power.[233] Under the Cholas, the South India reached new heights of excellence in art, religion and literature. In all of these spheres, the Chola period marked the culmination of movements that had begun in an earlier age under the Pallavas. Monumental architecture in the form of majestic temples and sculpture in stone and bronze reached a finesse never before achieved in India.[234]
The granite gopuram (tower) of Brihadeeswarar Temple, 1010 CE.
Chariot detail at Airavatesvara Temple built by Rajaraja Chola II in the 12th century CE.
The pyramidal structure above the sanctum at Brihadisvara Temple.
Brihadeeswara Temple Entrance Gopurams at Thanjavur.
The Western Chalukya Empire ruled most of the western Deccan, South India, between the 10th and 12th centuries.[235] Vast areas between the Narmada River in the north and Kaveri River in the south came under Chalukya control.[235] During this period the other major ruling families of the Deccan, the Hoysalas, the Seuna Yadavas of Devagiri, the Kakatiya dynasty and the Southern Kalachuris, were subordinates of the Western Chalukyas and gained their independence only when the power of the Chalukya waned during the latter half of the 12th century.[236]
The Western Chalukyas developed an architectural style known today as a transitional style, an architectural link between the style of the early Chalukya dynasty and that of the later Hoysala empire. Most of its monuments are in the districts bordering the Tungabhadra River in central Karnataka. Well known examples are the Kasivisvesvara Temple at Lakkundi, the Mallikarjuna Temple at Kuruvatti, the Kallesvara Temple at Bagali, Siddhesvara Temple at Haveri, and the Mahadeva Temple at Itagi.[237] This was an important period in the development of fine arts in Southern India, especially in literature as the Western Chalukya kings encouraged writers in the native language of Kannada, and Sanskrit like the philosopher and statesman Basava and the great mathematician Bhāskara II.[238][239]
Shrine outer wall and Dravida style superstructure (shikhara) at Siddhesvara Temple at Haveri.
Ornate entrance to the closed hall from the south at Kalleshvara Temple at Bagali.
Shrine wall relief, molding frieze and miniature decorative tower in Mallikarjuna Temple at Kuruvatti.
Rear view showing lateral entrances of the Mahadeva Temple at Itagi.
The late medieval period is marked by repeated invasions of the Muslim Central Asian nomadic clans,[240][241] the rule of the Delhi sultanate, and by the growth of other dynasties and empires, built upon military technology of the Sultanate.[242]
The Delhi Sultanate was a series of successive Islamic states based in Delhi, ruled by several dynasties of Turkic, Turko-Indian[244] and Pashtun origins.[245] It ruled large parts of the Indian subcontinent from the 13th century to the early 16th century.[246] In the 12th and 13th centuries, Central Asian Turks invaded parts of northern India and established the Delhi Sultanate in the former Hindu holdings.[247] The subsequent Mamluk dynasty of Delhi managed to conquer large areas of northern India, while the Khalji dynasty conquered most of central India while forcing the principal Hindu kingdoms of South India to become vassal states.[246]
The Sultanate ushered in a period of Indian cultural renaissance. The resulting "Indo-Muslim" fusion of cultures left lasting syncretic monuments in architecture, music, literature, religion, and clothing. It is surmised that the language of Urdu was born during the Delhi Sultanate period as a result of the intermingling of the local speakers of Sanskritic Prakrits with immigrants speaking Persian, Turkic, and Arabic under the Muslim rulers. The Delhi Sultanate is the only Indo-Islamic empire to enthrone one of the few female rulers in India, Razia Sultana (1236–1240).
During the Delhi Sultanate, there was a synthesis between Indian civilisation and Islamic civilisation[citation needed]. The latter was a cosmopolitan civilisation, with a multicultural and pluralistic society, and wide-ranging international networks, including social and economic networks, spanning large parts of Afro-Eurasia, leading to escalating circulation of goods, peoples, technologies and ideas. While initially disruptive due to the passing of power from native Indian elites to Turkic Muslim elites, the Delhi Sultanate was responsible for integrating the Indian subcontinent into a growing world system, drawing India into a wider international network, which had a significant impact on Indian culture and society.[248] However, the Delhi Sultanate also caused large-scale destruction and desecration of temples in the Indian subcontinent.[249]
The Mongol invasions of India were successfully repelled by the Delhi Sultanate during the rule of Alauddin Khalji. A major factor in their success was their Turkic Mamluk slave army, who were highly skilled in the same style of nomadic cavalry warfare as the Mongols, as a result of having similar nomadic Central Asian roots. It is possible that the Mongol Empire may have expanded into India were it not for the Delhi Sultanate's role in repelling them.[250] By repeatedly repulsing the Mongol raiders, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north.[251][250]
A Turco-Mongol conqueror in Central Asia, Timur (Tamerlane), attacked the reigning Sultan Nasir-u Din Mehmud of the Tughlaq dynasty in the north Indian city of Delhi.[252] The Sultan's army was defeated on 17 December 1398. Timur entered Delhi and the city was sacked, destroyed, and left in ruins after Timur's army had killed and plundered for three days and nights. He ordered the whole city to be sacked except for the sayyids, scholars, and the "other Muslims" (artists); 100,000 war prisoners were put to death in one day.[253] The Sultanate suffered significantly from the sacking of Delhi. Though revived briefly under the Lodi dynasty, it was but a shadow of the former.
Qutb Minar, a UNESCO World Heritage Site, whose construction was begun by Qutb ud-Din Aibak, the first Sultan of Delhi.
Dargahs of Sufi-saint Nizamuddin Auliya, and poet and musician Amir Khusro in Delhi.
The grave of Razia, the Sultana of Delhi, from 1236 CE to 1240 CE, the only female ruler of a major realm on the Indian subcontinent until modern times.[citation needed]
Mausoleum of Ghiyasuddin Tughluq in Tughluqabad.
Lodhi Gardens in Delhi.
The Vijayanagara Empire was established in 1336 by Harihara I and his brother Bukka Raya I of Sangama Dynasty,[254] which originated as a political heir of the Hoysala Empire, Kakatiya Empire,[255] and the Pandyan Empire.[256] The empire rose to prominence as a culmination of attempts by the south Indian powers to ward off Islamic invasions by the end of the 13th century. It lasted until 1646, although its power declined after a major military defeat in 1565 by the combined armies of the Deccan sultanates. The empire is named after its capital city of Vijayanagara, whose ruins surround present day Hampi, now a World Heritage Site in Karnataka, India.[257]
In the first two decades after the founding of the empire, Harihara I gained control over most of the area south of the Tungabhadra river and earned the title of Purvapaschima Samudradhishavara ("master of the eastern and western seas"). By 1374 Bukka Raya I, successor to Harihara I, had defeated the chiefdom of Arcot, the Reddys of Kondavidu, and the Sultan of Madurai and had gained control over Goa in the west and the Tungabhadra-Krishna River doab in the north.[258][259]
With the Vijayanagara Kingdom now imperial in stature, Harihara II, the second son of Bukka Raya I, further consolidated the kingdom beyond the Krishna River and brought the whole of South India under the Vijayanagara umbrella.[260] The next ruler, Deva Raya I, emerged successful against the Gajapatis of Odisha and undertook important works of fortification and irrigation.[261] Italian traveler Niccolo de Conti wrote of him as the most powerful ruler of India.[262] Deva Raya II (called Gajabetekara)[263] succeeded to the throne in 1424 and was possibly the most capable of the Sangama Dynasty rulers.[264] He quelled rebelling feudal lords as well as the Zamorin of Calicut and Quilon in the south. He invaded the island of Sri Lanka and became overlord of the kings of Burma at Pegu and Tanasserim.[265][266][267]
The Vijayanagara Emperors were tolerant of all religions and sects, as writings by foreign visitors show.[268] The kings used titles such as Gobrahamana Pratipalanacharya (literally, "protector of cows and Brahmins") and Hindurayasuratrana (lit, "upholder of Hindu faith") that testified to their intention of protecting Hinduism and yet were at the same time staunchly Islamicate in their court ceremonials and dress.[269] The empire's founders, Harihara I and Bukka Raya I, were devout Shaivas (worshippers of Shiva), but made grants to the Vaishnava order of Sringeri with Vidyaranya as their patron saint, and designated Varaha (the boar, an Avatar of Vishnu) as their emblem.[270] Over one-fourth of the archaeological dig found an "Islamic Quarter" not far from the "Royal Quarter". Nobles from Central Asia's Timurid kingdoms also came to Vijayanagara. The later Saluva and Tuluva kings were Vaishnava by faith, but worshipped at the feet of Lord Virupaksha (Shiva) at Hampi as well as Lord Venkateshwara (Vishnu) at Tirupati. A Sanskrit work, Jambavati Kalyanam by King Krishnadevaraya, called Lord Virupaksha Karnata Rajya Raksha Mani ("protective jewel of Karnata Empire").[271][full citation needed] The kings patronised the saints of the dvaita order (philosophy of dualism) of Madhvacharya at Udupi.[272]
An 1868 photograph of the ruins of the Vijayanagara Empire at Hampi, now a UNESCO World Heritage Site[273]
Gajashaala or elephant's stable, built by the Vijayanagar rulers for their war elephants.[274]
Vijayanagara marketplace at Hampi, along with the sacred tank located on the side of Krishna temple.
Stone temple car in Vitthala Temple at Hampi.
The empire's legacy includes many monuments spread over South India, the best known of which is the group at Hampi. The previous temple building traditions in South India came together in the Vijayanagara Architecture style. The mingling of all faiths and vernaculars inspired architectural innovation of Hindu temple construction, first in the Deccan and later in the Dravidian idioms using the local granite. South Indian mathematics flourished under the protection of the Vijayanagara Empire in Kerala. The south Indian mathematician Madhava of Sangamagrama founded the famous Kerala School of Astronomy and Mathematics in the 14th century which produced a lot of great south Indian mathematicians like Parameshvara, Nilakantha Somayaji and Jyeṣṭhadeva in medieval south India.[275] Efficient administration and vigorous overseas trade brought new technologies such as water management systems for irrigation.[276] The empire's patronage enabled fine arts and literature to reach new heights in Kannada, Telugu, Tamil, and Sanskrit, while Carnatic music evolved into its current form.[277]
Vijayanagara went into decline after the defeat in the Battle of Talikota (1565). After the death of Aliya Rama Raya in the Battle of Talikota, Tirumala Deva Raya started the Aravidu dynasty, moved and founded a new capital of Penukonda to replace the destroyed Hampi, and attempted to reconstitute the remains of Vijayanagara Empire.[278] Tirumala abdicated in 1572, dividing the remains of his kingdom to his three sons, and pursued a religious life until his death in 1578. The Aravidu dynasty successors ruled the region but the empire collapsed in 1614, and the final remains ended in 1646, from continued wars with the Bijapur sultanate and others.[279][280][281] During this period, more kingdoms in South India became independent and separate from Vijayanagara. These include the Mysore Kingdom, Keladi Nayaka, Nayaks of Madurai, Nayaks of Tanjore, Nayakas of Chitradurga and Nayak Kingdom of Gingee – all of which declared independence and went on to have a significant impact on the history of South India in the coming centuries.[282]
Vijaya Stambha (Tower of Victory)
Temple inside Chittorgarh fort
The Man Singh (Manasimha) palace at the Gwalior fort
Chinese manuscript Tribute Giraffe with Attendant, depicting a giraffe presented by Bengali envoys in the name of Sultan Saifuddin Hamza Shah of Bengal to the Yongle Emperor of Ming China.
Mahmud Gawan Madrasa was built by Mahmud Gawan, the Wazir of the Bahmani Sultanate as the centre of religious as well as secular education.
15th century copper plate grant of Gajapati king Purushottama Deva
For two and a half centuries from the mid-13th century, politics in Northern India was dominated by the Delhi Sultanate, and in Southern India by the Vijayanagar Empire. However, there were other regional powers present as well. After fall of Pala Empire, the Chero dynasty ruled much of Eastern Uttar Pradesh, Bihar and Jharkhand from 12th CE to 18th CE.[283][284][285] The Reddy dynasty successfully defeated the Delhi Sultanate; and extended their rule from Cuttack in the north to Kanchi in the south, eventually being absorbed into the expanding Vijayanagara Empire.[286]
In the north, the Rajput kingdoms remained the dominant force in Western and Central India. The Mewar dynasty under Maharana Hammir defeated and captured Muhammad Tughlaq with the Bargujars as his main allies. Tughlaq had to pay a huge ransom and relinquish all of Mewar's lands. After this event, the Delhi Sultanate did not attack Chittor for a few hundred years. The Rajputs re-established their independence, and Rajput states were established as far east as Bengal and north into the Punjab. The Tomaras established themselves at Gwalior, and Man Singh Tomar reconstructed the Gwalior Fort which still stands there.[287] During this period, Mewar emerged as the leading Rajput state; and Rana Kumbha expanded his kingdom at the expense of the Sultanates of Malwa and Gujarat.[287][288] The next great Rajput ruler, Rana Sanga of Mewar, became the principal player in Northern India. His objectives grew in scope – he planned to conquer the much sought after prize of the Muslim rulers of the time, Delhi. But, his defeat in the Battle of Khanwa consolidated the new Mughal dynasty in India.[287] The Mewar dynasty under Maharana Udai Singh II faced further defeat by Mughal emperor Akbar, with their capital Chittor being captured. Due to this event, Udai Singh II founded Udaipur, which became the new capital of the Mewar kingdom. His son, Maharana Pratap of Mewar, firmly resisted the Mughals. Akbar sent many missions against him. He survived to ultimately gain control of all of Mewar, excluding the Chittor Fort.[289]
In the south, the Bahmani Sultanate, which was established either by a Brahman convert or patronised by a Brahman and from that source it was given the name Bahmani,[290] was the chief rival of the Vijayanagara, and frequently created difficulties for the Vijayanagara.[291] In the early 16th century Krishnadevaraya of the Vijayanagar Empire defeated the last remnant of Bahmani Sultanate power. After which, the Bahmani Sultanate collapsed,[292] resulting it being split into five small Deccan sultanates.[293] In 1490, Ahmadnagar declared independence, followed by Bijapur and Berar in the same year; Golkonda became independent in 1518 and Bidar in 1528.[294] Although generally rivals, they did ally against the Vijayanagara Empire in 1565, permanently weakening Vijayanagar in the Battle of Talikota.
In the East, the Gajapati Kingdom remained a strong regional power to reckon with, associated with a high point in the growth of regional culture and architecture. Under Kapilendradeva, Gajapatis became an empire stretching from the lower Ganga in the north to the Kaveri in the south.[295] In Northeast India, the Ahom Kingdom was a major power for six centuries;[296][297] led by Lachit Borphukan, the Ahoms decisively defeated the Mughal army at the Battle of Saraighat during the Ahom-Mughal conflicts.[298] Further east in Northeastern India was the Kingdom of Manipur, which ruled from their seat of power at Kangla Fort and developed a sophisticated Hindu Gaudiya Vaishnavite culture.[299][300][301]
The Sultanate of Bengal was the dominant power of the Ganges–Brahmaputra Delta, with a network of mint towns spread across the region. It was a Sunni Muslim monarchy with Indo-Turkic, Arab, Abyssinian and Bengali Muslim elites. The sultanate was known for its religious pluralism where non-Muslim communities co-existed peacefully. The Bengal Sultanate had a circle of vassal states, including Odisha in the southwest, Arakan in the southeast, and Tripura in the east. In the early 16th century, the Bengal Sultanate reached the peak of its territorial growth with control over Kamrup and Kamata in the northeast and Jaunpur and Bihar in the west. It was reputed as a thriving trading nation and one of Asia's strongest states.The Bengal Sultanate was described by contemporary European and Chinese visitors as a relatively prosperous kingdom. Due to the abundance of goods in Bengal, the region was described as the "richest country to trade with". The Bengal Sultanate left a strong architectural legacy. Buildings from the period show foreign influences merged into a distinct Bengali style. The Bengal Sultanate was also the largest and most prestigious authority among the independent medieval Muslim-ruled states in the history of Bengal. Its decline began with an interregnum by the Suri Empire, followed by Mughal conquest and disintegration into petty kingdoms.
The Bhakti movement refers to the theistic devotional trend that emerged in medieval Hinduism[302] and later revolutionised in Sikhism.[303] It originated in the seventh-century south India (now parts of Tamil Nadu and Kerala), and spread northwards.[302] It swept over east and north India from the 15th century onwards, reaching its zenith between the 15th and 17th century CE.[304]
Rang Ghar, built by Pramatta Singha in Ahom kingdom's capital Rangpur, is one of the earliest pavilions of outdoor stadia in the Indian subcontinent.
Chittor Fort is the largest fort on the Indian subcontinent; it is one of the six Hill Forts of Rajasthan.
Ranakpur Jain temple was built in the 15th century with the support of the Rajput state of Mewar.
Gol Gumbaz built by the Bijapur Sultanate, has the second largest pre-modern dome in the world after the Byzantine Hagia Sophia.
The early modern period of Indian history is dated from 1526 CE to 1858 CE, corresponding to the rise and fall of the Mughal Empire, which inherited from the Timurid Renaissance. During this age India's economy expanded, relative peace was maintained and arts were patronized. This period witnessed the further development of Indo-Islamic architecture;[314][315] the growth of Mahrattas and Sikhs enabled them to rule significant regions of India in the waning days of the Mughal empire, which formally came to an end when the British Raj was founded.[22] With the discovery of the Cape route in the 1500s, the first Europeans to arrive by sea and establish themselves, were the Portuguese in Goa and Bombay.[316]
In 1526, Babur, a Timurid descendant of Timur and Genghis Khan from Fergana Valley (modern day Uzbekistan), swept across the Khyber Pass and established the Mughal Empire, which at its zenith covered much of South Asia.[318] However, his son Humayun was defeated by the Afghan warrior Sher Shah Suri in the year 1540, and Humayun was forced to retreat to Kabul. After Sher Shah's death, his son Islam Shah Suri and his Hindu general Hemu Vikramaditya established secular rule in North India from Delhi until 1556, when Akbar (r. 1556–1605), grandson of Babur, defeated Hemu in the Second Battle of Panipat on 6 November 1556 after winning Battle of Delhi. Akbar tried to establish a good relationship with the Hindus. Akbar declared "Amari" or non-killing of animals in the holy days of Jainism. He rolled back the jizya tax for non-Muslims. The Mughal emperors married local royalty, allied themselves with local maharajas, and attempted to fuse their Turko-Persian culture with ancient Indian styles, creating a unique Indo-Persian culture and Indo-Saracenic architecture.
Akbar married a Rajput princess, Mariam-uz-Zamani, and they had a son, Jahangir (r. 1605–1627), who was part-Mughal and part-Rajput, as were future Mughal emperors.[319] Jahangir more or less followed his father's policy. The Mughal dynasty ruled most of the Indian subcontinent by 1600. The reign of Shah Jahan (r. 1628–1658) was the golden age of Mughal architecture. He erected several large monuments, the most famous of which is the Taj Mahal at Agra, as well as the Moti Masjid in Agra, the Red Fort, the Jama Masjid, Delhi, and the Lahore Fort.
It was one of the largest empires to have existed in the Indian subcontinent,[320] and surpassed China to become the world's largest economic power, controlling 24.4% of the world economy,[321] and the world leader in manufacturing,[322] producing 25% of global industrial output.[323] The economic and demographic upsurge was stimulated by Mughal agrarian reforms that intensified agricultural production,[324] a proto-industrializing economy that began moving towards industrial manufacturing,[325] and a relatively high degree of urbanisation for its time.[326]
The Agra Fort showing the river Yamuna and the Taj Mahal in the background
Fatehpur Sikri, near Agra, showing Buland Darwaza, the complex built by Akbar, the third Mughal emperor.
Humayun's Tomb in Delhi, built in 1570 CE.
The Red Fort, Delhi, its construction begun in 1639 CE, and ended in 1648 CE.
The Mughal Empire reached the zenith of its territorial expanse during the reign of Aurangzeb (r. 1658–1707), under whose reign the proto-industrialization[327] was waved and India surpassed Qing China in becoming the world's largest economy.[328][329] Aurangzeb was less tolerant than his predecessors, reintroducing the jizya tax and destroying several historical temples, while at the same time building more Hindu temples than he destroyed,[330] employing significantly more Hindus in his imperial bureaucracy than his predecessors, and advancing administrators based on their ability rather than their religion.[331] However, he is often blamed for the erosion of the tolerant syncretic tradition of his predecessors, as well as increasing religious controversy and centralisation. The English East India Company suffered a defeat at the Anglo-Mughal War.[332][333]
The empire went into decline thereafter. The Mughals suffered several blows due to invasions from Marathas, Rajputs, Jats and Afghans. In 1737, the Maratha general Bajirao of the Maratha Empire invaded and plundered Delhi. Under the general Amir Khan Umrao Al Udat, the Mughal Emperor sent 8,000 troops to drive away the 5,000 Maratha cavalry soldiers. Baji Rao, however, easily routed the novice Mughal general and the rest of the imperial Mughal army fled. In 1737, in the final defeat of Mughal Empire, the commander-in-chief of the Mughal Army, Nizam-ul-mulk, was routed at Bhopal by the Maratha army. This essentially brought an end to the Mughal Empire. While Bharatpur State under Jat ruler Suraj Mal, overran the Mughal garrison at Agra and plundered the city taking with them the two great silver doors of the entrance of the famous Taj Mahal; which were then melted down by Suraj Mal in 1761.[334] In 1739, Nader Shah, emperor of Iran, defeated the Mughal army at the Battle of Karnal.[335] After this victory, Nader captured and sacked Delhi, carrying away many treasures, including the Peacock Throne.[336] Mughal rule was further weakened by constant native Indian resistance; Banda Singh Bahadur led the Sikh Khalsa against Mughal religious oppression; Hindu Rajas of Bengal, Pratapaditya and Raja Sitaram Ray revolted; and Maharaja Chhatrasal, of Bundela Rajputs, fought the Mughals and established the Panna State.[337] The Mughal dynasty was reduced to puppet rulers by 1757. Vadda Ghalughara took place under the Muslim provincial government based at Lahore to wipe out the Sikhs, with 30,000 Sikhs being killed, an offensive that had begun with the Mughals, with the Chhota Ghallughara,[338] and lasted several decades under its Muslim successor states.[339]
The Maratha kingdom was founded and consolidated by Chatrapati Shivaji, a Maratha aristocrat of the Bhonsle clan.[340] However, the credit for making the Marathas formidable power nationally goes to Peshwa (chief minister) Bajirao I. Historian K.K. Datta wrote that Bajirao I "may very well be regarded as the second founder of the Maratha Empire".[341]
In the early 18th century, under the Peshwas, the Marathas consolidated and ruled over much of South Asia. The Marathas are credited to a large extent for ending Mughal rule in India.[342][343][344] In 1737, the Marathas defeated a Mughal army in their capital, in the Battle of Delhi. The Marathas continued their military campaigns against the Mughals, Nizam, Nawab of Bengal and the Durrani Empire to further extend their boundaries. By 1760, the domain of the Marathas stretched across most of the Indian subcontinent.[citation needed] The Marathas even attempted to capture Delhi and discussed putting Vishwasrao Peshwa on the throne there in place of the Mughal emperor.[345]
The Maratha empire at its peak stretched from Tamil Nadu[346] in the south, to Peshawar (modern-day Khyber Pakhtunkhwa, Pakistan[347] [note 3]) in the north, and Bengal in the east. The Northwestern expansion of the Marathas was stopped after the Third Battle of Panipat (1761). However, the Maratha authority in the north was re-established within a decade under Peshwa Madhavrao I.[349]
Under Madhavrao I, the strongest knights were granted semi-autonomy, creating a confederacy of United Maratha states under the Gaekwads of Baroda, the Holkars of Indore and Malwa, the Scindias of Gwalior and Ujjain, the Bhonsales of Nagpur and the Puars of Dhar and Dewas. In 1775, the East India Company intervened in a Peshwa family succession struggle in Pune, which led to the First Anglo-Maratha War, resulting in a Maratha victory.[350] The Marathas remained a major power in India until their defeat in the Second and Third Anglo-Maratha Wars (1805–1818), which resulted in the East India Company controlling most of India.
The Sikh Empire, ruled by members of the Sikh religion, was a political entity that governed the Northwestern regions of the Indian subcontinent. The empire, based around the Punjab region, existed from 1799 to 1849. It was forged, on the foundations of the Khalsa, under the leadership of Maharaja Ranjit Singh (1780–1839) from an array of autonomous Punjabi Misls of the Sikh Confederacy.[citation needed]
Maharaja Ranjit Singh consolidated many parts of northern India into an empire. He primarily used his Sikh Khalsa Army that he trained in European military techniques and equipped with modern military technologies. Ranjit Singh proved himself to be a master strategist and selected well-qualified generals for his army. He continuously defeated the Afghan armies and successfully ended the Afghan-Sikh Wars. In stages, he added central Punjab, the provinces of Multan and Kashmir, and the Peshawar Valley to his empire.[352][353]
At its peak, in the 19th century, the empire extended from the Khyber Pass in the west, to Kashmir in the north, to Sindh in the south, running along Sutlej river to Himachal in the east. After the death of Ranjit Singh, the empire weakened, leading to conflict with the British East India Company. The hard-fought First Anglo-Sikh War and Second Anglo-Sikh War marked the downfall of the Sikh Empire, making it among the last areas of the Indian subcontinent to be conquered by the British.
The Kingdom of Mysore in southern India expanded to its greatest extent under Hyder Ali and his son Tipu Sultan in the later half of the 18th century. Under their rule, Mysore fought series of wars against the Marathas and British or their combined forces. The Maratha–Mysore War ended in April 1787, following the finalizing of treaty of Gajendragad, in which, Tipu Sultan was obligated to pay tribute to the Marathas. Concurrently, the Anglo-Mysore Wars took place, where the Mysoreans used the Mysorean rockets. The Fourth Anglo-Mysore War (1798–1799) saw the death of Tipu. Mysore's alliance with the French was seen as a threat to the British East India Company, and Mysore was attacked from all four sides. The Nizam of Hyderabad and the Marathas launched an invasion from the north. The British won a decisive victory at the Siege of Seringapatam (1799).
Hyderabad was founded by the Qutb Shahi dynasty of Golconda in 1591. Following a brief Mughal rule, Asif Jah, a Mughal official, seized control of Hyderabad and declared himself Nizam-al-Mulk of Hyderabad in 1724. The Nizams lost considerable territory and paid tribute to the Maratha Empire after being routed in multiple battles, such as the Battle of Palkhed.[354] However, the Nizams maintained their sovereignty from 1724 until 1948 through paying tributes to the Marathas, and later, being vassels of the British. Hyderabad State became a princely state in British India in 1798.
The Nawabs of Bengal had become the de facto rulers of Bengal following the decline of Mughal Empire. However, their rule was interrupted by Marathas who carried out six expeditions in Bengal from 1741 to 1748, as a result of which Bengal became a tributary state of Marathas. On 23 June 1757, Siraj ud-Daulah, the last independent Nawab of Bengal was betrayed in the Battle of Plassey by Mir Jafar. He lost to the British, who took over the charge of Bengal in 1757, installed Mir Jafar on the Masnad (throne) and established itself to a political power in Bengal.[355] In 1765 the system of Dual Government was established, in which the Nawabs ruled on behalf of the British and were mere puppets to the British. In 1772 the system was abolished and Bengal was brought under the direct control of the British. In 1793, when the Nizamat (governorship) of the Nawab was also taken away from them, they remained as the mere pensioners of the British East India Company.[356][357]
In the 18th century, the whole of Rajputana was virtually subdued by the Marathas. The Second Anglo-Maratha War distracted the Marathas from 1807 to 1809, but afterward Maratha domination of Rajputana resumed. In 1817, the British went to war with the Pindaris, raiders who were fled in Maratha territory, which quickly became the Third Anglo-Maratha War, and the British government offered its protection to the Rajput rulers from the Pindaris and the Marathas. By the end of 1818 similar treaties had been executed between the other Rajput states and Britain. The Maratha Sindhia ruler of Gwalior gave up the district of Ajmer-Merwara to the British, and Maratha influence in Rajasthan came to an end.[358] Most of the Rajput princes remained loyal to Britain in the Revolt of 1857, and few political changes were made in Rajputana until Indian independence in 1947. The Rajputana Agency contained more than 20 princely states, most notable being Udaipur State, Jaipur State, Bikaner State and Jodhpur State.
After the fall of the Maratha Empire, many Maratha dynasties and states became vassals in a subsidiary alliance with the British, to form the largest bloc of princely states in the British Raj, in terms of territory and population.[citation needed] With the decline of the Sikh Empire, after the First Anglo-Sikh War in 1846, under the terms of the Treaty of Amritsar, the British government sold Kashmir to Maharaja Gulab Singh and the princely state of Jammu and Kashmir, the second-largest princely state in British India, was created by the Dogra dynasty.[359][360] While in Eastern and Northeastern India, the Hindu and Buddhist states of Cooch Behar Kingdom, Twipra Kingdom and Kingdom of Sikkim were annexed by the British and made vassal princely state.
After the fall of the Vijayanagara Empire, Polygar states emerged in Southern India; and managed to weather invasions and flourished until the Polygar Wars, where they were defeated by the British East India Company forces.[361] Around the 18th century, the Kingdom of Nepal was formed by Rajput rulers.[362]
In 1498, a Portuguese fleet under Vasco da Gama discovered a new sea route from Europe to India, which paved the way for direct Indo-European commerce. The Portuguese soon set up trading posts in Velha Goa, Damaon, Dio island, and Bombay. After their conquest in Goa, the Portuguese instituted the Goa Inquisition, where new Indian converts were punished for suspected heresy against Christianity and non-Christians were condemned for discouraging those considering conversion or for convincing others to renounce Christianity.[363] Goa remained the main Portuguese territory until it was annexed by India in 1961.[364][page needed]
The next to arrive were the Dutch, with their main base in Ceylon. They established ports in Malabar. However, their expansion into India was halted after their defeat in the Battle of Colachel by the Kingdom of Travancore during the Travancore-Dutch War. The Dutch never recovered from the defeat and no longer posed a large colonial threat to India.[365][366]
The internal conflicts among Indian kingdoms gave opportunities to the European traders to gradually establish political influence and appropriate lands. Following the Dutch, the British—who set up in the west coast port of Surat in 1619—and the French both established trading outposts in India. Although these continental European powers controlled various coastal regions of southern and eastern India during the ensuing century, they eventually lost all their territories in India to the British, with the exception of the French outposts of Pondichéry and Chandernagore,[367][368] and the Portuguese colonies of Goa, Damaon& Diu.[369]
The English East India Company was founded in 1600 as The Company of Merchants of London Trading into the East Indies. It gained a foothold in India with the establishment of a factory in Masulipatnam on the Eastern coast of India in 1611 and a grant of rights by the Mughal emperor Jahangir to establish a factory in Surat in 1612. In 1640, after receiving similar permission from the Vijayanagara ruler farther south, a second factory was established in Madras on the southeastern coast. The islet of Bom Bahia in present-day Mumbai (Bombay), was a Portuguese outpost not far from Surat, it was presented to Charles II of England as dowry, in his marriage to Catherine of Braganza, Charles in turn leased Bombay to the Company in 1668. Two decades later, the company established a trade post in the River Ganges delta, when a factory was set up in Calcutta (Kolkata). During this time other companies established by the Portuguese, Dutch, French, and Danish were similarly expanding in the sub-continent.
The company's victory under Robert Clive in the 1757 Battle of Plassey and another victory in the 1764 Battle of Buxar (in Bihar), consolidated the company's power, and forced emperor Shah Alam II to appoint it the diwan, or revenue collector, of Bengal, Bihar, and Orissa. The company thus became the de facto ruler of large areas of the lower Gangetic plain by 1773. It also proceeded by degrees to expand its dominions around Bombay and Madras. The Anglo-Mysore Wars (1766–99) and the Anglo-Maratha Wars (1772–1818) left it in control of large areas of India south of the Sutlej River. With the defeat of the Marathas, no native power represented a threat for the company any longer.[370]
The expansion of the company's power chiefly took two forms. The first of these was the outright annexation of Indian states and subsequent direct governance of the underlying regions that collectively came to comprise British India. The annexed regions included the North-Western Provinces (comprising Rohilkhand, Gorakhpur, and the Doab) (1801), Delhi (1803), Assam (Ahom Kingdom 1828) and Sindh (1843). Punjab, North-West Frontier Province, and Kashmir were annexed after the Anglo-Sikh Wars in 1849–56 (Period of tenure of Marquess of Dalhousie Governor General). However, Kashmir was immediately sold under the Treaty of Amritsar (1850) to the Dogra Dynasty of Jammu and thereby became a princely state. In 1854, Berar was annexed along with the state of Oudh two years later.[371]
Warren Hastings, the first governor-general of Fort William (Bengal) who oversaw the company's territories in India.
Gold coin, minted 1835, with obverse showing the bust of William IV, king of United Kingdom from 26 June 1830 to 20 June 1837, and reverse marked "Two mohurs" in English (do ashrafi in Urdu) issued during Company rule in India
Photograph (1855) showing the construction of the Bhor Ghaut incline bridge, Bombay; the incline was conceived by George Clark, the Chief Engineer in the East India Company's Government of Bombay.
Watercolor (1863) titled, The Ganges Canal, Roorkee, Saharanpur District (U.P.). The canal was the brainchild of Sir Proby Cautley; construction began in 1840, and the canal was opened by Governor-General Lord Dalhousie in April 1854
The second form of asserting power involved treaties in which Indian rulers acknowledged the company's hegemony in return for limited internal autonomy. Since the company operated under financial constraints, it had to set up political underpinnings for its rule.[372] The most important such support came from the subsidiary alliances with Indian princes during the first 75 years of Company rule.[372] In the early 19th century, the territories of these princes accounted for two-thirds of India.[372] When an Indian ruler who was able to secure his territory wanted to enter such an alliance, the company welcomed it as an economical method of indirect rule that did not involve the economic costs of direct administration or the political costs of gaining the support of alien subjects.[373]
In return, the company undertook the "defense of these subordinate allies and treated them with traditional respect and marks of honor."[373] Subsidiary alliances created the Princely States of the Hindu maharajas and the Muslim nawabs. Prominent among the princely states were Cochin (1791), Jaipur (1794), Travancore (1795), Hyderabad (1798), Mysore (1799), Cis-Sutlej Hill States (1815), Central India Agency (1819), Cutch and Gujarat Gaikwad territories (1819), Rajputana (1818) and Bahawalpur (1833).[371]
The Indian indenture system was an ongoing system of indenture, a form of debt bondage, by which 3.5 million Indians were transported to various colonies of European powers to provide labor for the (mainly sugar) plantations. It started from the end of slavery in 1833 and continued until 1920. This resulted in the development of a large Indian diaspora that spread from the Caribbean (e.g. Trinidad and Tobago) to the Pacific Ocean (e.g. Fiji) and the growth of large Indo-Caribbean and Indo-African populations.
Charles Canning, the Governor-General of India during the rebellion.
Lord Dalhousie, the Governor-General of India from 1848 to 1856, who devised the Doctrine of Lapse.
Lakshmibai, the Rani of Jhansi, one of the principal leaders of the rebellion who earlier had lost her kingdom as a result of the Doctrine of lapse.
Bahadur Shah Zafar the last Mughal Emperor, crowned Emperor of India by the rebels, he was deposed by the British, and died in exile in Burma
The Indian rebellion of 1857 was a large-scale rebellion by soldiers employed by the British East India Company in northern and central India against the company's rule. The spark that led to the mutiny was the issue of new gunpowder cartridges for the Enfield rifle, which was insensitive to local religious prohibition. The key mutineer was Mangal Pandey.[374] In addition, the underlying grievances over British taxation, the ethnic gulf between the British officers and their Indian troops and land annexations played a significant role in the rebellion. Within weeks after Pandey's mutiny, dozens of units of the Indian army joined peasant armies in widespread rebellion. The rebel soldiers were later joined by Indian nobility, many of whom had lost titles and domains under the Doctrine of Lapse and felt that the company had interfered with a traditional system of inheritance. Rebel leaders such as Nana Sahib and the Rani of Jhansi belonged to this group.[375]
After the outbreak of the mutiny in Meerut, the rebels very quickly reached Delhi. The rebels had also captured large tracts of the North-Western Provinces and Awadh (Oudh). Most notably, in Awadh, the rebellion took on the attributes of a patriotic revolt against British presence.[376] However, the British East India Company mobilised rapidly with the assistance of friendly Princely states, but it took the British the remainder of 1857 and the better part of 1858 to suppress the rebellion. Due to the rebels being poorly equipped and having no outside support or funding, they were brutally subdued by the British.[377]
In the aftermath, all power was transferred from the British East India Company to the British Crown, which began to administer most of India as a number of provinces. The Crown controlled the company's lands directly and had considerable indirect influence over the rest of India, which consisted of the Princely states ruled by local royal families. There were officially 565 princely states in 1947, but only 21 had actual state governments, and only three were large (Mysore, Hyderabad, and Kashmir). They were absorbed into the independent nation in 1947–48.[378]
After 1857, the colonial government strengthened and expanded its infrastructure via the court system, legal procedures, and statutes. The Indian Penal Code came into being.[379] In education, Thomas Babington Macaulay had made schooling a priority for the Raj in his famous minute of February 1835 and succeeded in implementing the use of English as the medium of instruction. By 1890 some 60,000 Indians had matriculated.[380] The Indian economy grew at about 1% per year from 1880 to 1920, and the population also grew at 1%. However, from 1910s Indian private industry began to grow significantly. India built a modern railway system in the late 19th century which was the fourth largest in the world.[381] The British Raj invested heavily in infrastructure, including canals and irrigation systems in addition to railways, telegraphy, roads and ports.[382] However, historians have been bitterly divided on issues of economic history, with the Nationalist school arguing that India was poorer at the end of British rule than at the beginning and that impoverishment occurred because of the British.[383]
In 1905, Lord Curzon split the large province of Bengal into a largely Hindu western half and "Eastern Bengal and Assam", a largely Muslim eastern half. The British goal was said to be for efficient administration but the people of Bengal were outraged at the apparent "divide and rule" strategy. It also marked the beginning of the organised anti-colonial movement. When the Liberal party in Britain came to power in 1906, he was removed. Bengal was reunified in 1911. The new Viceroy Gilbert Minto and the new Secretary of State for India John Morley consulted with Congress leaders on political reforms. The Morley-Minto reforms of 1909 provided for Indian membership of the provincial executive councils as well as the Viceroy's executive council. The Imperial Legislative Council was enlarged from 25 to 60 members and separate communal representation for Muslims was established in a dramatic step towards representative and responsible government.[384] Several socio-religious organisations came into being at that time. Muslims set up the All India Muslim League in 1906. It was not a mass party but was designed to protect the interests of the aristocratic Muslims. It was internally divided by conflicting loyalties to Islam, the British, and India, and by distrust of Hindus.[citation needed] The Hindu Mahasabha and Rashtriya Swayamsevak Sangh (RSS) sought to represent Hindu interests though the latter always claimed it to be a "cultural" organisation.[385] Sikhs founded the Shiromani Akali Dal in 1920.[386] However, the largest and oldest political party Indian National Congress, founded in 1885, attempted to keep a distance from the socio-religious movements and identity politics.[387]
Two silver rupee coins issued by the British Raj in 1862 and 1886 respectively, the first in obverse showing a bust of Victoria, Queen, the second of Victoria, Empress.  Victoria became Empress of India in 1876.
Ronald Ross, left, at Cunningham's laboratory of Presidency Hospital in Calcutta, where the transmission of malaria by mosquitoes was discovered, winning Ross the second Nobel Prize for Physiology or Medicine in 1902.
A Darjeeling Himalayan Railway train shown in 1870.  The railway became a UNESCO World Heritage Site in 1999.
A second-day cancellation of the stamps issued in February 1931 to commemorate the inauguration of New Delhi as the capital of the British Indian Empire. Between 1858 and 1911, Calcutta had been the capital of the Raj
Sir Syed Ahmad Khan (1817–1898), the author of Causes of the Indian Mutiny, was the founder of Muhammadan Anglo-Oriental College, later the Aligarh Muslim University.
Pandita Ramabai (1858–1922) was a social reformer, and a pioneer in the education and emancipation of women in India.
Rabindranath Tagore (1861–1941) was a Bengali language poet, short-story writer, and playwright, and in addition a music composer and painter, who won the Nobel prize for Literature in 1913.
Srinivasa Ramanujan (1887–1920) was an Indian mathematician who made seminal contributions to number theory.
The Bengali Renaissance refers to a social reform movement, dominated by Bengali Hindus, in the Bengal region of the Indian subcontinent during the nineteenth and early twentieth centuries, a period of British rule. Historian Nitish Sengupta describes the renaissance as having started with reformer and humanitarian Raja Ram Mohan Roy (1775–1833), and ended with Asia's first Nobel laureate Rabindranath Tagore (1861–1941).[388] This flowering of religious and social reformers, scholars, and writers is described by historian David Kopf as "one of the most creative periods in Indian history."[389]
During this period, Bengal witnessed an intellectual awakening that is in some way similar to the Renaissance. This movement questioned existing orthodoxies, particularly with respect to women, marriage, the dowry system, the caste system, and religion. One of the earliest social movements that emerged during this time was the Young Bengal movement, which espoused rationalism and atheism as the common denominators of civil conduct among upper caste educated Hindus.[390] It played an important role in reawakening Indian minds and intellect across the Indian subcontinent.
Map of famines in India 1800–1885
Engraving from The Graphic, October 1877, showing the plight of animals as well as humans in Bellary district, Madras Presidency, British India during the Great Famine of 1876–1878.
Government famine relief, Ahmedabad, India, during the Indian famine of 1899–1900.
A picture of orphans who survived the Bengal famine of 1943
During British East India Company and British Crown rule, India experienced some of deadliest ever recorded famines. These famines, usually resulting from crop failures due to El Niño and often exacerbated by policies of the colonial government,[391] included the Great Famine of 1876–1878 in which 6.1 million to 10.3 million people died,[392] the Great Bengal famine of 1770 where between 1 and 10 million people died,[393][394] the Indian famine of 1899–1900 in which 1.25 to 10 million people died,[391] and the Bengal famine of 1943 where between 2.1 and 3.8 million people died.[395] The Third plague pandemic in the mid-19th century killed 10 million people in India.[396] Despite persistent diseases and famines, the population of the Indian subcontinent, which stood at up to 200 million in 1750,[397] had reached 389 million by 1941.[398]
Indian Cavalry on the Western front 1914.
Indian cavalry from the Deccan Horse during the Battle of Bazentin Ridge in 1916.
Indian Army gunners (probably 39th Battery) with 3.7-inch mountain howitzers, Jerusalem 1917.
India Gate is a memorial to 70,000 soldiers of the British Indian Army who died in the period 1914–21 in the First World War.
During World War I, over 800,000 volunteered for the army, and more than 400,000 volunteered for non-combat roles, compared with the pre-war annual recruitment of about 15,000 men.[399] The Army saw action on the Western Front within a month of the start of the war at the First Battle of Ypres. After a year of front-line duty, sickness and casualties had reduced the Indian Corps to the point where it had to be withdrawn. Nearly 700,000 Indians fought the Turks in the Mesopotamian campaign. Indian formations were also sent to East Africa, Egypt, and Gallipoli.[400]
Indian Army and Imperial Service Troops fought during the Sinai and Palestine Campaign's defence of the Suez Canal in 1915, at Romani in 1916 and to Jerusalem in 1917. India units occupied the Jordan Valley and after the German spring offensive they became the major force in the Egyptian Expeditionary Force during the Battle of Megiddo and in the Desert Mounted Corps' advance to Damascus and on to Aleppo. Other divisions remained in India guarding the North-West Frontier and fulfilling internal security obligations.
One million Indian troops served abroad during the war. In total, 74,187 died,[401] and another 67,000 were wounded.[402] The roughly 90,000 soldiers who died fighting in World War I and the Afghan Wars are commemorated by the India Gate.
General Claude Auchinleck (right), Commander-in-Chief of the Indian Army, with the then Viceroy Wavell (centre) and General Montgomery (left)
Indian women training for Air Raid Precautions (ARP) duties in Bombay, 1942
Indian infantrymen of the 7th Rajput Regiment about to go on patrol on the Arakan front in Burma, 1944.
The stamp series "Victory" issued by the Government of British India to commemorate allied victory in World War II.
British India officially declared war on Nazi Germany in September 1939.[403] The British Raj, as part of the Allied Nations, sent over two and a half million volunteer soldiers to fight under British command against the Axis powers. Additionally, several Indian Princely States provided large donations to support the Allied campaign during the War. India also provided the base for American operations in support of China in the China Burma India Theatre.
Indians fought with distinction throughout the world, including in the European theatre against Germany, in North Africa against Germany and Italy, against the Italians in East Africa, in the Middle East against the Vichy French, in the South Asian region defending India against the Japanese and fighting the Japanese in Burma. Indians also aided in liberating British colonies such as Singapore and Hong Kong after the Japanese surrender in August 1945. Over 87,000 soldiers from the subcontinent died in World War II.
The Indian National Congress, denounced Nazi Germany but would not fight it or anyone else until India was independent. Congress launched the Quit India Movement in August 1942, refusing to co-operate in any way with the government until independence was granted. The government was ready for this move. It immediately arrested over 60,000 national and local Congress leaders. The Muslim League rejected the Quit India movement and worked closely with the Raj authorities.
Subhas Chandra Bose (also called Netaji) broke with Congress and tried to form a military alliance with Germany or Japan to gain independence. The Germans assisted Bose in the formation of the Indian Legion;[404] however, it was Japan that helped him revamp the Indian National Army (INA), after the First Indian National Army under Mohan Singh was dissolved. The INA fought under Japanese direction, mostly in Burma.[405] Bose also headed the Provisional Government of Free India (or Azad Hind), a government-in-exile based in Singapore.[406][407] The government of Azad Hind had its own currency, court, and civil code; and in the eyes of some Indians its existence gave a greater legitimacy to the independence struggle against the British.[citation needed]
By 1942, neighbouring Burma was invaded by Japan, which by then had already captured the Indian territory of Andaman and Nicobar Islands. Japan gave nominal control of the islands to the Provisional Government of Free India on 21 October 1943, and in the following March, the Indian National Army with the help of Japan crossed into India and advanced as far as Kohima in Nagaland. This advance on the mainland of the Indian subcontinent reached its farthest point on Indian territory, retreating from the Battle of Kohima in June and from that of Imphal on 3 July 1944.
The region of Bengal in British India suffered a devastating famine during 1940–1943. An estimated 2.1–3 million died from the famine, frequently characterised as "man-made",[408] with most sources asserting that wartime colonial policies exacerbated the crisis.[409]
The first session of the Indian National Congress in 1885.  A. O. Hume, the founder, is shown in the middle (third row from the front).  The Congress was the first modern nationalist movement to emerge in the British Empire in Asia and Africa.[410]
Surya Sen, leader of the Chittagong armoury raid, a raid on 18 April 1930 on the armoury of police and auxiliary forces in Chittagong, Bengal, now Bangladesh
Front page of the Tribune (25 March 1931), reporting the execution of Bhagat Singh, Rajguru and Sukhdev by the British for the murder of 21-year-old police officer J. P. Saunders. Bhagat Singh quickly became a folk hero of the Indian independence movement.
From the late 19th century, and especially after 1920, under the leadership of Mahatma Gandhi (right), the Congress became the principal leader of the Indian independence movement.[411]  Gandhi is shown here with Jawaharlal Nehru, later the first prime minister of India.
The numbers of British in India were small,[412] yet they were able to rule 52% of the Indian subcontinent directly and exercise considerable leverage over the princely states that accounted for 48% of the area.[413]
One of the most important events of the 19th century was the rise of Indian nationalism,[414] leading Indians to seek first "self-rule" and later "complete independence". However, historians are divided over the causes of its rise. Probable reasons include a "clash of interests of the Indian people with British interests",[414] "racial discriminations",[415] and "the revelation of India's past".[416]
The first step toward Indian self-rule was the appointment of councillors to advise the British viceroy in 1861 and the first Indian was appointed in 1909. Provincial Councils with Indian members were also set up. The councillors' participation was subsequently widened into legislative councils. The British built a large British Indian Army, with the senior officers all British and many of the troops from small minority groups such as Gurkhas from Nepal and Sikhs.[417] The civil service was increasingly filled with natives at the lower levels, with the British holding the more senior positions.[418]
Bal Gangadhar Tilak, an Indian nationalist leader, declared Swaraj (home rule) as the destiny of the nation. His popular sentence "Swaraj is my birthright, and I shall have it"[419] became the source of inspiration for Indians. Tilak was backed by rising public leaders like Bipin Chandra Pal and Lala Lajpat Rai, who held the same point of view, notably they advocated the Swadeshi movement involving the boycott of all imported items and the use of Indian-made goods;[420] the triumvirate were popularly known as Lal Bal Pal. Under them, India's three big provinces – Maharashtra, Bengal and Punjab shaped the demand of the people and India's nationalism.[citation needed] In 1907, the Congress was split into two factions: The radicals, led by Tilak, advocated civil agitation and direct revolution to overthrow the British Empire and the abandonment of all things British. The moderates, led by leaders like Dadabhai Naoroji and Gopal Krishna Gokhale, on the other hand, wanted reform within the framework of British rule.[420]
The partition of Bengal in 1905 further increased the revolutionary movement for Indian independence. The disenfranchisement lead some to take violent action.
The British themselves adopted a "carrot and stick" approach in recognition of India's support during the First World War and in response to renewed nationalist demands. The means of achieving the proposed measure were later enshrined in the Government of India Act 1919, which introduced the principle of a dual mode of administration, or diarchy, in which elected Indian legislators and appointed British officials shared power.[421] In 1919, Colonel Reginald Dyer ordered his troops to fire their weapons on peaceful protestors, including unarmed women and children, resulting in the Jallianwala Bagh massacre; which led to the Non-cooperation Movement of 1920–1922. The massacre was a decisive episode towards the end of British rule in India.[422]
From 1920 leaders such as Mahatma Gandhi began highly popular mass movements to campaign against the British Raj using largely peaceful methods. The Gandhi-led independence movement opposed the British rule using non-violent methods like non-co-operation, civil disobedience and economic resistance. However, revolutionary activities against the British rule took place throughout the Indian subcontinent and some others adopted a militant approach like the Hindustan Republican Association, founded by Chandrasekhar Azad, Bhagat Singh, Sukhdev Thapar and others, that sought to overthrow British rule by armed struggle.
The All India Azad Muslim Conference gathered in Delhi in April 1940 to voice its support for an independent and united India.[423] Its members included several Islamic organisations in India, as well as 1400 nationalist Muslim delegates.[424][425][426] The pro-separatist All-India Muslim League worked to try to silence those nationalist Muslims who stood against the partition of India, often using "intimidation and coercion".[425][426] The murder of the All India Azad Muslim Conference leader Allah Bakhsh Soomro also made it easier for the pro-separatist All-India Muslim League to demand the creation of a Pakistan.[426]
"A moment comes, which comes but rarely in history, when we step out from the old to the new; when an age ends; and when the soul of a nation long suppressed finds utterance."
 — From, Tryst with destiny, a speech given by Jawaharlal Nehru to the Constituent Assembly of India on the eve of independence, 14 August 1947.[427]
In January 1946, several mutinies broke out in the armed services, starting with that of RAF servicemen frustrated with their slow repatriation to Britain. The mutinies came to a head with mutiny of the Royal Indian Navy in Bombay in February 1946, followed by others in Calcutta, Madras, and Karachi. The mutinies were rapidly suppressed. Also in early 1946, new elections were called and Congress candidates won in eight of the eleven provinces.
Late in 1946, the Labour government decided to end British rule of India, and in early 1947 it announced its intention of transferring power no later than June 1948 and participating in the formation of an interim government.
Along with the desire for independence, tensions between Hindus and Muslims had also been developing over the years. The Muslims had always been a minority within the Indian subcontinent, and the prospect of an exclusively Hindu government made them wary of independence; they were as inclined to mistrust Hindu rule as they were to resist the foreign Raj.
Muslim League leader Muhammad Ali Jinnah proclaimed 16 August 1946 as Direct Action Day, with the stated goal of highlighting, peacefully, the demand for a Muslim homeland in British India, which resulted in the outbreak of the cycle of violence that would be later called the "Great Calcutta Killing of August 1946". The communal violence spread to Bihar (where Muslims were attacked by Hindus), to Noakhali in Bengal (where Hindus were targeted by Muslims), in Garhmukteshwar in the United Provinces (where Muslims were attacked by Hindus), and on to Rawalpindi in March 1947 in which Hindus were attacked or driven out by Muslims.
A map of the prevailing religions of the British Indian empire based on district-wise majorities based on the Indian census of 1909, and published in the Imperial Gazetteer of India.  The partition of the Punjab and Bengal was based on such majorities.
Gandhi touring Bela, Bihar, a village struck by religious rioting in March 1947. On the right is Khan Abdul Gaffar Khan.
Jawaharlal Nehru being sworn in as the first prime minister of independent India by viceroy Lord Louis Mountbatten at 8:30 AM 15 August 1947.
In August 1947, the British Indian Empire was partitioned into the Union of India and Dominion of Pakistan. In particular, the partition of Punjab and Bengal led to rioting between Hindus, Muslims, and Sikhs in these provinces and spread to other nearby regions, leaving some 500,000 dead. The police and army units were largely ineffective. The British officers were gone, and the units were beginning to tolerate if not actually indulge in violence against their religious enemies.[428][429][430] Also, this period saw one of the largest mass migrations anywhere in modern history, with a total of 12 million Hindus, Sikhs and Muslims moving between the newly created nations of India and Pakistan (which gained independence on 15 and 14 August 1947 respectively).[429] In 1971, Bangladesh, formerly East Pakistan and East Bengal, seceded from Pakistan.[431]
In recent decades there have been four main schools of historiography in how historians study India: Cambridge, Nationalist, Marxist, and subaltern. The once common "Orientalist" approach, with its image of a sensuous, inscrutable, and wholly spiritual India, has died out in serious scholarship.[432]
The "Cambridge School", led by Anil Seal,[433] Gordon Johnson,[434] Richard Gordon, and David A. Washbrook,[435] downplays ideology.[436] However, this school of historiography is criticised for western bias or Eurocentrism.[437]
The Nationalist school has focused on Congress, Gandhi, Nehru and high level politics. It highlighted the Mutiny of 1857 as a war of liberation, and Gandhi's 'Quit India' begun in 1942, as defining historical events. This school of historiography has received criticism for Elitism.[438]
The Marxists have focused on studies of economic development, landownership, and class conflict in precolonial India and of deindustrialisation during the colonial period. The Marxists portrayed Gandhi's movement as a device of the bourgeois elite to harness popular, potentially revolutionary forces for its own ends. Again, the Marxists are accused of being "too much" ideologically influenced.[439]
The "subaltern school", was begun in the 1980s by Ranajit Guha and Gyan Prakash.[440] It focuses attention away from the elites and politicians to "history from below", looking at the peasants using folklore, poetry, riddles, proverbs, songs, oral history and methods inspired by anthropology. It focuses on the colonial era before 1947 and typically emphasises caste and downplays class, to the annoyance of the Marxist school.[441]
More recently, Hindu nationalists have created a version of history to support their demands for Hindutva ('Hinduness') in Indian society. This school of thought is still in the process of development.[442] In March 2012, Diana L. Eck, professor of Comparative Religion and Indian Studies at Harvard University, authored in her book India: A Sacred Geography, that the idea of India dates to a much earlier time than the British or the Mughals; it was not just a cluster of regional identities and it was not ethnic or racial.[443][444][445][446]


The history of the lands that became the United States began with the arrival of the first people in the Americas around 15,000 BC. Numerous indigenous cultures formed, and many saw transformations in the 16th century away from more densely populated lifestyles and towards reorganized polities elsewhere.  The European colonization of the Americas began in the late 15th century, however most colonies in what would later become the United States were settled after 1600.  By the 1760s, the thirteen British colonies contained 2.5 million people and were established along the Atlantic Coast east of the Appalachian Mountains. After defeating France, the British government imposed a series of taxes, including the Stamp Act of 1765, rejecting the colonists' constitutional argument that new taxes needed their approval. Resistance to these taxes, especially the Boston Tea Party in 1773, led to Parliament issuing punitive laws designed to end self-government. Armed conflict began in Massachusetts in 1775.
In 1776, in Philadelphia, the Second Continental Congress declared the independence of the colonies as the "United States". Led by General George Washington, it won the Revolutionary War. The peace treaty of 1783 established the borders of the new nation. The Articles of Confederation established a central government, but it was ineffectual at providing stability as it could not collect taxes and had no executive officer. A convention wrote a new Constitution that was adopted in 1789 and a Bill of Rights was added in 1791 to guarantee inalienable rights. With Washington as the first president and Alexander Hamilton his chief adviser, a strong central government was created. Purchase of the Louisiana Territory from France in 1803 doubled the size of the United States.
Encouraged by the notion of manifest destiny, the United States expanded to the Pacific Coast. While the nation was large in terms of area, its population in 1790 was only four million. Westward expansion was driven by a quest for inexpensive land for yeoman farmers and slave owners. The expansion of slavery was increasingly controversial and fueled political and constitutional battles, which were resolved by compromises. Slavery was abolished in all states north of the Mason–Dixon line by 1804, but states in the south continued the institution, to support the kinds of large scale agriculture that dominated the southern economy. The division of the country along these lines formed the major political issue of the first eight decades of the growth of the United States. Precipitated by the election of Abraham Lincoln as president in 1860, the Civil War began as the southern states seceded from the Union to form their own pro-slavery country, the Confederate States of America. The defeat of the Confederates in 1865 led to the abolition of slavery. In the Reconstruction era following the war, legal and voting rights were extended to freed slaves. The national government emerged much stronger, and gained explicit duty to protect individual rights. However, when white southern Democrats regained their political power in the South in 1877, often by paramilitary suppression of voting, they passed Jim Crow laws to maintain white supremacy, as well as new state constitutions that legalized discrimination based on race and prevented most African Americans from participating in public life.
The United States became the world's leading industrial power at the turn of the 20th century, due to an outburst of entrepreneurship and industrialization and the arrival of millions of immigrant workers and farmers. A national railroad network was completed and large-scale mines and factories were established. Mass dissatisfaction with corruption, inefficiency, and traditional politics stimulated the Progressive movement, from the 1890s to the 1920s, leading to reforms, including the federal income tax, direct election of Senators, granting of citizenship to many indigenous people, alcohol prohibition, and women's suffrage. Initially neutral during World War I, the United States declared war on Germany in 1917 and funded the Allied victory the following year. After the prosperous Roaring Twenties, the Wall Street Crash of 1929 marked the onset of the decade-long worldwide Great Depression. President Franklin D. Roosevelt implemented his New Deal programs, including relief for the unemployed, support for farmers, social security, and a minimum wage. The New Deal defined modern American liberalism.[1] Following the Japanese attack on Pearl Harbor, the United States entered World War II and financed the Allied war effort, and helped defeat Nazi Germany and Fascist Italy in the European theater. Its involvement culminated in using newly American invented nuclear weapons on Hiroshima and Nagasaki to defeat Imperial Japan in the Pacific War.
The United States and the Soviet Union emerged as rival superpowers in the aftermath of World War II. During the Cold War, the two countries confronted each other indirectly in the arms race, the Space Race, propaganda campaigns, and localized wars against communist expansion. In the 1960s, in large part due to the strength of the civil rights movement, another wave of social reforms was enacted which enforced the constitutional rights of voting and freedom of movement to African Americans. The Cold War ended when the Soviet Union was officially dissolved, leaving the United States as the world's sole superpower. Foreign policy after the Cold War has often focused on many modern conflicts in the Middle East, especially in response to the September 11 attacks. Early in the 21st century, the United States experienced the Great Recession and the COVID-19 pandemic, which had a negative effect on the local economy.
It is not definitively known how or when Native Americans first settled the Americas and the present-day United States. The prevailing theory proposes that people from Eurasia followed game across Beringia, a land bridge that connected Siberia to present-day Alaska during the Ice Age, and then spread southward throughout the Americas. This migration may have begun as early as 30,000 years ago[2] and continued through to about 10,000 years ago, when the land bridge became submerged by the rising sea level caused by the melting glaciers.[3]These early inhabitants, called Paleo-Indians, soon diversified into hundreds of culturally distinct nations and tribes.
This pre-Columbian era incorporates all periods in the history of the Americas before the appearance of European influences on the American continents, spanning from the original settlement in the Upper Paleolithic period to European colonization during the early modern period. While the term technically refers to the era before Christopher Columbus' voyage in 1492, in practice the term usually includes the history of American indigenous cultures until they were conquered or significantly influenced by Europeans, even if this happened decades or centuries after Columbus's initial landing.[4]
By 10,000 BCE, humans were relatively well-established throughout North America. Originally, Paleo-Indians hunted Ice Age megafauna like mammoths, but as they began to go extinct, people turned instead to bison as a food source. As time went on, foraging for berries and seeds became an important alternative to hunting. Paleo-Indians in central Mexico were the first in the Americas to farm, starting to plant corn, beans, and squash around 8,000 BCE. Eventually, the knowledge began to spread northward. By 3,000 BCE, corn was being grown in the valleys of Arizona and New Mexico, followed by primitive irrigation systems and early villages of the Hohokam.[5][6]
One of the earlier cultures in the present-day United States was the Clovis culture, who are primarily identified by the use of fluted spear points called the Clovis point. From 9,100 to 8,850 BCE, the culture ranged over much of North America and also appeared in South America. Artifacts from this culture were first excavated in 1932 near Clovis, New Mexico. The Folsom culture was similar, but is marked by the use of the Folsom point.
A later migration identified by linguists, anthropologists, and archeologists occurred around 8,000 BCE. This included Na-Dene-speaking peoples, who reached the Pacific Northwest by 5,000 BCE.[7] From there, they migrated along the Pacific Coast and into the interior and constructed large multi-family dwellings in their villages, which were used only seasonally in the summer to hunt and fish, and in the winter to gather food supplies.[8] Another group, the Oshara tradition people, who lived from 5,500 BCE to 600 CE, were part of the Archaic Southwest.
The Adena began constructing large earthwork mounds around 600 BCE. They are the earliest known people to have been Mound Builders, however, there are mounds in the United States that predate this culture. Watson Brake is an 11-mound complex in Louisiana that dates to 3,500 BCE, and nearby Poverty Point, built by the Poverty Point culture, is an earthwork complex that dates to 1,700 BCE. These mounds likely served a religious purpose.
The Adenans were absorbed into the Hopewell tradition, a powerful people who traded tools and goods across a wide territory. They continued the Adena tradition of mound-building, with remnants of several thousand still in existence across the core of their former territory in southern Ohio. The Hopewell pioneered a trading system called the Hopewell Exchange System, which at its greatest extent ran from the present-day Southeast up to the Canadian side of Lake Ontario.[9] By 500 CE, the Hopewellians had too disappeared, absorbed into the larger Mississippian culture.
The Mississippians were a broad group of tribes. Their most important city was Cahokia, near modern-day St. Louis, Missouri. At its peak in the 12th century, the city had an estimated population of 20,000, larger than the population of London at the time. The entire city was centered around a mound that stood 100 feet (30 m) tall. Cahokia, like many other cities and villages of the time, depended on hunting, foraging, trading, and agriculture, and developed a class system with slaves and human sacrifice that was influenced by societies to the south, like the Mayans.[5]
In the Southwest, the Anasazi began constructing stone and adobe pueblos around 900 BCE.[10] These apartment-like structures were often built into cliff faces, as seen in the Cliff Palace at Mesa Verde. Some grew to be the size of cities, with Pueblo Bonito along the Chaco River in New Mexico once consisting of 800 rooms.[5]
The indigenous peoples of the Pacific Northwest were likely the most affluent Native Americans. Many distinct cultural and political nations developed there, but they all shared certain beliefs, traditions, and practices, such as the centrality of salmon as a resource and spiritual symbol. Permanent villages began to develop in this region as early as 1,000 BCE, and these communities celebrated by the gift-giving feast of the potlatch. These gatherings were usually organized to commemorate special events such as the raising of a Totem pole or the celebration of a new chief.
In present-day upstate New York, the Iroquois formed a confederacy of tribal nations in the mid-15th century, consisting of the Oneida, Mohawk, Onondaga, Cayuga, and Seneca. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.[11][12][13] Each tribe had seats in a group of 50 sachem chiefs. It has been suggested that their culture contributed to political thinking during the development of the United States government. The Iroquois were powerful, waging war with many neighboring tribes, and later, Europeans. As their territory expanded, smaller tribes were forced further west, including the Osage, Kaw, Ponca, and Omaha peoples.[13][14]
The exact date for the settling of Hawaii is disputed but the first settlment most likly took place between 940 and 1130 CE.[15] Around 1200 CE, Tahitian explorers found and began settling the area as well establishing a new caste system. This marked the rise of the Hawaiian civilization, which would be largely separated from the rest of the world until the arrival of the British 600 years later.[16][17][18] Europeans under the British explorer James Cook arrived in the Hawaiian Islands in 1778, and within five years of contact, European military technology would help Kamehameha I conquer most of the people, and eventually unify the islands for the first time; establishing the Hawaiian Kingdom.[19]
The island of Puerto Rico has been settled for at least for 4,000 years dating back to the remains of Puerto Ferro man. Starting with the Ortoiroid culture, successive generations of native migrations arrived replacing or absorbing local populations. By the year 1000 Arawak people had arrived from South America via the Lesser Antilles, these settlers would become the Taíno encountered by the Spanish in 1493. Upon European contact a native population between 30,000 and 60,000 was likely, led by a single chief called a Cacique.[20] Colonization resulted in the decimation of the local inhabitants due to the harsh Encomienda system and epidemics caused by Old World diseases. Puerto Rico would remain a part of Spain until American annexation in 1898.[20]
The earliest recorded European mention of America is in a historical treatise by the medieval chronicler Adam of Bremen, circa 1075, where it is referred to as Vinland.[a] It is also extensively referred to in the 13th-century Norse Vinland sagas, which relate to events which occurred around 1000. Whilst the strongest archaeological evidence of the existence of Norse settlements in America is located in Canada, most notably at L'Anse aux Meadows and dated to circa 1000, there is significant scholarly debate as to whether Norse explorers also made landfall in New England and other east-coast areas.[22] In 1925, President Calvin Coolidge declared that a Norse explorer called Leif Erikson (c.970 – c.1020) was the first European to discover America.[23]
After a period of exploration sponsored by major European nations, the first successful English settlement was established in 1607. Europeans brought horses, cattle, and hogs to the Americas and, in turn, took back maize, turkeys, tomatoes, potatoes, tobacco, beans, and squash to Europe. Many explorers and early settlers died after being exposed to new diseases in the Americas. However, the effects of new Eurasian diseases carried by the colonists, especially smallpox and measles, were much worse for the Native Americans, as they had no immunity to them. They suffered epidemics and died in very large numbers, usually before large-scale European settlement began. Their societies were disrupted and hollowed out by the scale of deaths.[24][25]
Spanish explorers were the first Europeans to reach the present-day United States, after Christopher Columbus's expeditions (beginning in 1492) established possessions in the Caribbean, including the modern-day U.S. territories of Puerto Rico, and parts of the U.S. Virgin Islands. Juan Ponce de León landed in Florida in 1513.[26] Spanish expeditions quickly reached the Appalachian Mountains, the Mississippi River, the Grand Canyon,[27] and the Great Plains.[28]
In 1539, Hernando de Soto extensively explored the Southeast,[28] and a year later Francisco Coronado explored from Arizona to central Kansas in search of gold.[28] Escaped horses from Coronado's party spread over the Great Plains, and the Plains Indians mastered horsemanship within a few generations.[5] Small Spanish settlements eventually grew to become important cities, such as San Antonio, Albuquerque, Tucson, Los Angeles, and San Francisco.[29]
The Dutch West India Company sent explorer Henry Hudson to search for a Northwest Passage to Asia in 1609. New Netherland was established in 1621 by the company to capitalize on the North American fur trade. Growth was slow at first due to mismanagement by the Dutch and Native American conflicts. After the Dutch purchased the island of Manhattan from the Native Americans for a reported price of US$24, the land was named New Amsterdam and became the capital of New Netherland. The town rapidly expanded and in the mid-1600s it became an important trading center and port. Despite being Calvinists and building the Reformed Church in America, the Dutch were tolerant of other religions and cultures and traded with the Iroquois to the north.[30]
The colony served as a barrier to British expansion from New England, and as a result a series of wars were fought. The colony was taken over by Britain as New York in 1664 and its capital was renamed New York City. New Netherland left an enduring legacy on American cultural and political life of religious tolerance and sensible trade in urban areas and rural traditionalism in the countryside (typified by the story of Rip Van Winkle). Notable Americans of Dutch descent include Martin Van Buren, Theodore Roosevelt, Franklin D. Roosevelt, Eleanor Roosevelt and the Frelinghuysens.[30]
In the early years of the Swedish Empire, Swedish, Dutch, and German stockholders formed the New Sweden Company to trade furs and tobacco in North America. The company's first expedition was led by Peter Minuit, who had been governor of New Netherland from 1626 to 1631 but left after a dispute with the Dutch government, and landed in Delaware Bay in March 1638. The settlers founded Fort Christina at the site of modern-day Wilmington, Delaware, and made treaties with the indigenous groups for land ownership on both sides of the Delaware River.[31][32]
Over the following seventeen years, 12 more expeditions brought settlers from the Swedish Empire (which also included contemporary Finland, Estonia, and portions of Latvia, Norway, Russia, Poland, and Germany) to New Sweden. The colony established 19 permanent settlements along with many farms, extending into modern-day Maryland, Pennsylvania, and New Jersey. It was incorporated into New Netherland in 1655 after a Dutch invasion from the neighboring New Netherland colony during the Second Northern War.[31][32]
Giovanni da Verrazzano landed in North Carolina in 1524, and was the first European to sail into New York Harbor and Narragansett Bay. A decade later, Jacques Cartier sailed in search of the Northwest Passage, but instead discovered the Saint Lawrence River and laid the foundation for French colonization of the Americas in New France. After the collapse of the first Quebec colony in the 1540s, French Huguenots settled at Fort Caroline near present-day Jacksonville in Florida. In 1565, Spanish forces led by Pedro Menéndez destroyed the settlement and established the first European settlement in what would become the United States — St. Augustine.
After this, the French mostly remained in Quebec and Acadia, but far-reaching trade relationships with Native Americans throughout the Great Lakes and Midwest spread their influence. French colonists in small villages along the Mississippi and Illinois rivers lived in farming communities that served as a grain source for Gulf Coast settlements. The French established plantations in Louisiana along with settling New Orleans, Mobile and Biloxi.
The English, drawn in by Francis Drake's raids on Spanish treasure ships leaving the New World, settled the strip of land along the east coast in the 1600s. The first British colony in North America was established at Roanoke by Walter Raleigh in 1585, but failed. It would be twenty years before another attempt.[5]
The early British colonies were established by private groups seeking profit, and were marked by starvation, disease, and Native American attacks. Many immigrants were people seeking religious freedom or escaping political oppression, peasants displaced by the Industrial Revolution, or those simply seeking adventure and opportunity.
In some areas, Native Americans taught colonists how to plant and harvest the native crops. In others, they attacked the settlers. Virgin forests provided an ample supply of building material and firewood. Natural inlets and harbors lined the coast, providing easy ports for essential trade with Europe. Settlements remained close to the coast due to this as well as Native American resistance and the Appalachian Mountains that were found in the interior.[5]
The first successful English colony, Jamestown, was established by the Virginia Company in 1607 on the James River in Virginia. The colonists were preoccupied with the search for gold and were ill-equipped for life in the New World. Captain John Smith held the fledgling Jamestown together in the first year, and the colony descended into anarchy and nearly failed when he returned to England two years later. John Rolfe began experimenting with tobacco from the West Indies in 1612, and by 1614 the first shipment arrived in London. It became Virginia's chief source of revenue within a decade.
In 1624, after years of disease and Indian attacks, including the Powhatan attack of 1622, King James I revoked the Virginia Company's charter and made Virginia a royal colony.
New England was initially settled primarily by Puritans fleeing religious persecution. The Pilgrims sailed for Virginia on the Mayflower in 1620, but were knocked off course by a storm and landed at Plymouth, where they agreed to a social contract of rules in the Mayflower Compact. Like Jamestown, Plymouth suffered from disease and starvation, but local Wampanoag Indians taught the colonists how to farm maize.
Plymouth was followed by the Puritans and Massachusetts Bay Colony in 1630. They maintained a charter for self-government separate from England, and elected founder John Winthrop as the governor for most of its early years. Roger Williams opposed Winthrop's treatment of Native Americans and religious intolerance, and established the colony of Providence Plantations, later Rhode Island, on the basis of freedom of religion. Other colonists established settlements in the Connecticut River Valley, and on the coasts of present-day New Hampshire and Maine. Native American attacks continued, with the most significant occurring in the 1637 Pequot War and the 1675 King Philip's War.
New England became a center of commerce and industry due to the poor, mountainous soil making agriculture difficult. Rivers were harnessed to power grain mills and sawmills, and the numerous harbors facilitated trade. Tight-knit villages developed around these industrial centers, and Boston became one of America's most important ports.
In the 1660s, the Middle Colonies of New York, New Jersey, and Delaware were established in the former Dutch New Netherland, and were characterized by a large degree of ethnic and religious diversity. At the same time, the Iroquois of New York, strengthened by years of fur trading with Europeans, formed the powerful Iroquois Confederacy.
The last colony in this region was Pennsylvania, established in 1681 by William Penn as a home for religious dissenters, including Quakers, Methodists, and the Amish.[34] The capital of the colony, Philadelphia, became a dominant commercial center in a few short years, with busy docks and brick houses. While Quakers populated the city, German immigrants began to flood into the Pennsylvanian hills and forests, while the Scots-Irish pushed into the far western frontier.
The extremely rural southern colonies contrasted greatly with the north. Outside of Virginia, the first British colony south of New England was Maryland, established as a Catholic haven in 1632. The economy of these two colonies was built entirely on yeoman farmers and planters. The planters established themselves in the Tidewater region of Virginia, establishing massive plantations with slave labor, while the small-scale farmers made their way into political office.
In 1670, the Province of Carolina was established, and Charleston became the region's great trading port. While Virginia's economy was based on tobacco, Carolina was much more diversified, exporting rice, indigo, and lumber as well. In 1712 the colony was split in two, creating North and South Carolina. The Georgia Colony – the last of the Thirteen Colonies – was established by James Oglethorpe in 1732 as a border to Spanish Florida and a reform colony for former prisoners and the poor.[34]
Religiosity expanded greatly after the First Great Awakening, a religious revival in the 1740s which was led by preachers such as Jonathan Edwards and George Whitefield. American Evangelicals affected by the Awakening added a new emphasis on divine outpourings of the Holy Spirit and conversions that implanted new believers with an intense love for God. Revivals encapsulated those hallmarks and carried the newly created evangelicalism into the early republic, setting the stage for the Second Great Awakening in the late 1790s.[35] In the early stages, evangelicals in the South, such as Methodists and Baptists, preached for religious freedom and abolition of slavery; they converted many slaves and recognized some as preachers.
Each of the 13 American colonies had a slightly different governmental structure. Typically, a colony was ruled by a governor appointed from London who controlled the executive administration and relied upon a locally elected legislature to vote on taxes and make laws. By the 18th century, the American colonies were growing very rapidly as a result of low death rates along with ample supplies of land and food. The colonies were richer than most parts of Britain, and attracted a steady flow of immigrants, especially teenagers who arrived as indentured servants.[36]
Over half of all European immigrants to Colonial America arrived as indentured servants.[37] Few could afford the cost of the journey to America, and so this form of unfree labor provided a means to immigrate. Typically, people would sign a contract agreeing to a set term of labor, usually four to seven years, and in return would receive transport to America and a piece of land at the end of their servitude. In some cases, ships' captains received rewards for the delivery of poor migrants, and so extravagant promises and kidnapping were common. The Virginia Company and the Massachusetts Bay Company also used indentured servant labor.[5]
The first African slaves were brought to Virginia[38] in 1619,[39] just twelve years after the founding of Jamestown. Initially regarded as indentured servants who could buy their freedom, the institution of slavery began to harden and the involuntary servitude became lifelong[39] as the demand for labor on tobacco and rice plantations grew in the 1660s.[citation needed] Slavery became identified with brown skin color, at the time seen as a "black race", and the children of slave women were born slaves (partus sequitur ventrem).[39] By the 1770s African slaves comprised a fifth of the American population.
The question of independence from Britain did not arise as long as the colonies needed British military support against the French and Spanish powers. Those threats were gone by 1765. However, London continued to regard the American colonies as existing for the benefit of the mother country in a policy known as mercantilism.[36]
Colonial America was defined by a severe labor shortage that used forms of unfree labor, such as slavery and indentured servitude. The British colonies were also marked by a policy of avoiding strict enforcement of parliamentary laws, known as salutary neglect. This permitted the development of an American spirit distinct from that of its European founders.[40]
An upper-class emerged in South Carolina and Virginia, with wealth based on large plantations operated by slave labor. A unique class system operated in upstate New York, where Dutch tenant farmers rented land from very wealthy Dutch proprietors, such as the Van Rensselaer family. The other colonies were more egalitarian, with Pennsylvania being representative. By the mid-18th century Pennsylvania was basically a middle-class colony with limited respect for its small upper-class. A writer in the Pennsylvania Journal in 1756 wrote:
The People of this Province are generally of the middling Sort, and at present pretty much upon a Level. They are chiefly industrious Farmers, Artificers or Men in Trade; they enjoy in are fond of Freedom, and the meanest among them thinks he has a right to Civility from the greatest.[41]The French and Indian War (1754–1763), part of the larger Seven Years' War, was a watershed event in the political development of the colonies. The influence of the French and Native Americans, the main rivals of the British Crown in the colonies and Canada, was significantly reduced and the territory of the Thirteen Colonies expanded into New France, both in Canada and Louisiana.[citation needed] The war effort also resulted in greater political integration of the colonies, as reflected in the Albany Congress and symbolized by Benjamin Franklin's call for the colonies to "Join, or Die." Franklin was a man of many inventions – one of which was the concept of a United States of America, which emerged after 1765 and would be realized a decade later.[42]
Following Britain's acquisition of French territory in North America, King George III issued the Royal Proclamation of 1763, with the goal of organizing the new North American empire and protecting the Native Americans from colonial expansion into western lands beyond the Appalachian Mountains. In the following years, strains developed in the relations between the colonists and the Crown. The British Parliament passed the Stamp Act of 1765, imposing a tax on the colonies, without going through the colonial legislatures. The issue was drawn: did Parliament have the right to tax Americans who were not represented in it? Crying "No taxation without representation", the colonists refused to pay the taxes as tensions escalated in the late 1760s and early 1770s.[43]
The Boston Tea Party in 1773 was a direct action by activists in the town of Boston to protest against the new tax on tea. Parliament quickly responded the next year with the Intolerable Acts, stripping Massachusetts of its historic right of self-government and putting it under military rule, which sparked outrage and resistance in all thirteen colonies. Patriot leaders from every colony convened the First Continental Congress to coordinate their resistance to the Intolerable Acts. The Congress called for a boycott of British trade, published a list of rights and grievances, and petitioned the king to rectify those grievances.[44] This appeal to the Crown had no effect, though, and so the Second Continental Congress was convened in 1775 to organize the defense of the colonies against the British Army.
Common people became insurgents against the British even though they were unfamiliar with the ideological rationales being offered. They held very strongly a sense of "rights" that they felt the British were deliberately violating – rights that stressed local autonomy, fair dealing, and government by consent. They were highly sensitive to the issue of tyranny, which they saw manifested by the arrival in Boston of the British Army to punish the Bostonians. This heightened their sense of violated rights, leading to rage and demands for revenge, and they had faith that God was on their side.[45]
The American Revolutionary War began at Lexington and Concord in Massachusetts in April 1775 when the British tried to seize ammunition supplies and arrest the Patriot leaders.
In terms of political values, the Americans were largely united on a concept called Republicanism, which rejected aristocracy and emphasized civic duty and a fear of corruption. For the Founding Fathers, according to one team of historians, "republicanism represented more than a particular form of government. It was a way of life, a core ideology, an uncompromising commitment to liberty, and a total rejection of aristocracy."[46]The Thirteen Colonies began a rebellion against British rule in 1775 and proclaimed their independence in 1776 as the United States of America. In the American Revolutionary War (1775–1783) the Americans captured the British invasion army at Saratoga in 1777, secured the Northeast and encouraged the French to make a military alliance with the United States. France brought in Spain and the Netherlands, thus balancing the military and naval forces on each side as Britain had no allies.[47]
General George Washington proved an excellent organizer and administrator who worked successfully with Congress and the state governors, selecting and mentoring his senior officers, supporting and training his troops, and maintaining an idealistic Republican Army. His biggest challenge was logistics because neither Congress nor the states had the funding to provide adequately for the equipment, munitions, clothing, paychecks, or even the food supply of the soldiers.
As a battlefield tactician, Washington often was outmaneuvered by his British counterparts. As a strategist, however, he had a better idea of how to win the war than they did. The British sent four invasion armies. Washington's strategy forced the first army out of Boston in 1776, and was responsible for the surrender of the second and third armies at Saratoga (1777) and Yorktown (1781). He limited the British control to New York City and a few places while keeping Patriot control of the great majority of the population.[48]
The Loyalists, whom the British counted upon heavily, comprised about 20% of the population but suffered weak organization. As the war ended, the final British army sailed out of New York City in November 1783, taking the Loyalist leadership with them. Washington unexpectedly then, instead of seizing power for himself, retired to his farm in Virginia.[48] Political scientist Seymour Martin Lipset observes, "The United States was the first major colony successfully to revolt against colonial rule. In this sense, it was the first 'new nation'."[49]
On July 2, 1776, the Second Continental Congress, meeting in Philadelphia, declared the independence of the colonies by adopting the resolution from Richard Henry Lee, that stated:
That these United Colonies are, and of right ought to be, free and independent States, that they are absolved from all allegiance to the British Crown, and that all political connection between them and the State of Great Britain is, and ought to be, totally dissolved; that measures should be immediately taken for procuring the assistance of foreign powers, and a Confederation be formed to bind the colonies more closely together.On July 4, 1776, they adopted the Declaration of Independence and this date is celebrated as the nation's birthday. Congress shortly thereafter officially changed the nation's name to the "United States of America" from the "United Colonies of America".[50]
The new nation was founded on Enlightenment ideals of liberalism and what Thomas Jefferson called the unalienable rights to "life, liberty and the pursuit of happiness". It was dedicated strongly to republican principles, which emphasized that people are sovereign (not hereditary kings), demanded civic duty, feared corruption, and rejected any aristocracy.[51]
In the 1780s the national government was able to settle the issue of the western regions of the young United States, which were ceded by the states to Congress and became territories. With the migration of settlers to the Northwest, soon they became states. Nationalists worried that the new nation was too fragile to withstand an international war, or even internal revolts such as the Shays' Rebellion of 1786 in Massachusetts.[52]
Nationalists – most of them war veterans – organized in every state and convinced Congress to call the Philadelphia Convention in 1787. The delegates from every state wrote a new Constitution that created a much more powerful and efficient central government, one with a strong president, and powers of taxation. The new government reflected the prevailing republican ideals of guarantees of individual liberty and of constraining the power of government through a system of separation of powers.[52]
The Congress was given authority to ban the international slave trade after 20 years (which it did in 1807). The Three-fifths Compromise gave the South Congressional apportionment out of proportion to its free population by allowing it to include three-fifths of the number of slaves in each state's total population. This provision increased the political power of southern representatives in the United States Congress, especially as slavery was extended into the Deep South through removal of Native Americans and transportation of slaves by an extensive domestic slave trade.
To assuage the Anti-Federalists who feared a too-powerful national government, the nation adopted the United States Bill of Rights in 1791. Comprising the first ten amendments of the Constitution, it guaranteed individual liberties such as freedom of speech and religious practice, jury trials, and stated that citizens and states had reserved rights (which were not specified).[53]
George Washington – a renowned hero of the American Revolutionary War, commander-in-chief of the Continental Army, and president of the Constitutional Convention – became the first President of the United States under the new Constitution in 1789. The national capital moved from New York to Philadelphia in 1790 and finally settled in Washington D.C. in 1800.
The major accomplishments of the Washington Administration were creating a strong national government that was recognized without question by all Americans.[54] His government, following the vigorous leadership of Treasury Secretary Alexander Hamilton, assumed the debts of the states (the debt holders received federal bonds), created the Bank of the United States to stabilize the financial system, and set up a uniform system of tariffs (taxes on imports) and other taxes to pay off the debt and provide a financial infrastructure. To support his programs Hamilton created a new political party – the first in the world based on voters – the Federalist Party.
Thomas Jefferson and James Madison formed an opposition Republican Party (usually called the Democratic-Republican Party by political scientists). Hamilton and Washington presented the country in 1794 with the Jay Treaty that reestablished good relations with Britain. The Jeffersonians vehemently protested, and the voters aligned behind one party or the other, thus setting up the First Party System. Federalists promoted business, financial and commercial interests and wanted more trade with Britain. Republicans accused the Federalists of plans to establish a monarchy, turn the rich into a ruling class, and making the United States a pawn of the British.[55] The treaty passed, but politics became intensely heated.[56]
Serious challenges to the new federal government included the Northwest Indian War, the ongoing Cherokee–American wars, and the 1794 Whiskey Rebellion, in which western settlers protested against a federal tax on liquor. Washington called the state militia and personally led an army against the settlers, as the insurgents melted away and the power of the national government was firmly established.[48]
Washington refused to serve more than two terms – setting a precedent – and in his famous farewell address, he extolled the benefits of federal government and importance of ethics and morality while warning against foreign alliances and the formation of political parties.[57]
John Adams, a Federalist, defeated Jefferson in the 1796 election. War loomed with France and the Federalists used the opportunity to try to silence the Republicans with the Alien and Sedition Acts, build up a large army with Hamilton at the head, and prepare for a French invasion. However, the Federalists became divided after Adams sent a successful peace mission to France that ended the Quasi-War of 1798.[55][58]
During the first two decades after the Revolutionary War, there were dramatic changes in the status of slavery among the states and an increase in the number of freed blacks. Inspired by revolutionary ideals of the equality of men and influenced by their lesser economic reliance on slavery, northern states abolished slavery.
States of the Upper South made manumission easier, resulting in an increase in the proportion of free blacks in the Upper South (as a percentage of the total non-white population) from less than one percent in 1792 to more than 10 percent by 1810. By that date, a total of 13.5 percent of all blacks in the United States were free.[59] After that date, with the demand for slaves on the rise because of the Deep South's expanding cotton cultivation, the number of manumissions declined sharply; and an internal U.S. slave trade became an important source of wealth for many planters and traders.
In 1807, Congress severed the U.S.'s involvement with the Atlantic slave trade.[60]
Jefferson defeated Adams massively for the presidency in the 1800 election. Jefferson's major achievement as president was the Louisiana Purchase in 1803, which provided U.S. settlers with vast potential for expansion west of the Mississippi River.[61]
Jefferson, a scientist, supported expeditions to explore and map the new domain, most notably the Lewis and Clark Expedition.[62] Jefferson believed deeply in republicanism and argued it should be based on the independent yeoman farmer and planter; he distrusted cities, factories and banks. He also distrusted the federal government and judges, and tried to weaken the judiciary. However he met his match in John Marshall, a Federalist from Virginia. Although the Constitution specified a Supreme Court, its functions were vague until Marshall, the Chief Justice of the United States (1801–1835), defined them, especially the power to overturn acts of Congress or states that violated the Constitution, first enunciated in 1803 in Marbury v. Madison.[63]
Americans were increasingly angry at the British violation of American ships' neutral rights to hurt France, the impressment (seizure) of 10,000 American sailors needed by the Royal Navy to fight Napoleon, and British support for hostile Indians attacking American settlers in the American Midwest with the goal of creating a pro-British Indian barrier state to block American expansion westward. They may also have desired to annex all or part of British North America, although this is still heavily debated.[64][65][66][67][68] Despite strong opposition from the Northeast, especially from Federalists who did not want to disrupt trade with Britain, Congress declared war on June 18, 1812.[69]
The war was frustrating for both sides. Both sides tried to invade the other and were repulsed. The American high command remained incompetent until the last year. The American militia proved ineffective because the soldiers were reluctant to leave home and efforts to invade Canada repeatedly failed. The British blockade ruined American commerce, bankrupted the Treasury, and further angered New Englanders, who smuggled supplies to Britain. The Americans under General William Henry Harrison finally gained naval control of Lake Erie and defeated the Indians under Tecumseh in Canada,[71] while Andrew Jackson ended the Indian threat in the Southeast. The Indian threat to expansion into the Midwest was permanently ended. The British invaded and occupied much of Maine.
The British raided and burned Washington, but were repelled at Baltimore in 1814 – where the "Star Spangled Banner" was written to celebrate the American success. In upstate New York a major British invasion of New York State was turned back at the Battle of Plattsburgh. Finally in early 1815 Andrew Jackson decisively defeated a major British invasion at the Battle of New Orleans, making him the most famous war hero.[72]
With Napoleon (apparently) gone, the causes of the war had evaporated and both sides agreed to a peace that left the prewar boundaries intact. Americans claimed victory on February 18, 1815, as news came almost simultaneously of Jackson's victory of New Orleans and the peace treaty that left the prewar boundaries in place. Americans swelled with pride at success in the "second war of independence"; the naysayers of the antiwar Federalist Party were put to shame and the party never recovered. This helped lead to an emerging American identity that cemented national pride over state pride.[73]
Britain never achieved the war goal of granting the Indians a barrier state to block further American settlement and this allowed settlers to pour into the Midwest without fear of a major threat.[72] The War of 1812 also destroyed America's negative perception of a standing army, which was proved useful in many areas against the British as opposed to ill-equipped and poorly-trained militias in the early months of the war, and War Department officials instead decided to place regular troops as the nation's main defense.[74]
The Second Great Awakening was a Protestant revival movement that affected the entire nation during the early 19th century and led to rapid church growth. The movement began around 1790, gained momentum by 1800, and, after 1820 membership rose rapidly among Baptist and Methodist congregations, whose preachers led the movement. It was past its peak by the 1840s.[75]
It enrolled millions of new members in existing evangelical denominations and led to the formation of new denominations. Many converts believed that the Awakening heralded a new millennial age. The Second Great Awakening stimulated the establishment of many reform movements – including abolitionism and temperance designed to remove the evils of society before the anticipated Second Coming of Jesus Christ.[76]
As strong opponents of the War of 1812, the Federalists held the Hartford Convention in 1814 that hinted at disunion. National euphoria after the victory at New Orleans ruined the prestige of the Federalists and they no longer played a significant role as a political party.[77] President Madison and most Republicans realized they were foolish to let the First Bank of the United States close down, for its absence greatly hindered the financing of the war. So, with the assistance of foreign bankers, they chartered the Second Bank of the United States in 1816.[78][79]
The Republicans also imposed tariffs designed to protect the infant industries that had been created when Britain was blockading the U.S. With the collapse of the Federalists as a party, the adoption of many Federalist principles by the Republicans, and the systematic policy of President James Monroe in his two terms (1817–1825) to downplay partisanship, the nation entered an Era of Good Feelings, with far less partisanship than before (or after), and closed out the First Party System.[78][79]
The Monroe Doctrine, expressed in 1823, proclaimed the United States' opinion that European powers should no longer colonize or interfere in the Americas. This was a defining moment in the foreign policy of the United States. The Monroe Doctrine was adopted in response to American and British fears over Russian and French expansion into the Western Hemisphere.[80]
In 1832, President Andrew Jackson, 7th President of the United States, ran for a second term under the slogan "Jackson and no bank" and did not renew the charter of the Second Bank of the United States of America, ending the Bank in 1836.[81] Jackson was convinced that central banking was used by the elite to take advantage of the average American, and instead implemented state banks, popularly known as "pet banks".[81]
In 1830, Congress passed the Indian Removal Act, which authorized the president to negotiate treaties that exchanged Native American tribal lands in the eastern states for lands west of the Mississippi River.[82] Its goal was primarily to remove Native Americans, including the Five Civilized Tribes, from the American Southeast – they occupied land that settlers wanted.[83]
Jacksonian Democrats demanded the forcible removal of native populations who refused to acknowledge state laws to reservations in the West. Whigs and religious leaders opposed the move as inhumane. Thousands of deaths resulted from the relocations, as seen in the Cherokee Trail of Tears.[83] The Trail of Tears resulted in approximately 2,000–8,000 of the 16,543 relocated Cherokee perishing along the way.[84][full citation needed][85] Many of the Seminole Indians in Florida refused to move west; they fought the Army for years in the Seminole Wars.
After the First Party System of Federalists and Republicans withered away in the 1820s, the stage was set for the emergence of a new party system based on well organized local parties that appealed for the votes of (almost) all adult white men. The former Jeffersonian (Democratic-Republican) party split into factions. They split over the choice of a successor to President James Monroe, and the party faction that supported many of the old Jeffersonian principles, led by Andrew Jackson and Martin Van Buren, became the Democratic Party. As Norton explains the transformation in 1828:
Jacksonians believed the people's will had finally prevailed. Through a lavishly financed coalition of state parties, political leaders, and newspaper editors, a popular movement had elected the president. The Democrats became the nation's first well-organized national party, and tight party organization became the hallmark of nineteenth-century American politics.[86]Opposing factions led by Henry Clay helped form the Whig Party. The Democratic Party had a small but decisive advantage over the Whigs until the 1850s, when the Whigs fell apart over the issue of slavery.
The great majority of anti-slavery activists, such as Abraham Lincoln and Mr. Walters, rejected Garrison's theology and held that slavery was an unfortunate social evil, not a sin.[87][88]

The American colonies and the new nation grew rapidly in population and area, as pioneers pushed the frontier of settlement west.[89][90] The process finally ended around 1890–1912 as the last major farmlands and ranch lands were settled. Native American tribes in some places resisted militarily, but they were overwhelmed by settlers and the army and after 1830 were relocated to reservations in the west. The highly influential "Frontier thesis" of Wisconsin historian Frederick Jackson Turner argues that the frontier shaped the national character, with its boldness, violence, innovation, individualism, and democracy.[91]
Recent historians have emphasized the multicultural nature of the frontier. Enormous popular attention in the media focuses on the "Wild West" of the second half of the 19th century. As defined by Hine and Faragher, "frontier history tells the story of the creation and defense of communities, the use of the land, the development of markets, and the formation of states". They explain, "It is a tale of conquest, but also one of survival, persistence, and the merging of peoples and cultures that gave birth and continuing life to America."[91]
The first settlers in the west were the Spanish in New Mexico; they became U.S. citizens in 1848. The Hispanics in California ("Californios") were overwhelmed by over 100,000 California Gold Rush miners. California grew explosively. San Francisco by 1880 had become the economic hub of the entire Pacific Coast with a diverse population of a quarter million.
From the early 1830s to 1869, the Oregon Trail and its many offshoots were used by over 300,000 settlers. '49ers (in the California Gold Rush), ranchers, farmers, and entrepreneurs and their families headed to California, Oregon, and other points in the far west. Wagon-trains took five or six months on foot; after 1869, the trip took 6 days by rail.[92]
Manifest destiny was the belief that American settlers were destined to expand across the continent. This concept was born out of "A sense of mission to redeem the Old World by high example ... generated by the potentialities of a new earth for building a new heaven".[93] Manifest Destiny was rejected by modernizers, especially the Whigs like Henry Clay and Abraham Lincoln who wanted to build cities and factories – not more farms.[b] Democrats strongly favored expansion, and won the key election of 1844. After a bitter debate in Congress the Republic of Texas was annexed in 1845, leading to war with Mexico, who considered Texas to be a part of Mexico due to the large numbers of Mexican settlers.[95]
The Mexican–American War (1846–1848) broke out with the Whigs opposed to the war, and the Democrats supporting the war. The U.S. army, using regulars and large numbers of volunteers, defeated the Mexican armies, invaded at several points, captured Mexico City and won decisively. The Treaty of Guadalupe Hidalgo ended the war in 1848. Many Democrats wanted to annex all of Mexico, but that idea was rejected by White Southerners who argued that by incorporating millions of Mexican people, mainly of mixed race, would undermine the United States as an exclusively white republic.[94]
Instead the U.S. took Texas and the lightly settled northern parts (California and New Mexico). The Hispanic residents were given full citizenship and the Mexican Indians became American Indians. Simultaneously, gold was discovered in California in 1849, attracting over 100,000 men to northern California in a matter of months in the California Gold Rush. Thousands of California Indians were killed by U.S. government agents or settlers in the decades after the U.S. took California in what has since been termed the California genocide.[96] A peaceful compromise with Britain gave the U.S. ownership of the Oregon Country, which was renamed the Oregon Territory.[95]
The demand for guano (prized as an agricultural fertilizer) led the United States to pass the Guano Islands Act in 1856, which enabled citizens of the United States to take possession, in the name of the United States, of unclaimed islands containing guano deposits. Under the act the United States annexed nearly 100 islands in the Pacific Ocean and the Caribbean Sea. By 1903, 66 of these islands were recognized as territories of the United States.[97]
The central issue after 1848 was the expansion of slavery, with the anti-slavery elements in the North pitted against the pro-slavery elements that dominated the South. A small number of active Northerners were abolitionists who declared that ownership of slaves was a sin (in terms of Protestant theology) and demanded its immediate abolition. Much larger numbers in the North were against the expansion of slavery, seeking to put it on the path to extinction so that America would be committed to free land (as in low-cost farms owned and cultivated by a family), free labor, and free speech (as opposed to censorship of abolitionist material in the South). Southern white Democrats insisted that slavery was of economic, social, and cultural benefit to all whites (and even to the slaves themselves), and denounced all anti-slavery spokesmen as "abolitionists".[98]
Justifications of slavery included economics, history, religion, legality, social good, and even humanitarianism. Defenders of slavery argued that the sudden end to the slave economy would have had a profound and fatal economic impact in the South where reliance on slave labor was the foundation of their economy. They also argued that if all the slaves were freed, there would be widespread unemployment and chaos.[99]
Religious activists were split on slavery, with the Methodists and Baptists dividing into northern and southern denominations. In the North, the Methodists, Congregationalists, and Quakers included many abolitionists, especially among women activists. (The Catholic, Episcopal and Lutheran denominations largely ignored the slavery issue.)[100]
The issue of slavery in the new territories was seemingly settled by the Compromise of 1850, brokered by Whig Henry Clay and Democrat Stephen Douglas; the Compromise included the admission of California as a free state in exchange for no federal restrictions on slavery placed on Utah or New Mexico.[101] A point of contention was the Fugitive Slave Act, which increased federal enforcement and required even free states to cooperate in turning over fugitive slaves to their owners. Abolitionists highlighted the Act as particularly harmful in their fight against slavery, as evinced in the best-selling anti-slavery novel Uncle Tom's Cabin by Harriet Beecher Stowe.[102]
The Compromise of 1820 was repealed in 1854 with the Kansas–Nebraska Act, promoted by Senator Douglas in the name of "popular sovereignty" and democracy. It permitted voters to decide on the legality of slavery in each territory, and allowed Douglas to adopt neutrality on the issue of slavery. Anti-slavery forces rose in anger and alarm, forming the new Republican Party. Pro- and anti- contingents rushed to Kansas to vote slavery up or down, resulting in a miniature civil war called Bleeding Kansas. By the late 1850s, the young Republican Party dominated nearly all northern states and thus the electoral college. It insisted that slavery would never be allowed to expand (and thus would slowly die out).[103]
The Southern slavery-based societies had become wealthy based on their cotton and other agricultural commodity production, and some particularly profited from the internal slave trade. Northern cities such as Boston and New York, and regional industries, were tied economically to slavery by banking, shipping, and manufacturing, including textile mills. By 1860, there were four million slaves in the South, nearly eight times as many as there were nationwide in 1790. The plantations were highly profitable, due to the heavy European demand for raw cotton. Most of the profits were invested in new lands and in purchasing more slaves (largely drawn from the declining tobacco regions).
For 50 of the nation's first 72 years, a slaveholder served as President of the United States and, during that period, only slaveholding presidents were re-elected to second terms.[104] In addition, southern states benefited by their increased apportionment in Congress due to the partial counting of slaves in their populations.
There was resistance to slavery by both peaceful and violent means. Slave rebellions, by Gabriel Prosser (1800), Denmark Vesey (1822), Nat Turner (1831), and most famously by John Brown (1859), caused fear in the white South, which imposed stricter oversight of slaves and reduced the rights of free blacks. 
Abolitionism grew in the north in the decades before the Civil War, while southern states entreched themselves on upholding slaverly firecly.  Former slaves Frederick Douglass and Harriet Tubman became leading advocates for abolition.[105] [106] However, before 1860 only a minority of northern whites supported abolition, which was often seen as a 'radical' measure. There were violent reactions to abolitionist advocates in the north, notably the burning of an anti slavery society in Pennslyvania Hall.[107]
With politlcal power in congress being a key flash-point between slave and free states The Fugitive Slave Act of 1850 required the states to cooperate with slave owners when attempting to recover escaped slaves, which outraged Northerners. Formerly, an escaped slave that reached a non-slave state was presumed to have attained sanctuary and freedom under the Missouri Compromise. The Supreme Court's 1857 decision in Dred Scott v. Sandford ruled that the Missouri Compromise was unconstitutional and that free blacks were not citizens of the United States; the decision enraged all major political forces except for southern states. The Republicans worried the decision could be used to expand slavery through the entire nation. With senator Abraham Lincoln leading critcism of the ruling, the stage was set for the 1860 presidential election. [108][109]
After Abraham Lincoln won the 1860 election, seven Southern states seceded from the union and set up a new nation, the Confederate States of America (Confederacy), on February 8, 1861. It attacked Fort Sumter, a U.S. Army fort in South Carolina, thus igniting the war. When Lincoln called for troops to suppress the Confederacy in April 1861, four more states seceded and joined the Confederacy. A few of the (northernmost) "slave states" did not secede and became known as the border states; these were Delaware, Maryland, Kentucky, and Missouri.
During the war, the northwestern portion of Virginia seceded from the Confederacy. and became the new Union state of West Virginia.[110] West Virginia is usually associated with the border states.
The Civil War began April 12, 1861, when Confederate forces attacked a U.S. military installation at Fort Sumter in South Carolina. In response, Lincoln called on the states to send troops to recapture forts, protect the capital, and "preserve the Union," which in his view still existed intact despite the actions of the seceding states. The two armies had their first major clash at the First Battle of Bull Run, which proved to both sides that the war would be much longer and bloodier than originally anticipated.[111]
In the western theater, the Union Army was relatively successful, with major battles, such as Perryville and Shiloh along with Union Navy gunboat dominance of navigable rivers producing strategic Union victories and destroying major Confederate operations.[112]
Warfare in the eastern theater began poorly for the Union. U.S. General George B. McClellan failed to capture the Confederate capital of Richmond, Virginia in his Peninsula campaign and retreated after attacks from Confederate General Robert E. Lee.[113] Meanwhile, both sides concentrated in 1861–1862 on raising and training new armies. The main action was Union success in controlling the border states, with Confederates largely driven out of border states.[114]
The autumn 1862 Confederate retreat at the Battle of Antietam led to Lincoln's warning he would issue an Emancipation Proclamation in January 1863 if the states did not return. Making slavery a central war goal energized Republicans in the North, as well as their enemies, the anti-war Copperhead Democrats and ended the chance of British and French intervention.[114]
Lee's smaller Army of Northern Virginia won battles in late 1862 and Spring 1863, but he pushed too hard and ignored the Union threat in the west. Lee invaded Pennsylvania in search of supplies and to cause war-weariness in the North. In perhaps the turning point of the war, Lee's army was badly beaten by the Army of the Potomac at the July 1863 Battle of Gettysburg and barely made it back to Virginia.[114] Survivors of the Battle of Gettysburg were immedietely redployed to supress the New York City draft riots by Irish Americans first resisting Civil War conscription but later out of racism aganist the city's free black population.[115]
In July 1863, Union forces under General Ulysses S. Grant gained control of the Mississippi River at the Battle of Vicksburg, thereby splitting the Confederacy. In 1864, Union General William Tecumseh Sherman marched south from Chattanooga to capture Atlanta, a decisive victory that ended war jitters among Republicans in the North and helped Lincoln win re-election.
On the homefront, industrial expansion in the North expanded dramatically, using its extensive railroad service, and moving industrial workers into munitions factories. Foreign trade increased, with the United States providing both food and cotton to Britain, and Britain sending in manufactured products and thousands of volunteers for the Union Army (plus a few to the Confederate army). The British operated blockade runners bringing in food, luxury items and munitions to the Confederacy, bringing out tobacco and cotton. The Union blockade increasingly shut down Confederate ports, and by late 1864 the blockade runners were usually captured before they could make more than a handful of runs.
The last two years of the war were bloody for both sides, with Sherman marching almost unopposed through southern states, burning cities, destroying plantations, ruining railroads and bridges, but avoiding civilian casualties. Sherman demonstrated that the South was unable to resist a Union invasion. Much of the Confederate heartland was destroyed, and could no longer provide desperately needed supplies to its armies. In spring 1864, Grant launched a war of attrition and pursued Lee to the final, Appomattox campaign which resulted in Lee surrendering in April 1865.
The American Civil War was the world's earliest industrial war. Railroads, the telegraph, steamships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I. It remains the deadliest war in American history, resulting in the deaths of about 750,000 soldiers and an undetermined number of civilian casualties.[c] About ten percent of all Northern males 20–45 years old, and 30 percent of all Southern white males aged 18–40 died.[118] Its legacy includes ending slavery in the United States, restoring the Union, and strengthening the role of the federal government.
According to historian Allan Nevins, the Civil War had a major long-term impact on the United States in terms of developing its leadership potential and moving the entire nation beyond the adolescent stage:
The fighting and its attendant demands upon industry, finance, medicine, and law also helped train a host of leaders who during the next 35 years, to 1900, made their influence powerfully felt on most of the social, economic, and cultural fronts. It broke down barriers of parochialism; it ended distrust of large-scale effort; it hardened and matured the whole people emotionally. The adolescent land of the 1850s…rose under the blows of battle to adult estate. The nation of the post-Appomattox generation, though sadly hurt (especially in the South) by war losses, and deeply scarred psychologically (especially in the North) by war hatreds and greeds, had at last the power, resolution, and self-trust of manhood.[119]
The Emancipation Proclamation was an executive order issued by President Abraham Lincoln on January 1, 1863. In a single stroke it changed the legal status, as recognized by the U.S. government, of 3 million slaves in designated areas of the Confederacy from "slave" to "free". It had the practical effect that as soon as a slave escaped the control of the Confederate government, by running away or through advances of federal troops, the slave became legally and actually free.[121]
The owners were never compensated. Plantation owners, realizing that emancipation would destroy their economic system, sometimes moved their slaves as far as possible out of reach of the Union army. By June 1865, the Union Army controlled all of the Confederacy and liberated all of the designated slaves.[121] Large numbers moved into camps run by the Freedmen's Bureau, where they were given food, shelter, medical care, and arrangements for their employment were made.
The severe dislocations of war and Reconstruction had a large negative impact on the black population, with a large amount of sickness and death.[122]
Reconstruction lasted from Lincoln's Emancipation Proclamation of January 1, 1863, to the Compromise of 1877.[111][123][124]
The major issues faced by Lincoln were the status of the ex-slaves ("Freedmen"), the loyalty and civil rights of ex-rebels, the status of the 11 ex-Confederate states, the powers of the federal government needed to prevent a future civil war, and the question of whether Congress or the President would make the major decisions.
The severe threats of starvation and displacement of the unemployed Freedmen were met by the first major federal relief agency, the Freedmen's Bureau, operated by the Army.[125]
Three "Reconstruction Amendments" were passed to expand civil rights for black Americans: the Thirteenth Amendment outlawed slavery; the Fourteenth Amendment guaranteed equal rights for all and citizenship for blacks; the Fifteenth Amendment prevented race from being used to disenfranchise men.
Ex-Confederates remained in control of most Southern states for over two years, but changed when the Radical Republicans gained control of Congress in the 1866 elections. President Andrew Johnson, who sought easy terms for reunions with ex-rebels, was virtually powerless in the face of the Radical Republican Congress; he was impeached, but the Senate's attempt to remove him from office failed by one vote. Congress enfranchised black men and temporarily stripped many ex-Confederate leaders of the right to hold office. New Republican governments came to power based on a coalition of Freedmen made up of Carpetbaggers (new arrivals from the North), and Scalawags (native white Southerners). They were backed by the U.S. Army. Opponents said they were corrupt and violated the rights of whites.[126]
State by state, the New Republicans lost power to a conservative-Democratic coalition, which gained control of the entire South by 1877. In response to Radical Reconstruction, the Ku Klux Klan (KKK) emerged in 1867 as a white-supremacist organization opposed to black civil rights and Republican rule. President Ulysses Grant's vigorous enforcement of the Ku Klux Klan Act of 1870 shut down the Klan, and it disbanded.[126]
Paramilitary groups, such as the White League and Red Shirts emerged about 1874 that worked openly to use intimidation and violence to suppress black voting to regain white political power in states across the South during the 1870s. One historian described them as the military arm of the Democratic Party.[126]
Reconstruction ended after the disputed 1876 election. The Compromise of 1877 gave Republican candidate Rutherford B. Hayes the White House in exchange for removing all remaining federal troops in the South. The federal government withdrew its troops from the South, and Southern Democrats took control of every Southern state.[127] In 1882, the United States passed its first major exclusionary immigration acts, the Chinese Exclusion Act (which barred all Chinese immigrants except for students and businessmen),[128] and the Immigration Act of 1882 (which barred all immigrants with mental health issues).[129]
From 1890 to 1908, southern states effectively disfranchised most black voters and many poor whites by making voter registration more difficult through poll taxes, literacy tests, and other arbitrary devices. They passed segregation laws and imposed second-class status on blacks in a system known as Jim Crow that lasted until the Civil rights movement.[130]
The latter half of the nineteenth century was marked by the rapid development and settlement of the far West, first by wagon trains and riverboats and then aided by the completion of the transcontinental railroad. Large numbers of European immigrants (especially from Germany and Scandinavia) took up low-cost or free farms in the Prairie States. Mining for silver and copper opened up the Mountain West.
The United States Army fought frequent small-scale wars with Native Americans as settlers encroached on their traditional lands. Gradually the U.S. purchased the Native American tribal lands and extinguished their claims, forcing most tribes onto subsidized reservations. According to the U.S. Bureau of the Census (1894), from 1789 to 1894:
The Indian wars under the government of the United States have been more than 40 in number. They have cost the lives of about 19,000 white men, women and children, including those killed in individual combats, and the lives of about 30,000 Indians. The actual number of killed and wounded Indians must be very much higher than the given… Fifty percent additional would be a safe estimate.[131]The "Gilded Age" was a term that Mark Twain used to describe the period of the late 19th century with a dramatic expansion of American wealth and prosperity, underscored by the mass corruption in the government. Reforms of the Age included the Civil Service Act, which mandated a competitive examination for applicants for government jobs. Other important legislation included the Interstate Commerce Act, which ended railroads' discrimination against small shippers, and the Sherman Antitrust Act, which outlawed monopolies in business. Twain believed that this age was corrupted by such elements as land speculators, scandalous politics, and unethical business practices.[132]
Since the days of Charles A. Beard and Matthew Josephson, some historians have argued that the United States was effectively plutocratic for at least part of the Gilded Age and Progressive Era.[133][134][135][136][137] As financiers and industrialists such as J.P. Morgan and John D. Rockefeller began to amass vast fortunes, many U.S. observers were concerned that the nation was losing its pioneering egalitarian spirit.[138]
By 1890 American industrial production and per capita income exceeded those of all other world nations. In response to heavy debts and decreasing farm prices, wheat and cotton farmers joined the Populist Party.[139] An unprecedented wave of immigration from Europe served to both provide the labor for American industry and create diverse communities in previously undeveloped areas. From 1880 to 1914, peak years of immigration, more than 22 million people migrated to the United States.[140]
Most were unskilled workers who quickly found jobs in mines, mills, and factories. Many immigrants were craftsmen (especially from Britain and Germany) bringing human skills, and others were farmers (especially from Germany and Scandinavia) who purchased inexpensive land on the Prairies from railroads who sent agents to Europe. Poverty, growing inequality and dangerous working conditions, along with socialist and anarchist ideas diffusing from European immigrants, led to the rise of the labor movement, which often included violent strikes.[141][142]
Skilled workers banded together to control their crafts and raise wages by forming labor unions in industrial areas of the Northeast. Before the 1930s few factory workers joined the unions in the labor movement. Samuel Gompers led the American Federation of Labor (1886–1924), coordinating multiple unions. Industrial growth was rapid, led by John D. Rockefeller in oil and Andrew Carnegie in steel; both became leaders of philanthropy (Gospel of Wealth), giving away their fortunes to create the modern system of hospitals, universities, libraries, and foundations.
The Panic of 1893 broke out and was a severe nationwide depression impacting farmers, workers, and businessmen who saw prices, wages, and profits fall.[144] Many railroads went bankrupt. The resultant political reaction fell on the Democratic Party, whose leader President Grover Cleveland shouldered much of the blame. Labor unrest involved numerous strikes, most notably the violent Pullman Strike of 1894, which was shut down by federal troops under Cleveland's orders. The Populist Party gained strength among cotton and wheat farmers, as well as coal miners, but was overtaken by the even more popular Free silver movement, which demanded using silver to enlarge the money supply, leading to inflation that the silverites promised would end the depression.[145]
The financial, railroad, and business communities fought back hard, arguing that only the gold standard would save the economy. In the most intense election in the nation's history, conservative Republican William McKinley defeated silverite William Jennings Bryan, who ran on the Democratic, Populist, and Silver Republican tickets. Bryan swept the South and West, but McKinley ran up landslides among the middle class, industrial workers, cities, and among upscale farmers in the Midwest.[146]
Prosperity returned under McKinley, the gold standard was enacted, and the tariff was raised. By 1900 the U.S. had the strongest economy on the globe. Apart from two short recessions (in 1907 and 1920) the overall economy remained prosperous and growing until 1929. Republicans, citing McKinley's policies, took the credit.[147]
The United States emerged as a world economic and military power after 1890. The main episode was the Spanish–American War, which began when Spain refused American demands to reform its oppressive policies in Cuba.[149] The "splendid little war", as one official called it, involved a series of quick American victories on land and at sea. At the Treaty of Paris peace conference the United States acquired the Philippines, Puerto Rico, and Guam.[150]
Cuba became an independent country, under close American tutelage. Although the war itself was widely popular, the peace terms proved controversial. William Jennings Bryan led his Democratic Party in opposition to control of the Philippines, which he denounced as imperialism unbecoming to American democracy.[150] President William McKinley defended the acquisition and was riding high as the nation had returned to prosperity and felt triumphant in the war. McKinley easily defeated Bryan in a rematch in the 1900 presidential election.[151]
After defeating an insurrection by Filipino nationalists, the United States achieved little in the Philippines except in education, and it did something in the way of public health. It also built roads, bridges, and wells, but infrastructural development lost much of its early vigor with the failure of the railroads.[152] By 1908, however, Americans lost interest in an empire and turned their international attention to the Caribbean, especially the building of the Panama Canal. The canal opened in 1914 and increased trade with Japan and the rest of the Far East. A key innovation was the Open Door Policy, whereby the imperial powers were given equal access to Chinese business, with not one of them allowed to take control of China.[153]
Dissatisfaction on the part of the growing middle class with the corruption and inefficiency of politics as usual, and the failure to deal with increasingly important urban and industrial problems, led to the dynamic Progressive Movement starting in the 1890s. In every major city and state, and at the national level as well, and in education, medicine, and industry, the progressives called for the modernization and reform of decrepit institutions, the elimination of corruption in politics, and the introduction of efficiency as a criterion for change.[154]
Leading politicians from both parties, most notably Theodore Roosevelt, Charles Evans Hughes, and Robert La Follette on the Republican side, and William Jennings Bryan and Woodrow Wilson on the Democratic side, took up the cause of progressive reform. Women became especially involved in demands for female suffrage, prohibition, and better schools. Their most prominent leader was Jane Addams of Chicago, who created settlement houses.[154]
"Muckraking" journalists such as Upton Sinclair, Lincoln Steffens and Jacob Riis exposed corruption in business and government along with rampant inner-city poverty. Progressives implemented antitrust laws and regulated such industries of meat-packing, drugs, and railroads. Four new constitutional amendments – the Sixteenth through Nineteenth – resulted from progressive activism, bringing the federal income tax, direct election of Senators, prohibition, and female suffrage.[154]
The period also saw a major transformation of the banking system with the creation of the Federal Reserve System in 1913[155] and the arrival of cooperative banking in the US with the founding of the first credit union in 1908.[156] The Progressive Movement lasted through the 1920s; the most active period was 1900–1918.[157]
The women's suffrage movement began with the June 1848 National Convention of the Liberty Party. Presidential candidate Gerrit Smith argued for and established women's suffrage as a party plank. One month later, his cousin Elizabeth Cady Stanton joined with Lucretia Mott and other women to organize the Seneca Falls Convention, featuring the Declaration of Sentiments demanding equal rights for women, and the right to vote.[d]
Many of these activists became politically aware during the abolitionist movement. The women's rights campaign during "first-wave feminism" was led by Stanton, Lucy Stone and Susan B. Anthony, among many others. Stone and Paulina Wright Davis organized the prominent and influential National Women's Rights Convention in 1850.[159]
The movement reorganized after the Civil War, gaining experienced campaigners, many of whom had worked for prohibition in the Women's Christian Temperance Union. By the end of the 19th century a few western states had granted women full voting rights,[159] though women had made significant legal victories, gaining rights in areas such as property and child custody.[160]
Around 1912 the feminist movement began to reawaken, putting an emphasis on its demands for equality and arguing that the corruption of American politics demanded purification by women because men could not do that job.[161] Protests became increasingly common as suffragette Alice Paul led parades through the capital and major cities. Paul split from the large National American Woman Suffrage Association (NAWSA), which favored a more moderate approach and supported the Democratic Party and Woodrow Wilson, led by Carrie Chapman Catt, and formed the more militant National Woman's Party. Suffragists were arrested during their "Silent Sentinels" pickets at the White House, the first time such a tactic was used, and were taken as political prisoners.[162]
The old anti-suffragist argument that only men could fight a war, and therefore only men deserve the right to vote, was refuted by the enthusiastic participation of tens of thousands of American women on the home front in World War I. Across the world, grateful nations gave women the right to vote. Furthermore, most of the Western states had already given the women the right to vote in state and national elections, and the representatives from those states, including the first woman elected to the House of Representatives, Jeannette Rankin of Montana, demonstrated that woman suffrage was a success. The main resistance came from the south, where white leaders were worried about the threat of black women voting. Congress passed the Nineteenth Amendment in 1919, and women could vote in 1920.[163]
NAWSA became the League of Women Voters, and the National Woman's Party began lobbying for full equality and the Equal Rights Amendment, which would pass Congress during the second wave of the women's movement in 1972. Politicians responded to the new electorate by emphasizing issues of special interest to women, especially prohibition, child health, and world peace.[164][165] The main surge of women voting came in 1928, when the big-city machines realized they needed the support of women to elect Al Smith, a Catholic from New York City. Meanwhile, Protestants mobilized women to support Prohibition and vote for Republican Herbert Hoover.[166]
Women suffragists demonstrating for the right to vote in 1913.
Women's suffragists parade in New York City in 1917, carrying placards with signatures of more than a million women.[167]
Women surrounded by posters in English and Yiddish supporting Franklin D. Roosevelt, Herbert H. Lehman, and the American Labor Party teach other women how to vote, 1936.
As World War I raged in Europe from 1914, President Woodrow Wilson took full control of foreign policy, declaring neutrality but warning Germany that resumption of unrestricted submarine warfare against American ships supplying goods to Allied nations would mean war. Germany decided to take the risk and try to win by cutting off supplies to Britain through the sinking of ships such as the RMS Lusitania. The U.S. declared war in April 1917 mainly from the threat of the Zimmermann Telegram.[168]
American money, food, and munitions arrived in Europe quickly, but troops had to be drafted and trained. By the summer of 1918 American Expeditionary Forces soldiers under General John J. Pershing's American Expeditionary Forces arrived at the rate of 10,000 a day, while Germany was unable to replace its losses.[169] Dissent against the war was suppressed by the Sedition Act of 1918 & Espionage Act of 1917. German language, leftist & pacifist publications were suppressed, and over 2,000 were imprisoned for speaking out against the war. The political prisoners were later released by U.S. President Warren G. Harding.[170]
The result was Allied victory in November 1918. President Wilson demanded Germany depose the Kaiser and accept his terms in the famed Fourteen Points speech. Wilson dominated the 1919 Paris Peace Conference but Germany was treated harshly by the Allies in the Treaty of Versailles (1919) as Wilson put all his hopes in the new League of Nations. Wilson refused to compromise with Senate Republicans over the issue of Congressional power to declare war, and the Senate rejected the Treaty and the League.[171]
In the 1920s the U.S. grew steadily in stature as an economic and military world power. The United States Senate did not ratify the Treaty of Versailles imposed by its Allies on the defeated Central Powers; instead, the United States chose to pursue unilateralism.[172] The aftershock of Russia's October Revolution resulted in real fears of Communism in the United States, leading to a Red Scare and the deportation of aliens considered subversive.
While public health facilities grew rapidly in the Progressive Era, and hospitals and medical schools were modernized,[173] the nation in 1918 and 1919 lost approximately 675,000 lives to the Spanish flu pandemic.[174]
In 1920, the manufacture, sale, import and export of alcohol were prohibited by the Eighteenth Amendment, Prohibition. The result was that in cities illegal alcohol became a big business, largely controlled by racketeers.
The second Ku Klux Klan grew rapidly in 1922–1925, then collapsed. Immigration laws were passed to strictly limit the number of new entries. The 1920s were called the Roaring Twenties due to the great economic prosperity during this period. Jazz became popular among the younger generation, and thus the decade was also called the Jazz Age.
The Great Depression (1929–1939) and the New Deal (1933–1936) were decisive moments in American political, economic, and social history that reshaped the nation.[175]
During the 1920s, the nation enjoyed widespread prosperity, albeit with a weakness in agriculture. A financial bubble was fueled by an inflated stock market, which later led to the Stock Market Crash on October 29, 1929.[176][full citation needed] This, along with many other economic factors, triggered a worldwide depression known as the Great Depression. During this time, the United States experienced deflation as prices fell, unemployment soared from 3% in 1929 to 25% in 1933, farm prices fell by half, and manufacturing output plunged by one-third.
In 1932, Democratic presidential nominee Franklin D. Roosevelt promised "a New Deal for the American people", coining the enduring label for his domestic policies. The result was a series of permanent reform programs including Relief for the unemployed, assistance for the elderly, jobs for young men, social security, unemployment insurance, public housing, bankruptcy insurance, farm subsidies, and regulation of financial securities.[177]
State governments added new programs as well and introduced the sales tax to pay for them. Ideologically the revolution established modern liberalism in the United States and kept the Democrats in power in Washington almost continuously for three decades thanks to the New Deal coalition of ethnic whites, blacks, blue-collar workers, labor unions, and white Southerners. It provided relief to the long-term unemployed through numerous programs, such as the Works Progress Administration (WPA) and for young men, the Civilian Conservation Corps. Large scale spending projects designed to provide private sector construction jobs and rebuild the infrastructure were under the purview of the Public Works Administration.[177]
The Second New Deal was a turn to the left in 1935–1936, building up labor unions through the Wagner Act. Unions became a powerful element of the merging New Deal coalition, which won reelection for Roosevelt in 1936, 1940, and 1944 by mobilizing union members, blue-collar workers, relief recipients, big city machines, ethnic, and religious groups (especially Catholics and Jews) and the white South, along with blacks in the North (where they could vote). Roosevelt seriously weakened his second term by a failed effort to pack the Supreme Court, which had been a center of conservative resistance to his programs.[177]
Most of the relief programs were dropped after 1938 in the 1940s when the conservatives regained power in Congress through the Conservative coalition. Of special importance is the Social Security program, begun in 1935. The economy basically recovered by 1936, but had a sharp, short recession in 1937–1938; long-term unemployment, however, remained a problem until it was solved by wartime spending.[177]
In an effort to denounce past U.S. interventionism and subdue any subsequent fears of Latin Americans, Roosevelt announced on March 4, 1933, during his inaugural address, "In the field of World policy, I would dedicate this nation to the policy of the good neighbor, the neighbor who resolutely respects himself and, because he does so, respects the rights of others, the neighbor who respects his obligations and respects the sanctity of his agreements in and with a World of neighbors."[178]
To create a friendly relationship between the United States and Central as well as South American countries, Roosevelt sought to stray from asserting military force in the region.[179] This position was affirmed by Cordell Hull, Roosevelt's Secretary of State at a conference of American states in Montevideo in December 1933.
As a result of the Great Depression, 355,000 to 500,000 Mexicans and Mexican Americans were voluntarily repatriated or formally deported to Mexico during the 1930s, in what is known as the Mexican Repatriation.[180][181]
In the Depression years, the United States remained focused on domestic concerns while democracy declined across the world and many countries fell under the control of dictators. Imperial Japan asserted dominance in East Asia and in the Pacific. Nazi Germany and Fascist Italy militarized and threatened conquests, while Britain and France attempted appeasement to avert another war in Europe. U.S. legislation in the Neutrality Acts sought to avoid foreign conflicts; however, policy clashed with increasing anti-Nazi feelings following the German invasion of Poland in September 1939 that started World War II.[182]
At first, Roosevelt positioned the U.S. as the "Arsenal of Democracy," pledging full-scale financial and munitions support for the Allies and Lend-Lease agreements – but no military personnel.[182] Japan tried to neutralize America's power in the Pacific by attacking Pearl Harbor in 1941, but instead it catalyzed American support to enter the war.[183]
The main contributions of the U.S. to the Allied war effort comprised money, industrial output, food, petroleum, technological innovation, and (especially 1944–1945), military personnel. Much of the focus the U.S. government was in maximizing the national economic output, causing a dramatic increase in GDP, the export of vast quantities of supplies to the Allies and to American forces overseas, the end of unemployment, and a rise in civilian consumption even as 40% of the GDP went to the war effort.[175]
Tens of millions of workers moved from low-productivity occupations to high-efficiency jobs, improving productivity through better technology and management. Students, retired people, housewives, and the unemployed moved into the active labor force. Economic mobilization was managed by the War Production Board and a wartime production boom led to full employment, wiping out this vestige of the Great Depression. Labor shortages encouraged industry to look for new sources of workers, finding new roles for women and Blacks.[175]
Most durable goods became unavailable, and meat, clothing, and gasoline were tightly rationed. In industrial areas housing was in short supply as people doubled up and lived in cramped quarters. Prices and wages were controlled, and Americans saved a high portion of their incomes, which led to renewed growth after the war instead of a return to depression.[184][185] Americans on the home front tolerated the extra work because of patriotism, increased pay, and the confidence that it was only "for the duration," and life would return to normal as soon as the war was won.
The Allies – the United States, Britain, and the Soviet Union and other countries – saw Germany as the main threat and gave the highest priority to Europe. The U.S. dominated the war against Japan and stopped Japanese expansion in the Pacific in 1942. After the attack at Pearl Harbor and the loss of the Philippines to Japanese conquests, as well as a draw in the Battle of the Coral Sea (May 1942), the American Navy then inflicted a decisive blow at Midway (June 1942). American ground forces assisted in the North African campaign that eventually concluded with the collapse of Mussolini's fascist government in 1943, as Italy switched to the Allied side. A more significant European front was opened on D-Day, June 6, 1944, in which American and Allied forces invaded Nazi-occupied France from Britain.
War fervor also inspired anti-Japanese sentiment, leading to internment of Japanese Americans.[186] Roosevelt's Executive Order 9066 resulted in over 120,000 Americans of Japanese descent being removed from their homes and placed in internment camps. Two-thirds of those interned were American citizens and half of them were children.[187][188][189]
Military research and development also increase, leading to the Manhattan Project, a secret effort to harness nuclear fission to produce atomic bombs.[190] The first nuclear device ever detonated was conducted July 16, 1945.[191]
The Allies pushed the Germans out of France but the western front stopped short, leaving Berlin to the Soviets as the Nazi regime formally capitulated in May 1945, ending the war in Europe.[192] In the Pacific, the U.S. implemented an island hopping strategy toward Tokyo. The Philippines was eventually reconquered, after Japan and the United States fought in history's largest naval battle, "The Battle of Leyte Gulf".[193] However, the war wiped out all the development the United States invested in the Philippines as cities and towns were completely destroyed.[194]
The United States then established airfields for bombing runs against mainland Japan from the Mariana Islands, achieving hard-fought victories at Iwo Jima and Okinawa in 1945.[195] Bloodied at Okinawa, the U.S. prepared to invade Japan's home islands when B-29s dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki, compelling Japan to surrender and ending World War II.[196] The U.S. occupied Japan (and part of Germany), and restructured Japan along American lines.[197]
During the war, Roosevelt coined the term "Four Powers" to refer four major Allies of World War II, the United States, the United Kingdom, the Soviet Union, and China, which later became the foundation of the United Nations Security Council.[198] Though the nation lost more than 400,000 military personnel and civilians,[199] the U.S. mostly prospered untouched by the devastation of war that inflicted a heavy toll on Europe and Asia.
Participation in postwar foreign affairs marked the end of predominant American isolationism. The threat of nuclear weapons inspired both optimism and fear. Nuclear weapons have not been used since the war ended, and a "long peace" began between the global powers in era of competition that came to be known as the Cold War. The Truman Doctrine characterized this reality on May 22, 1947. Despite the absence of a global war during this period, there were, however, regional wars in Korea and Vietnam.[200]
Following World War II, the United States emerged as one of the two dominant superpowers, the Soviet Union being the other. The U.S. Senate on a bipartisan vote approved U.S. participation in the United Nations (UN), which marked a turn away from the traditional isolationism of the U.S. and toward increased international involvement.[201]
The primary American goal of 1945–1948 was to rescue Europe from the devastation of World War II and to contain the expansion of Communism, represented by the Soviet Union. U.S. foreign policy during the Cold War was built around the support of Western Europe and Japan along with the policy of containment, stopping the spread of communism. The U.S. joined the wars in Korea and Vietnam and toppled left-wing governments in the third world to try to stop its spread.[202]
The Truman Doctrine of 1947 provided military and economic aid to Greece and Turkey to counteract the threat of Communist expansion in the Balkans. In 1948, the United States replaced piecemeal financial aid programs with a comprehensive Marshall Plan, which pumped money into the economy of Western Europe, and removed trade barriers, while modernizing the managerial practices of businesses and governments.[203]
The Plan's $13 billion budget was in the context of a U.S. GDP of $258 billion in 1948 and was in addition to the $12 billion in American aid given to Europe between the end of the war and the start of the Marshall Plan. Soviet head of state Joseph Stalin prevented his satellite states from participating, and from that point on, Eastern Europe, with inefficient centralized economies, fell further and further behind Western Europe in terms of economic development and prosperity. In 1949, the United States, rejecting the long-standing policy of no military alliances in peacetime, formed the North Atlantic Treaty Organization (NATO) alliance, which continues into the 21st century. In response the Soviets formed the Warsaw Pact of communist states, leading to the "Iron Curtain".[203]
In August 1949 the Soviets tested their first nuclear weapon, thereby escalating the risk of warfare. The threat of mutually assured destruction however, prevented both powers from nuclear war, and resulted in proxy wars, especially in Korea and Vietnam, in which the two sides did not directly confront each other.[200]
President Dwight D. Eisenhower, elected in a landslide as the first Republican president since 1932, had a lasting impact on American life and politics.[204] He ended the Korean War, and avoided any other major conflict. He cut military spending by reliance on very high technology, such as nuclear weapons carried by long-range bombers and intercontinental missiles. He gave strong support to the NATO alliance and built other alliances along similar lines, but they never were especially effective.[205]
After Stalin died in 1953, Eisenhower worked to obtain friendlier relationships with the Soviet Union. At home, he ended McCarthyism, expanded the Social Security program and presided over a decade of bipartisan comity. He promoted civil rights cautiously, and sent in the Army when trouble threatened over racial integration in Little Rock, Arkansas.[205]
The unexpected leapfrogging of American technology by the Soviets in 1957 with Sputnik, the first Earth satellite, began the Space Race, won in 1969 by the Americans as Apollo 11 landed astronauts on the Moon. The angst about the weaknesses of American education led to large-scale federal support for science education and research.[206] In the decades after World War II, the United States became a global influence in economic, political, military, cultural, and technological affairs.
In 1960, John F. Kennedy was elected President and his administration saw the acceleration of the nation's role in the Space Race, escalation of the American role in the Vietnam War, the Bay of Pigs Invasion, the Cuban Missile Crisis, and the jailing of Martin Luther King Jr. during the Birmingham campaign. Kennedy was assassinated on November 22, 1963, leaving the nation in profound shock.[207]
President Lyndon B. Johnson secured congressional passage of his Great Society programs in the mid-1960s.[208] They included civil rights, the end of legal segregation, Medicare, extension of welfare, federal aid to education at all levels, subsidies for the arts and humanities, environmental activism, and a series of programs designed to wipe out poverty.[209][210] As later historians explained:
Gradually, liberal intellectuals crafted a new vision for achieving economic and social justice. The liberalism of the early 1960s contained no hint of radicalism, little disposition to revive new deal era crusades against concentrated economic power, and no intention to redistribute wealth or restructure existing institutions. Internationally it was strongly anti-Communist. It aimed to defend the free world, to encourage economic growth at home, and to ensure that the resulting plenty was fairly distributed. Their agenda-much influenced by Keynesian economic theory-envisioned massive public expenditure that would speed economic growth, thus providing the public resources to fund larger welfare, housing, health, and educational programs.[211]
Johnson was rewarded with an electoral landslide in 1964 against conservative Barry Goldwater, which broke the decades-long control of Congress by the Conservative coalition. However, the Republicans bounced back in 1966 and elected Richard Nixon in 1968. Nixon largely continued the New Deal and Great Society programs he inherited; conservative reaction would come with the election of Ronald Reagan in 1980.[212] Meanwhile, the American people completed a great migration from farms into the cities and experienced a period of sustained economic expansion.
Starting in the late 1950s, institutionalized racism across the United States, but especially in the South, was increasingly challenged by the growing Civil Rights Movement. The activism of African-American leaders Rosa Parks and Martin Luther King Jr. led to the Montgomery bus boycott, which launched the movement. For years African Americans would struggle with violence against them but would achieve great steps toward equality with Supreme Court decisions, including Brown v. Board of Education and Loving v. Virginia, the Civil Rights Act of 1964, the Voting Rights Act of 1965, and the Fair Housing Act of 1968, which ended the Jim Crow laws that legalized racial segregation between whites and blacks.[213]
Martin Luther King Jr., who had won the 1964 Nobel Peace Prize for his efforts to achieve equality of the races, was assassinated in 1968. Following his death others led the movement, most notably King's widow, Coretta Scott King, who was also active, like her husband, in the Opposition to the Vietnam War, and in the Women's Liberation Movement. There were 164 riots in 128 American cities in the first nine months of 1967.[214] Frustrations with the seemingly slow progress of the integration movement led to the emergence of more radical discourses during the early 1960s, which, in turn, gave rise to the Black Power movement of the late 1960s and early 1970s.[215]
The decade would ultimately bring about positive strides toward integration, especially in government service, sports, and entertainment. Native Americans turned to the federal courts to fight for their land rights. They held protests highlighting the federal government's failure to honor treaties. One of the most outspoken Native American groups was the American Indian Movement (AIM). In the 1960s, Cesar Chavez began organizing poorly paid Mexican-American farm workers in California. He led a five-year-long strike by grape pickers. Then Chávez formed the nation's first successful union of farm workers. His United Farm Workers of America (UFW) faltered after a few years but after Chavez died in 1993 he became an iconic "folk saint" in the pantheon of Mexican Americans.[216]
A new consciousness of the inequality of American women began sweeping the nation, starting with the 1963 publication of Betty Friedan's best-seller, The Feminine Mystique, which explained how many housewives felt trapped and unfulfilled, assaulted American culture for its creation of the notion that women could only find fulfillment through their roles as wives, mothers, and keepers of the home, and argued that women were just as able as men to do every type of job. In 1966 Friedan and others established the National Organization for Women (NOW) to act for women as the NAACP did for African Americans.[160][217]
Protests began, and the new women's liberation movement grew in size and power, gained much media attention, and, by 1968, had replaced the Civil Rights Movement as the U.S's main social revolution. Marches, parades, rallies, boycotts, and pickets brought out thousands, sometimes millions. There were striking gains for women in medicine, law, and business, while only a few were elected to office.[218][219]
The women's movement was split into factions by political ideology early on, with NOW on the left, the Women's Equity Action League (WEAL) on the right, the National Women's Political Caucus (NWPC) in the center, and more radical groups formed by younger women on the far-left. The proposed Equal Rights Amendment to the Constitution, passed by Congress in 1972 was defeated by a conservative coalition mobilized by Phyllis Schlafly. They argued that it degraded the position of the housewife and made young women susceptible to the military draft.[218][219]
However, many federal laws (i.e. those equalizing pay, employment, education, employment opportunities, and credit; ending pregnancy discrimination; and requiring NASA, the Military Academies, and other organizations to admit women), state laws (i.e., those ending spousal abuse and marital rape), Supreme Court rulings (i.e. ruling that the equal protection clause of the Fourteenth Amendment applied to women), and state ERAs established women's equal status under the law, and social custom and consciousness began to change, accepting women's equality. The controversial issue of abortion, deemed by the Supreme Court as a fundamental right in Roe v. Wade (1973), is still a point of debate today.[220]
Amid the Cold War, the United States entered the Vietnam War, whose growing unpopularity fed already existing social movements, including those among women, minorities, and young people. President Lyndon B. Johnson's Great Society social programs and numerous rulings by the Warren Court added to the wide range of social reform during the 1960s and 1970s. Feminism and the environmental movement became political forces, and progress continued toward civil rights for all Americans. The Counterculture Revolution swept through the nation and much of the western world in the late sixties and early seventies, further dividing Americans in a "culture war" but also bringing forth more liberated social views.[221]
Johnson was succeeded in 1969 by Republican Richard Nixon, who attempted to gradually turn the war over to the South Vietnamese forces. He negotiated the peace treaty in 1973 which secured the release of POWs and led to the withdrawal of U.S. troops. The war had cost the lives of 58,000 American troops. Nixon manipulated the fierce distrust between the Soviet Union and China to the advantage of the United States, achieving détente with both parties.[222]
The Watergate scandal, involving Nixon's cover-up of his operatives' break-in into the Democratic National Committee headquarters at the Watergate office complex destroyed his political base, sent many aides to prison, and forced Nixon's resignation on August 9, 1974. He was succeeded by Vice President Gerald Ford. The Fall of Saigon, on April 30, 1975, ended the Vietnam War and resulted in North and South Vietnam being reunited. Communist victories in neighboring Cambodia and Laos occurred in the same year, with the fall of Cambodia's capital, Phon Phen on April 17 and the taking of Laos's capital, Venintine on December 2. [222]
The OPEC oil embargo marked a long-term economic transition since, for the first time, energy prices skyrocketed, and American factories faced serious competition from foreign automobiles, clothing, electronics, and consumer goods. By the late 1970s, the economy suffered an energy crisis, slow economic growth, high unemployment, and very high inflation coupled with high-interest rates (the term stagflation was coined). Since economists agreed on the wisdom of deregulation, many of the New Deal era regulations were ended, such as in transportation, banking, and telecommunications.[223]
Jimmy Carter, running as someone who was not a part of the Washington political establishment, was elected president in 1976.[224] On the world stage, Carter brokered the Camp David Accords between Israel and Egypt. In 1979, Iranian students stormed the U.S. embassy in Tehran and took 66 Americans hostage, resulting in the Iran hostage crisis. With the hostage crisis and continuing stagflation, Carter lost the 1980 election to the Republican Ronald Reagan.[225] On January 20, 1981, minutes after Carter's term in office ended, the remaining U.S. captives held at the U.S. embassy in Iran were released, ending the 444-day hostage crisis.[226]
Ronald Reagan produced a major political realignment with his 1980 and 1984 landslide elections. Reagan's economic policies (dubbed "Reaganomics") and the implementation of the Economic Recovery Tax Act of 1981 lowered the top marginal tax rate from 70% to 28% over the course of seven years.[227] Reagan continued to downsize government taxation and regulation.[228] The U.S. experienced a recession in 1982, but the negative indicators reversed, with the inflation rate decreasing from 11% to 2%, the unemployment rate decreasing from 10.8% in December 1982 to 7.5% in November 1984,[229] and the economic growth rate increasing from 4.5% to 7.2%.[230]
Reagan ordered a buildup of the U.S. military, incurring additional budget deficits. Reagan introduced a complicated missile defense system known as the Strategic Defense Initiative (SDI) (dubbed "Star Wars" by opponents) in which, theoretically, the U.S. could shoot down missiles with laser systems in space. The Soviets reacted harshly because they thought it violated the 1972 Anti-Ballistic Missile Treaty and would upset the balance of power by giving the U.S. a major military advantage. For years Soviet general secretary Mikhail Gorbachev argued vehemently against SDI. However, by the late 1980s he decided the system would never work and should not be used to block disarmament deals with the U.S.[231]
Historians argue how great an impact the SDI threat had on the Soviets – whether it was enough to force Gorbachev to initiate radical reforms, or whether the deterioration of the Soviet economy alone forced the reforms. There is agreement that the Soviets realized they were well behind the Americans in military technology, that to try to catch up would be very expensive, and that the military expenses were already a very heavy burden slowing down their economy.[232]
The 1983 Invasion of Grenada and 1986 bombing of Libya were popular in the U.S., though his backing of the Contras rebels was mired in the controversy over the Iran–Contra affair.[233]
Reagan met four times with Soviet leader Mikhail Gorbachev, who ascended to power as General Secretary of the Communist Party in 1985, and their summit conferences led to the signing of the Intermediate-Range Nuclear Forces Treaty. Gorbachev tried to save Communism in the Soviet Union first by ending the expensive nuclear arms race with America,[234] then by shedding the East European empire in 1989. The Soviet Union collapsed on Christmas Day 1991, ending the U.S–Soviet Cold War.
The United States emerged as the world's sole remaining superpower and continued to intervene in international affairs during the 1990s, including the 1991 Gulf War against Iraq. Following his election in 1992, President Bill Clinton oversaw one of the longest periods of economic expansion and unprecedented gains in securities values. President Clinton worked with the Republican Congress to pass the first balanced federal budget in 30 years.[235]Much of the econmic boom was a side effect of the Digital Revolution and new business opportunities created by the internet privatized in 1993. Prior to this time ARPNET a Department of Defense Project had developed the internet for governmental, and research purposes.[236]
In 1998, Clinton was impeached by the House of Representatives on charges of lying under oath about (perjury regarding) a sexual relationship with White House intern Monica Lewinsky. He was acquitted by the Senate. The failure of impeachment and the Democratic gains in the 1998 election forced House Speaker Newt Gingrich, a Republican, to resign from Congress.[235]
The Republican Party expanded its base throughout the South after 1968 (excepting 1976), largely due to its strength among socially conservative white Evangelical Protestants and traditionalist Roman Catholics, added to its traditional strength in the business community and suburbs. As white Democrats in the South lost dominance of the Democratic Party in the 1990s, the region took on the two-party apparatus which characterized most of the nation. The Republican Party's central leader by 1980 was Ronald Reagan, whose conservative policies called for reduced government spending and regulation, lower taxes, and a strong anti-Soviet foreign policy.[237]
His iconic status in the party persists into the 21st century, as practically all Republican Party leaders acknowledge his stature. Social scientists Theodore Caplow et al. argue, "The Republican party, nationally, moved from right-center toward the center in 1940s and 1950s, then moved right again in the 1970s and 1980s." They add: "The Democratic party, nationally, moved from left-center toward the center in the 1940s and 1950s, then moved further toward the right-center in the 1970s and 1980s."[237]
The close presidential election in 2000 between Governor George W. Bush and Al Gore helped lay the seeds for political polarization to come. The vote in the decisive states of New Mexico and Florida was extremely close and produced a dramatic dispute over the counting of votes.[238] Including 2000, the Democrats outpolled the Republicans in the national vote in every election from 1992 to 2020, except for 2004.[239]
On September 11, 2001 ("9/11"), the United States was struck by a terrorist attack when 19 al-Qaeda hijackers commandeered four airliners to be used in suicide attacks and intentionally crashed two into both twin towers of the World Trade Center and the third into the Pentagon, killing 2,937 victims—206 aboard the three airliners, 2,606 who were in the World Trade Center and on the ground, and 125 who were in the Pentagon.[240] The fourth plane was re-taken by the passengers and crew of the aircraft. While they were not able to land the plane safely, they were able to re-take control of the aircraft and crash it into an empty field in Pennsylvania, killing all 44 people including the four terrorists on board, thereby saving whatever target the terrorists were aiming for. Within two hours, both Twin Towers of the World Trade Center completely collapsed causing massive damage to the surrounding area and blanketing Lower Manhattan in toxic dust clouds. All in all, a total of 2,977 victims perished in the attacks. In response, President George W. Bush on September 20 announced a "war on terror". On October 7, 2001, the United States and NATO then invaded Afghanistan to oust the Taliban regime, which had provided safe haven to al-Qaeda and its leader Osama bin Laden.[241]
The federal government established new domestic efforts to prevent future attacks. The USA PATRIOT Act increased the government's power to monitor communications and removed legal restrictions on information sharing between federal law enforcement and intelligence services. The U.S. Department of Homeland Security was created to lead and coordinate federal counter-terrorism activities.[242] Since 2002, the U.S. government's indefinite detention of terrorism suspects captured abroad at the Guantanamo Bay detention camp, a prison at the U.S. naval base in Guantanamo Bay, Cuba, led to allegations of human rights abuses and violations of international law.[243][244][245]
In 2003, from March 19 to May 1, the United States launched an invasion of Iraq, which led to the collapse of the Iraq government and the eventual capture of Iraqi dictator Saddam Hussein, with whom the U.S. had long-standing tense relations. The reasons for the invasion cited by the Bush administration included the spreading of democracy, the elimination of weapons of mass destruction[246] (a key demand of the UN as well, though later investigations found parts of the intelligence reports to be inaccurate),[247] and the liberation of the Iraqi people. Despite some initial successes early in the invasion, the continued Iraq War fueled international protests and gradually saw domestic support decline as many people began to question whether or not the invasion was worth the cost.[248][249] In 2007, after years of violence by the Iraqi insurgency, President Bush deployed more troops in a strategy dubbed "the surge". While the death toll decreased, the political stability of Iraq remained in doubt.[250]
In 2008, the unpopularity of President Bush and the Iraq war, along with the 2008 financial crisis, led to the election of Barack Obama, the first multiracial[251] president, with African-American ancestry.[252] After his election, Obama reluctantly continued the war effort in Iraq until August 31, 2010, when he declared that combat operations had ended. However, 50,000 American soldiers and military personnel were kept in Iraq to assist Iraqi forces, help protect withdrawing forces, and work on counter-terrorism until December 15, 2011, when the war was declared formally over and the last troops left the country.[253] At the same time, Obama increased American involvement in Afghanistan, starting a surge strategy using an additional 30,000 troops, while proposing to begin withdrawing troops sometime in December 2014. In 2009, on his second day in office, Obama issued an executive order banning the use of torture,[254][255] a prohibition codified into law in 2016.[255] Obama also ordered the closure of secret CIA-run prisons overseas ("black sites").[256][257] Obama sought to close the Guantanamo Bay detention camp "as soon as practicable" and over his tenure the population of the detention camp declined from 242 inmates to 45 inmates; the Guantanamo Review Task Force cleared many prisoners for release and resettlement abroad.[258][259] Obama's efforts to close the prison entirely were stymied by Congress, which in 2011 enacted a measure blocking Obama from transferring any Guantanamo detainees to U.S. facilities.[258]
In May 2011, after nearly a decade in hiding, the founder and leader of Al Qaeda, Osama bin Laden, was killed in Pakistan in a raid conducted by U.S. naval special forces acting under President Obama's direct orders. While Al Qaeda was near collapse in Afghanistan, affiliated organizations continued to operate in Yemen and other remote areas as the CIA used drones to hunt down and remove its leadership.[260][261]
The Boston Marathon bombing was a bombing incident, followed by subsequent related shootings, that occurred when two pressure cooker bombs exploded during the Boston Marathon on April 15, 2013. The bombs exploded near the marathon's finish line, killing 3 people and injuring an estimated 264 others.
The Islamic State of Iraq and the Levant rose to prominence in September 2014. In addition to taking control of much of Western Iraq and Eastern Syria, ISIS also beheaded three journalists, two American and one British. These events lead to a major military offensive by the United States and its allies in the region.
On December 28, 2014, Obama officially ended the combat mission in Afghanistan and promised a withdrawal of all remaining U.S. troops at the end of 2016 with the exception of the embassy guards.[262] The US military mission formally ended on August 30, 2021.[263]
In September 2008, the United States and most of Europe entered the longest post–World War II recession, often called the "Great Recession".[264][265] Multiple overlapping crises were involved, especially the housing market crisis, a subprime mortgage crisis, soaring oil prices, an automotive industry crisis, rising unemployment, and the worst financial crisis since the Great Depression. The financial crisis threatened the stability of the entire economy in September 2008 when Lehman Brothers failed and other giant banks were in grave danger.[266] Starting in October the federal government lent $245 billion to financial institutions through the Troubled Asset Relief Program[267] which was passed by bipartisan majorities and signed by Bush.[268]
Following his election victory by a wide electoral margin in November 2008, Bush's successor – Barack Obama – signed into law the American Recovery and Reinvestment Act of 2009, which was a $787 billion economic stimulus aimed at helping the economy recover from the deepening recession. Obama, like Bush, took steps to rescue the auto industry and prevent future economic meltdowns. These included a bailout of General Motors and Chrysler, putting ownership temporarily in the hands of the government, and the "cash for clunkers" program which temporarily boosted new car sales.[269]
The recession officially ended in June 2009, and the economy slowly began to expand once again.[270] Beginning in December 2007, the unemployment rate steeply rose from around 5% to a peak of 10% before falling as the economy and labor markets experienced a recovery.[271] The economic expansion that followed the Great Recession was the longest in U.S. history;[272][273] strong growth led to the unemployment rate reaching a 50-year low in 2019.[274] Despite the strong economy, increases in the costs of housing, child care, higher education, and out-of-pocket healthcare expenses surpassed increases in wages, a phenomenon some referred to as an affordability crisis.[275][276] The economic expansion came to an end in early 2020 with a sharp economic contraction largely caused by the COVID-19 pandemic, which seriously affected the United States.[272][273]
From 2009 to 2010, the 111th Congress passed major legislation such as the Patient Protection and Affordable Care Act, informally known as Obamacare, the Dodd–Frank Wall Street Reform and Consumer Protection Act[277] and the Don't Ask, Don't Tell Repeal Act, which were signed into law by President Obama.[278] Following the 2010 midterm elections, which resulted in a Republican-controlled House of Representatives and a Democratic-controlled Senate,[279] Congress presided over a period of elevated gridlock and heated debates over whether or not to raise the debt ceiling, extend tax cuts for citizens making over $250,000 annually, and many other key issues.[280] These ongoing debates led to President Obama signing the Budget Control Act of 2011. Following Obama's 2012 re-election, Congressional gridlock continued as Congressional Republicans' call for the repeal of the Patient Protection and Affordable Care Act along with other various demands, resulted in the first government shutdown since the Clinton administration and almost led to the first default on U.S. debt since the 19th century. As a result of growing public frustration with both parties in Congress since the beginning of the decade, Congressional approval ratings fell to record lows.[281]
Recent events also include the rise of new political movements, such as the conservative Tea Party movement and the liberal Occupy movement. The debate over the issue of rights for the LGBT community, including same-sex marriage, began to shift in favor of same-sex couples.[282] In 2012, President Obama became the first president to openly support same-sex marriage, and the Supreme Court provided for federal recognition of same-sex unions and then nationwide legalized gay marriage in 2015.
Political debate has continued over tax reform, immigration reform, income inequality, and U.S. foreign policy in the Middle East, particularly with regards to global terrorism, the rise of the Islamic State of Iraq and the Levant and an accompanying climate of Islamophobia.[283]
The late 2010s were marked by widespread social upheaval and change in the United States. The #MeToo movement gained popularity, exposing alleged sexual harassment and abuse in the workplace.[284] Multiple prominent celebrities were accused of misconduct or rape.[285] During this period, the Black Lives Matter movement also gained support online, exacerbated by the police killings of multiple black Americans.[286] Multiple mass shootings, including the Pulse Nightclub shooting (2016) and the Las Vegas shooting, which claimed the lives of 61 people, led to increased calls for gun control and reform. Following the Stoneman Douglas High School shooting in 2018, gun control advocates organized the March for Our Lives, where millions of students across the country walked out of school to protest gun violence.[287][288] The Women's March protest against Trump's presidency in 2017 was one of the largest protests in American history.[289]
In 2016, following a contentious election, Republican Donald Trump was elected president.[290] The results of the election were called into question, and U.S. intelligence agencies concluded that associates of the Russian government interfered in the election "to undermine public faith in the U.S. democratic process". This, along with questions about potential collusion between the Trump campaign and Russian officials, led to investigations by the FBI and Congress.[291][292]
During Trump's presidency, he espoused an "America First" ideology, placing restrictions on asylum seekers and imposing a widely controversial ban on immigration from seven Muslim-majority nations. Many of his executive orders and other actions were challenged in court.[293][294] During his presidency he also engaged the United States in a trade war with China, imposing a wide range of tariffs on Chinese products.[295] In 2018, controversy erupted over the Trump administration's "zero tolerance" policy towards illegal immigrants, which involved the separation of thousands of undocumented children from their parents. After public outcry, Trump rescinded this policy.[296] Trump's term also saw the confirmation of three new justices to the Supreme Court, cementing a conservative majority.
In 2019, a whistleblower complaint alleged that Trump had withheld foreign aid from Ukraine under the demand that they investigate the business dealings of the son of Trump's political opponent.[297] As a result, Trump was impeached for abuse of power and obstruction of congress, becoming the third president to have been impeached, but he was acquitted.[298]
The worldwide COVID-19 pandemic having arrived in the United States was first confirmed in January 2020. As of May 2022[update], the U.S. has suffered more official COVID-19 deaths than any other nation with the death toll standing at 1 million,[299] with the U.S. death toll surpassing the number of U.S. deaths during the Spanish flu pandemic, although the Spanish flu killed 1 in 150 Americans compared to COVID-19 killing 1 in 500 Americans.[300] As a result of the COVID-19 pandemic, U.S. life expectancy fell by over a year in 2020 and unemployment rates rose to the worst rates since the Great Depression.[301] In 2021, U.S. life expectancy decreased by around half a year.[302] The May 2020 murder of George Floyd caused mass protests and riots in many major cities over police brutality, with many states calling in the National Guard.[303]
2020 was marked by a rise in domestic terrorist threats and widespread conspiracy theories around mail-in voting and COVID-19.[304][305] The QAnon conspiracy theory, a fringe far-right political movement among some ardent conservatives, gained publicity and multiple major cities were hit by rioting and brawls between far-left antifascist affiliated groups and far right groups such as the Proud Boys.[306][307][308]
Democrat Joe Biden defeated Trump in the 2020 presidential election, the first defeat of an incumbent president since 1992.[309] The election, with an exceptional amount of voting by mail and early voting due to the danger of contracting COVID-19 at traditional voting booths, had historically high voter turnout.[310] Trump then repeatedly made false claims of massive voter fraud and election rigging, leading to the January 6 United States Capitol attack by supporters of Trump and right-wing militias.[311][312][313][314][315] That storming led to Trump's impeachment, as the only U.S. president to be impeached twice.[316][317][318] The Senate later acquitted Trump despite some members of his own Republican party voting against him.[319][320] After the 2021 inauguration, Biden's running-mate, then-Senator Kamala Harris, became both the first African-American and first woman vice president of the United States.[321]
The biggest mass vaccination campaign in U.S. history kicked off on December 14, 2020, when ICU nurse Sandra Lindsay became the first person in the U.S. to receive the Pfizer-BioNTech COVID-19 vaccine. As of August 2021, 60% of the U.S. population has received a dose of the Pfizer BioNTech, Moderna, or Johnson & Johnson COVID-19 vaccine.
Following Biden's election, the date for US troops to withdraw from Afghanistan was moved back from April to August 31, 2021.[322] In Afghanistan, the withdrawal coincided with the 2021 Taliban offensive, culminating in the fall of Kabul. Following a massive airlift of over 120,000 people, the US military mission formally ended on August 30, 2021.
On June 24, 2022, the Supreme Court, in a landmark ruling, determined that abortion is not a protected right under the Consititution.[323] The ruling, Dobbs v. Jackson Women's Health Organization overturned Roe v. Wade and Planned Parenthood v. Casey and sparked protests outside of the Supreme Court building and across the country.[324]
Ketanji Brown Jackson succeeded Justice Breyer upon his retirement from the court on June 30, 2022.[325] She became the first black woman and the first former federal public defender to serve on the Supreme Court upon her swearing in.[326][327]

The earliest known written records of the history of China date from as early as 1250 BC, from the Shang dynasty (c. 1600–1046 BC), during the reign of king Wu Ding,[1][2] referred to in the records as the twenty-first King of Shang.[3][4] Ancient historical texts such as the Book of Documents (early chapters, 11th century BC), the Bamboo Annals (c. 296 BC) and the Records of the Grand Historian (c. 91 BC) mention and describe a Xia dynasty (c. 2070–1600 BC) before the Shang, but no writing is known from the period, and Shang writings do not indicate the existence of the Xia. The Shang ruled in the Yellow River valley, which is commonly held to be the cradle of Chinese civilization. However, Neolithic civilizations originated at various cultural centers along both the Yellow River and Yangtze River. These Yellow River and Yangtze civilizations arose millennia before the Shang. With thousands of years of continuous history, China is among the world's oldest civilizations and is regarded as one of the cradles of civilization.[5][6]
The Zhou dynasty (1046–256 BC) supplanted the Shang, and introduced the concept of the Mandate of Heaven to justify their rule. The central Zhou government began to weaken due to external and internal pressures in the 8th century BC, and the country eventually splintered into smaller states during the Spring and Autumn period. These states became independent and fought with one another in the following Warring States period. Much of traditional Chinese culture, literature and philosophy first developed during those troubled times.
In 221 BC, Qin Shi Huang conquered the various warring states and created for himself the title of Huangdi or "emperor" of the Qin, marking the beginning of imperial China. However, the oppressive government fell soon after his death, and was supplanted by the longer-lived Han dynasty (206 BC – 220 AD). Successive dynasties developed bureaucratic systems that enabled the emperor to control vast territories directly. In the 21 centuries from 206 BC until AD 1912, routine administrative tasks were handled by a special elite of scholar-officials. Young men, well-versed in calligraphy, history, literature, and philosophy, were carefully selected through difficult government examinations. China's last dynasty was the Qing (1636–1912), which was replaced by the Republic of China in 1912, and then in the mainland by the People's Republic of China in 1949. The Republic of China retreated to the island of Taiwan in 1949. Both the PRC and the ROC currently claim to be the sole legitimate government of China, resulting in an ongoing dispute even after the United Nations recognized the PRC as the government to represent China at all UN conferences in 1971. Hong Kong and Macau transferred sovereignty to China in 1997 and 1999 from the United Kingdom and Portugal respectively, becoming special administrative regions (SARs) of the PRC.
Chinese history has alternated between periods of political unity and peace, and periods of war and failed statehood—the most recent being the Chinese Civil War (1927–1949). China was occasionally dominated by steppe peoples, especially the Mongols and Manchus, most of whom were eventually assimilated into the Han Chinese culture and population. Between eras of multiple kingdoms and warlordism, Chinese dynasties have ruled parts or all of China; in some eras control stretched as far as Xinjiang, Tibet and Inner Mongolia, as at present. Traditional culture, and influences from other parts of Asia and the Western world (carried by waves of immigration, cultural assimilation, expansion, and foreign contact), form the basis of the modern culture of China.
What is now China was inhabited by Homo erectus more than one million years ago.[7] Recent study shows that the stone tools found at Xiaochangliang site are magnetostratigraphically dated to 1.36 million years ago.[8] The archaeological site of Xihoudu in Shanxi Province has evidence of use of fire by Homo erectus,[9] which is dated 1.27 million years ago,[7] and Homo erectus fossils in China include the Yuanmou Man, the Lantian Man and the Peking Man. Fossilised teeth of Homo sapiens dating to 125,000–80,000 BC have been discovered in Fuyan Cave in Dao County in Hunan.[10] Evidence of Middle Palaeolithic Levallois technology has been found in the lithic assemblage of Guanyindong Cave site in southwest China, dated to approximately 170,000–80,000 years ago.[11]
The Neolithic age in China can be traced back to about 10,000 BC.[12] One of the defining traits of the Neolithic is agriculture. Agriculture in China developed gradually, with initial domestication of a few grains and animals gradually being expanded by the addition of many others over subsequent millennia.[13] The earliest evidence of cultivated rice, found by the Yangtze River, is carbon-dated to 8,000 years ago.[14] Early evidence for proto-Chinese millet agriculture is radiocarbon-dated to about 7000 BC.[15] Farming gave rise to the Jiahu culture (7000 to 5800 BC). At Damaidi in Ningxia, 3,172 cliff carvings dating to 6000–5000 BC have been discovered, "featuring 8,453 individual characters such as the sun, moon, stars, gods and scenes of hunting or grazing", according to researcher Li Xiangshi. These pictographs are reputed to be similar to the earliest characters confirmed to be written Chinese.[16] Chinese proto-writing existed in Jiahu around 7000 BC,[17] Damaidi around 6000 BC, Dadiwan from 5800 BC to 5400 BC,[18] and Banpo dating from the 5th millennium BC. With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators.[14] The cultures of the middle and late Neolithic in the central Yellow River valley are known respectively as the Yangshao culture (5000 BC to 3000 BC) and the Longshan culture (3000 BC to 2000 BC). During the latter period domesticated cattle and sheep arrived from Western Asia. Wheat also arrived, but remained a minor crop.
Bronze artifacts have been found at the Majiayao culture site (between 3100 and 2700 BC).[19][20] The Bronze Age is also represented at the Lower Xiajiadian culture (2200–1600 BC[21]) site in northeast China. Sanxingdui located in what is now Sichuan province is believed to be the site of a major ancient city, of a previously unknown Bronze Age culture (between 2000 and 1200 BC). The site was first discovered in 1929 and then re-discovered in 1986. Chinese archaeologists have identified the Sanxingdui culture to be part of the ancient kingdom of Shu, linking the artifacts found at the site to its early legendary kings.[22][23]

Ferrous metallurgy begins to appear in the late 6th century in the Yangzi Valley.[24]
A bronze tomahawk with a blade of meteoric iron excavated near the city of Gaocheng in Shijiazhuang (now Hebei province) has been dated to the 14th century BC.
An Iron Age culture of the Tibetan Plateau has tentatively been associated with the Zhang Zhung culture described in early Tibetan writings.
Chinese historians in later periods were accustomed to the notion of one dynasty succeeding another, but the political situation in early China was much more complicated. Hence, as some scholars of China suggest, the Xia and the Shang can refer to political entities that existed concurrently, just as the early Zhou existed at the same time as the Shang.[25]
The Xia dynasty of China (from c. 2070 to c. 1600 BC) is the earliest of the Three Dynasties described in ancient historical records such as Sima Qian's Records of the Grand Historian and Bamboo Annals. The dynasty is generally considered mythical by Western scholars, but in China it is usually associated with the early Bronze Age site at Erlitou that was excavated in Henan in 1959. Since no writing was excavated at Erlitou or any other contemporaneous site, there is no way to prove whether the Xia dynasty ever existed. In any case, the site of Erlitou had a level of political organization that would not be incompatible with the legends of Xia recorded in later texts.[26] More importantly, the Erlitou site has the earliest evidence for an elite who conducted rituals using cast bronze vessels, which would later be adopted by the Shang and Zhou.[27]
Archaeological evidence, such as oracle bones and bronzes, as well as transmitted texts attest to the historical existence of the Shang dynasty (c. 1600–1046 BC). Findings from the earlier Shang period comes from excavations at Erligang, in present-day Zhengzhou. Findings from the later Shang or Yin (殷) period, were found in profusion at Anyang, in modern-day Henan, [28] the last of the Shang's capitals. The findings at Anyang include the earliest written record of the Chinese so far discovered: inscriptions of divination records in ancient Chinese writing on the bones or shells of animals—the "oracle bones", dating from around 1250 to 1046 BC.[1]
A series of thirty-one kings reigned over the Shang dynasty. During their reign, according to the Records of the Grand Historian, the capital city was moved six times.[29] The final (and most important) move was to Yin in around 1300 BC which led to the dynasty's golden age.[29] The term Yin dynasty has been synonymous with the Shang dynasty in history, although it has lately been used to refer specifically to the latter half of the Shang dynasty.
Although written records found at Anyang confirm the existence of the Shang dynasty,[30] Western scholars are often hesitant to associate settlements that are contemporaneous with the Anyang settlement with the Shang dynasty. For example, archaeological findings at Sanxingdui suggest a technologically advanced civilization culturally unlike Anyang. The evidence is inconclusive in proving how far the Shang realm extended from Anyang. The leading hypothesis is that Anyang, ruled by the same Shang in the official history, coexisted and traded with numerous other culturally diverse settlements in the area that is now referred to as China proper.[27]
The Zhou dynasty (1046 BC to approximately 256 BC) is the longest-lasting dynasty in Chinese history, though its power declined steadily over the almost eight centuries of its existence. In the late 2nd millennium BC, the Zhou dynasty arose in the Wei River valley of modern western Shaanxi Province, where they were appointed Western Protectors by the Shang. A coalition led by the ruler of the Zhou, King Wu, defeated the Shang at the Battle of Muye. They took over most of the central and lower Yellow River valley and enfeoffed their relatives and allies in semi-independent states across the region.[31] Several of these states eventually became more powerful than the Zhou kings.
The kings of Zhou invoked the concept of the Mandate of Heaven to legitimize their rule, a concept that was influential for almost every succeeding dynasty.[32] Like Shangdi, Heaven (tian) ruled over all the other gods, and it decided who would rule China.[33] It was believed that a ruler lost the Mandate of Heaven when natural disasters occurred in great number, and when, more realistically, the sovereign had apparently lost his concern for the people. In response, the royal house would be overthrown, and a new house would rule, having been granted the Mandate of Heaven.
The Zhou established two capitals Zongzhou (near modern Xi'an) and Chengzhou (Luoyang), with the king's court moving between them regularly. The Zhou alliance gradually expanded eastward into Shandong, southeastward into the Huai River valley, and southward into the Yangtze River valley.[31]
In 771 BC, King You and his forces were defeated in the Battle of Mount Li by rebel states and Quanrong barbarians. The rebel aristocrats established a new ruler, King Ping, in Luoyang,[34] beginning the second major phase of the Zhou dynasty: the Eastern Zhou period, which is divided into the Spring and Autumn and Warring States periods. The former period is named after the famous Spring and Autumn Annals. The decline of central power left a vacuum. The Zhou empire now consisted of hundreds of tiny states, some of them only as large as a walled town and surrounding land. These states began to fight against one another and vie for hegemony. The more powerful states tended to conquer and incorporate the weaker ones, so the number of states declined over time.[35] By the 6th century BC most small states had disappeared by being annexed and just a few large and powerful principalities remained. Some southern states, such as Chu and Wu, claimed independence from the Zhou, who undertook wars against some of them (Wu and Yue). Many new cities were established in this period and society gradually became more urbanized and commercialized. Many famous individuals such as Laozi, Confucius and Sun Tzu lived during this chaotic period.
Conflict in this period occurred both between and within states. Warfare between states forced the surviving states to develop better administrations to mobilize more soldiers and resources. Within states there was constant jockeying between elite families. For example, the three most powerful families in the Jin state—Zhao, Wei and Han—eventually overthrew the ruling family and partitioned the state between them.
The Hundred Schools of Thought of classical Chinese philosophy began blossoming during this period and the subsequent Warring States period. Such influential intellectual movements as Confucianism, Taoism, Legalism and Mohism were founded, partly in response to the changing political world. The first two philosophical thoughts would have an enormous influence on Chinese culture.
After further political consolidation, seven prominent states remained by the end of the 5th century BC, and the years in which these few states battled each other are known as the Warring States period. Though there remained a nominal Zhou king until 256 BC, he was largely a figurehead and held little real power.
Numerous developments were made during this period in culture and mathematics. Examples include an important literary achievement, the Zuo zhuan on the Spring and Autumn Annals, which summarizes the preceding Spring and Autumn period, and the bundle of 21 bamboo slips from the Tsinghua collection, which was invented during this period dated to 305 BC, are the world's earliest example of a two digit decimal multiplication table, indicating that sophisticated commercial arithmetic was already established during this period.[36]
As neighboring territories of these warring states, including areas of modern Sichuan and Liaoning, were annexed, they were governed under the new local administrative system of commandery and prefecture. This system had been in use since the Spring and Autumn period, and parts can still be seen in the modern system of Sheng and Xian (province and county).
The state of Qin became dominant in the last century of this period. Qin conquered the state of Shu in the Chengdu Plain, and eventually drove Chu from its home in the Han River valley. Qin copied the administrative reforms of the other states to become a powerhouse.[13] The final expansion in this period began during the reign of Ying Zheng, the king of Qin. His unification of the other six powers enabled him to proclaim himself the First Emperor (Qin Shi Huang).
Though the unified reign of Qin Shi Huang, the first Qin emperor, lasted only 12 years, he managed to subdue great parts of what constitutes the core of the Han Chinese homeland and to unite them under a tightly centralized Legalist government seated at Xianyang (close to modern Xi'an). The doctrine of Legalism that guided the Qin emphasized strict adherence to a legal code and the absolute power of the emperor. This philosophy, while effective for expanding the empire in a military fashion, proved unworkable for governing it in peacetime. The Qin Emperor presided over the brutal silencing of political opposition, including the event known as the burning of books and burying of scholars. This would be the impetus behind the later Han synthesis incorporating the more moderate schools of political governance.
Major contributions of the Qin include the concept of a centralized government, and the unification and development of the legal code, the written language, measurement, and currency of China after the divergences of the Spring and Autumn and Warring States periods. Even something as basic as the length of axles for carts—which need to match ruts in the roads—had to be made uniform to ensure a viable trading system throughout the empire. Also as part of its centralization, the Qin connected the northern border walls of the states it defeated, making the first, though rough, version of the Great Wall of China. The Qin Empire's economy was based on the grain taxes paid by its subjects as well as the corvée labor they served during the agricultural off-season.[13] This is now well understood because large numbers of Qin administrative texts have been excavated.
Qin's conquest and colonization of the Yangzi Valley played an important role in bringing this area under the control of Chinese empires.[37]
The tribes of the north, collectively called the Wu Hu by the Qin, were free from Chinese rule during the majority of the dynasty.[38] Prohibited from trading with Qin dynasty peasants, the Xiongnu tribe living in the Ordos region in northwest China often raided them instead, prompting the Qin to retaliate. After a military campaign led by General Meng Tian, the region was conquered in 215 BC and agriculture was established; the peasants, however, were discontented and later revolted. The succeeding Han dynasty also expanded into the Ordos due to overpopulation, but depleted their resources in the process. Indeed, this was true of the dynasty's borders in multiple directions; modern Inner Mongolia, Xinjiang, Tibet, Manchuria, and regions to the southeast were foreign to the Qin, and even areas over which they had military control were culturally distinct.[39]
After Qin Shi Huang's death the Qin government drastically deteriorated and eventually capitulated in 207 BC after the Qin capital was captured and sacked by rebels, which would ultimately lead to the establishment of the Han Empire.[40] Despite the short duration of the Qin dynasty, it was immensely influential on China and the structure of future Chinese dynasties.
The Han dynasty was founded by Liu Bang, who emerged victorious in the Chu–Han Contention that followed the fall of the Qin dynasty. A golden age in Chinese history, the Han dynasty's long period of stability and prosperity consolidated the foundation of China as a unified state under a central imperial bureaucracy, which was to last intermittently for most of the next two millennia. During the Han dynasty, territory of China was extended to most of the China proper and to areas far west. Confucianism was officially elevated to orthodox status and was to shape the subsequent Chinese civilization. Art, culture and science all advanced to unprecedented heights. With the profound and lasting impacts of this period of Chinese history, the dynasty name "Han" had been taken as the name of the Chinese people, now the dominant ethnic group in modern China, and had been commonly used to refer to Chinese language and written characters. After the initial laissez-faire policies of Emperors Wen and Jing, the ambitious Emperor Wu brought the empire to its zenith. To consolidate his power, he extended patronage to Confucianism, which emphasizes stability and order in a well-structured society. Imperial Universities were established to support its study. At the urging of his Legalist advisors, however, he also strengthened the fiscal structure of the dynasty with government monopolies.
Major military campaigns were launched to weaken the nomadic Xiongnu Empire, limiting their influence north of the Great Wall. Along with the diplomatic efforts led by Zhang Qian, the sphere of influence of the Han Empire extended to the states in the Tarim Basin, opened up the Silk Road that connected China to the west, stimulating bilateral trade and cultural exchange. To the south, various small kingdoms far beyond the Yangtze River Valley were formally incorporated into the empire.
Emperor Wu also dispatched a series of military campaigns against the Baiyue tribes. The Han annexed Minyue in 135 BC and 111 BC, Nanyue in 111 BC, and Dian in 109 BC.[41] Migration and military expeditions led to the cultural assimilation of the south.[42] It also brought the Han into contact with kingdoms in Southeast Asia, introducing diplomacy and trade.[43]
After Emperor Wu, the empire slipped into gradual stagnation and decline. Economically, the state treasury was strained by excessive campaigns and projects, while land acquisitions by elite families gradually drained the tax base. Various consort clans exerted increasing control over strings of incompetent emperors and eventually the dynasty was briefly interrupted by the usurpation of Wang Mang.
In AD 9, the usurper Wang Mang claimed that the Mandate of Heaven called for the end of the Han dynasty and the rise of his own, and he founded the short-lived Xin dynasty. Wang Mang started an extensive program of land and other economic reforms, including the outlawing of slavery and land nationalization and redistribution. These programs, however, were never supported by the landholding families, because they favored the peasants. The instability of power brought about chaos, uprisings, and loss of territories. This was compounded by mass flooding of the Yellow River; silt buildup caused it to split into two channels and displaced large numbers of farmers. Wang Mang was eventually killed in Weiyang Palace by an enraged peasant mob in AD 23.
Emperor Guangwu reinstated the Han dynasty with the support of landholding and merchant families at Luoyang, east of the former capital Xi'an. Thus, this new era is termed the Eastern Han dynasty. With the capable administrations of Emperors Ming and Zhang, former glories of the dynasty was reclaimed, with brilliant military and cultural achievements. The Xiongnu Empire was decisively defeated. The diplomat and general Ban Chao further expanded the conquests across the Pamirs to the shores of the Caspian Sea,[44] thus reopening the Silk Road, and bringing trade, foreign cultures, along with the arrival of Buddhism. With extensive connections with the west, the first of several Roman embassies to China were recorded in Chinese sources, coming from the sea route in AD 166, and a second one in AD 284.
The Eastern Han dynasty was one of the most prolific era of science and technology in ancient China, notably the historic invention of papermaking by Cai Lun, and the numerous scientific and mathematical contributions by the famous polymath Zhang Heng.
By the 2nd century, the empire declined amidst land acquisitions, invasions, and feuding between consort clans and eunuchs. The Yellow Turban Rebellion broke out in AD 184, ushering in an era of warlords. In the ensuing turmoil, three states emerged, trying to gain predominance and reunify the land, giving this historical period its name. The classic historical novel Romance of the Three Kingdoms dramatizes events of this period.
The warlord Cao Cao reunified the north in 208, and in 220 his son accepted the abdication of Emperor Xian of Han, thus initiating the Wei dynasty. Soon, Wei's rivals Shu and Wu proclaimed their independence. This period was characterized by a gradual decentralization of the state that had existed during the Qin and Han dynasties, and an increase in the power of great families.
In 266, the Jin dynasty overthrew the Wei and later unified the country in 280, but this union was short-lived.
The Jin dynasty was severely weakened by internecine fighting among imperial princes and lost control of northern China after non-Han Chinese settlers rebelled and captured Luoyang and Chang'an. In 317, a Jin prince in modern-day Nanjing became emperor and continued the dynasty, now known as the Eastern Jin, which held southern China for another century. Prior to this move, historians refer to the Jin dynasty as the Western Jin.
Northern China fragmented into a series of independent kingdoms, most of which were founded by Xiongnu, Xianbei, Jie, Di and Qiang rulers. These non-Han peoples were ancestors of the Turks, Mongols, and Tibetans. Many had, to some extent, been "sinicized" long before their ascent to power. In fact, some of them, notably the Qiang and the Xiongnu, had already been allowed to live in the frontier regions within the Great Wall since late Han times. During the period of the Sixteen Kingdoms, warfare ravaged the north and prompted large-scale Han Chinese migration south to the Yangtze River Basin and Delta.
In the early 5th century, China entered a period known as the Northern and Southern dynasties, in which parallel regimes ruled the northern and southern halves of the country. In the south, the Eastern Jin gave way to the Liu Song, Southern Qi, Liang and finally Chen. Each of these Southern dynasties were led by Han Chinese ruling families and used Jiankang (modern Nanjing) as the capital. They held off attacks from the north and preserved many aspects of Chinese civilization, while northern barbarian regimes began to sinify.
In the north, the last of the Sixteen Kingdoms was extinguished in 439 by the Northern Wei, a kingdom founded by the Xianbei, a nomadic people who unified northern China. The Northern Wei eventually split into the Eastern and Western Wei, which then became the Northern Qi and Northern Zhou. These regimes were dominated by Xianbei or Han Chinese who had married into Xianbei families. During this period most Xianbei people adopted Han surnames, eventually leading to complete assimilation into the Han.
Despite the division of the country, Buddhism spread throughout the land. In southern China, fierce debates about whether Buddhism should be allowed were held frequently by the royal court and nobles. By the end of the era, Buddhists and Taoists had become much more tolerant of each other.
The short-lived Sui dynasty was a pivotal period in Chinese history. Founded by Emperor Wen in 581 in succession of the Northern Zhou, the Sui went on to conquer the Southern Chen in 589 to reunify China, ending three centuries of political division. The Sui pioneered many new institutions, including the government system of Three Departments and Six Ministries, imperial examinations for selecting officials from commoners, while improved on the systems of fubing system of the army conscription and the Equal-field system of land distributions. These policies, which were adopted by later dynasties, brought enormous population growth, and amassed excessive wealth to the state. Standardized coinage were enforced throughout the unified empire. Buddhism took root as a prominent religion and was supported officially. Sui China was known for its numerous mega-construction projects. Intended for grains shipment and transporting troops, the Grand Canal was constructed, linking the capitals Daxing (Chang'an) and Luoyang to the wealthy southeast region, and in another route, to the northeast border. The Great Wall was also expanded, while series of military conquests and diplomatic maneuvers further pacified its borders. However, the massive invasions of the Korean Peninsula during the Goguryeo–Sui War failed disastrously, triggering widespread revolts that led to the fall of the dynasty.
The Tang dynasty was a golden age of Chinese civilization, a prosperous, stable, and creative period with significant developments in culture, art, literature, particularly poetry, and technology. Buddhism became the predominant religion for the common people. Chang'an (modern Xi'an), the national capital, was the largest city in the world during its time.[45]
The first emperor, Emperor Gaozu, came to the throne on 18 June 618, placed there by his son, Li Shimin, who became the second emperor, Taizong, one of the greatest emperors in Chinese history. Combined military conquests and diplomatic maneuvers reduced threats from Central Asian tribes, extended the border, and brought neighboring states into a tributary system. Military victories in the Tarim Basin kept the Silk Road open, connecting Chang'an to Central Asia and areas far to the west. In the south, lucrative maritime trade routes from port cities such as Guangzhou connected with distant countries, and foreign merchants settled in China, encouraging a cosmopolitan culture. The Tang culture and social systems were observed and adapted by neighboring countries, most notably Japan. Internally the Grand Canal linked the political heartland in Chang'an to the agricultural and economic centers in the eastern and southern parts of the empire. Xuanzang, a Chinese Buddhist monk, scholar, traveller, and translator who travelled to India on his own, and returned with, "over six hundred Mahayana and Hinayana texts, seven statues of the Buddha and more than a hundred sarira relics."
The prosperity of the early Tang dynasty was abetted by a centralized bureaucracy. The government was organized as "Three Departments and Six Ministries" to separately draft, review, and implement policies. These departments were run by royal family members and landed aristocrats, but as the dynasty wore on, were joined or replaced by scholar officials selected by imperial examinations, setting patterns for later dynasties.
Under the Tang "equal-field system" all land was owned by the Emperor and granted to each family according to household size. Men granted land were conscripted for military service for a fixed period each year, a military policy known as the "Fubing system". These policies stimulated a rapid growth in productivity and a significant army without much burden on the state treasury. By the dynasty's midpoint, however, standing armies had replaced conscription, and land was continuously falling into the hands of private owners and religious institutions granted exemptions.
The dynasty continued to flourish under the rule of Empress Wu Zetian, the only empress regnant in Chinese history, and reached its zenith during the long reign of Emperor Xuanzong, who oversaw an empire that stretched from the Pacific to the Aral Sea with at least 50 million people. There were vibrant artistic and cultural creations, including works of the greatest Chinese poets, Li Bai and Du Fu.
At the zenith of prosperity of the empire, the An Lushan Rebellion from 755 to 763 was a watershed event. War, disease, and economic disruption devastated the population and drastically weakened the central imperial government. Upon suppression of the rebellion, regional military governors, known as Jiedushi, gained increasingly autonomous status. With loss of revenue from land tax, the central imperial government came to rely heavily on salt monopoly. Externally, former submissive states raided the empire and the vast border territories were lost for centuries. Nevertheless, civil society recovered and thrived amidst the weakened imperial bureaucracy.
In late Tang period, the empire was worn out by recurring revolts of regional warlords, while internally, as scholar-officials engaged in fierce factional strife, corrupted eunuchs amassed immense power. Catastrophically, the Huang Chao Rebellion, from 874 to 884, devastated the entire empire for a decade. The sack of the southern port Guangzhou in 879 was followed by the massacre of most of its inhabitants, especially the large foreign merchant enclaves.[47][48] By 881, both capitals, Luoyang and Chang'an, fell successively. The reliance on ethnic Han and Turkic warlords in suppressing the rebellion increased their power and influence. Consequently, the fall of the dynasty following Zhu Wen's usurpation led to an era of division.
The period of political disunity between the Tang and the Song, known as the Five Dynasties and Ten Kingdoms period, lasted from 907 to 960. During this half-century, China was in all respects a multi-state system. Five regimes, namely, (Later) Liang, Tang, Jin, Han and Zhou, rapidly succeeded one another in control of the traditional Imperial heartland in northern China. Among the regimes, rulers of (Later) Tang, Jin and Han were sinicized Shatuo Turks, which ruled over the ethnic majority of Han Chinese. More stable and smaller regimes of mostly ethnic Han rulers coexisted in south and western China over the period, cumulatively constituted the "Ten Kingdoms".
Amidst political chaos in the north, the strategic Sixteen Prefectures (region along today's Great Wall) were ceded to the emerging Khitan Liao dynasty, which drastically weakened the defense of the China proper against northern nomadic empires. To the south, Vietnam gained lasting independence after being a Chinese prefecture for many centuries. With wars dominated in Northern China, there were mass southward migrations of population, which further enhanced the southward shift of cultural and economic centers in China. The era ended with the coup of Later Zhou general Zhao Kuangyin, and the establishment of the Song dynasty in 960, which eventually annihilated the remains of the "Ten Kingdoms" and reunified China.
In 960, the Song dynasty was founded by Emperor Taizu, with its capital established in Kaifeng (also known as Bianjing). In 979, the Song dynasty reunified most of the China proper, while large swaths of the outer territories were occupied by sinicized nomadic empires. The Khitan Liao dynasty, which lasted from 907 to 1125, ruled over Manchuria, Mongolia, and parts of Northern China. Meanwhile, in what are now the north-western Chinese provinces of Gansu, Shaanxi, and Ningxia, the Tangut tribes founded the Western Xia dynasty from 1032 to 1227.
Aiming to recover the strategic Sixteen Prefectures lost in the previous dynasty, campaigns were launched against the Liao dynasty in the early Song period, which all ended in failure. Then in 1004, the Liao cavalry swept over the exposed North China Plain and reached the outskirts of Kaifeng, forcing the Song's submission and then agreement to the Chanyuan Treaty, which imposed heavy annual tributes from the Song treasury. The treaty was a significant reversal of Chinese dominance of the traditional tributary system. Yet the annual outflow of Song's silver to the Liao was paid back through the purchase of Chinese goods and products, which expanded the Song economy, and replenished its treasury. This dampened the incentive for the Song to further campaign against the Liao. Meanwhile, this cross-border trade and contact induced further sinicization within the Liao Empire, at the expense of its military might which was derived from its primitive nomadic lifestyle. Similar treaties and social-economical consequences occurred in Song's relations with the Jin dynasty.
Within the Liao Empire, the Jurchen tribes revolted against their overlords to establish the Jin dynasty in 1115. In 1125, the devastating Jin cataphract annihilated the Liao dynasty, while remnants of Liao court members fled to Central Asia to found the Qara Khitai Empire (Western Liao dynasty). Jin's invasion of the Song dynasty followed swiftly. In 1127, Kaifeng was sacked, a massive catastrophe known as the Jingkang Incident, ending the Northern Song dynasty. Later the entire north of China was conquered. The survived members of Song court regrouped in the new capital city of Hangzhou, and initiated the Southern Song dynasty, which ruled territories south of the Huai River. In the ensuing years, the territory and population of China were divided between the Song dynasty, the Jin dynasty and the Western Xia dynasty. The era ended with the Mongol conquest, as Western Xia fell in 1227, the Jin dynasty in 1234, and finally the Southern Song dynasty in 1279.
Despite its military weakness, the Song dynasty is widely considered to be the high point of classical Chinese civilization. The Song economy, facilitated by technology advancement, had reached a level of sophistication probably unseen in world history before its time. The population soared to over 100 million and the living standards of common people improved tremendously due to improvements in rice cultivation and the wide availability of coal for production. The capital cities of Kaifeng and subsequently Hangzhou were both the most populous cities in the world for their time, and encouraged vibrant civil societies unmatched by previous Chinese dynasties. Although land trading routes to the far west were blocked by nomadic empires, there were extensive maritime trade with neighboring states, which facilitated the use of Song coinage as the de facto currency of exchange. Giant wooden vessels equipped with compasses traveled throughout the China Seas and northern Indian Ocean. The concept of insurance was practised by merchants to hedge the risks of such long-haul maritime shipments. With prosperous economic activities, the historically first use of paper currency emerged in the western city of Chengdu, as a supplement to the existing copper coins.
The Song dynasty was considered to be the golden age of great advancements in science and technology of China, thanks to innovative scholar-officials such as Su Song (1020–1101) and Shen Kuo (1031–1095). Inventions such as the hydro-mechanical astronomical clock, the first continuous and endless power-transmitting chain, woodblock printing and paper money were all invented during the Song dynasty.
There was court intrigue between the political reformers and conservatives, led by the chancellors Wang Anshi and Sima Guang, respectively. By the mid-to-late 13th century, the Chinese had adopted the dogma of Neo-Confucian philosophy formulated by Zhu Xi. Enormous literary works were compiled during the Song dynasty, such as the historical work, the Zizhi Tongjian ("Comprehensive Mirror to Aid in Government"). The invention of movable-type printing further facilitated the spread of knowledge. Culture and the arts flourished, with grandiose artworks such as Along the River During the Qingming Festival and Eighteen Songs of a Nomad Flute, along with great Buddhist painters such as the prolific Lin Tinggui.
The Song dynasty was also a period of major innovation in the history of warfare. Gunpowder, while invented in the Tang dynasty, was first put into use in battlefields by the Song army, inspiring a succession of new firearms and siege engines designs. During the Southern Song dynasty, as its survival hinged decisively on guarding the Yangtze and Huai River against the cavalry forces from the north, the first standing navy in China was assembled in 1132, with its admiral's headquarters established at Dinghai. Paddle-wheel warships equipped with trebuchets could launch incendiary bombs made of gunpowder and lime, as recorded in Song's victory over the invading Jin forces at the Battle of Tangdao in the East China Sea, and the Battle of Caishi on the Yangtze River in 1161.
The advances in civilization during the Song dynasty came to an abrupt end following the devastating Mongol conquest, during which the population sharply dwindled, with a marked contraction in economy. Despite viciously halting Mongol advance for more than three decades, the Southern Song capital Hangzhou fell in 1276, followed by the final annihilation of the Song standing navy at the Battle of Yamen in 1279.
The Yuan dynasty was formally proclaimed in 1271, when the Great Khan of Mongol, Kublai Khan, one of the grandsons of Genghis Khan, assumed the additional title of Emperor of China, and considered his inherited part of the Mongol Empire as a Chinese dynasty. In the preceding decades, the Mongols had conquered the Jin dynasty in Northern China, and the Southern Song dynasty fell in 1279 after a protracted and bloody war. The Mongol Yuan dynasty became the first conquest dynasty in Chinese history to rule the entire China proper and its population as an ethnic minority. The dynasty also directly controlled the Mongolian heartland and other regions, inheriting the largest share of territory of the divided Mongol Empire, which roughly coincided with the modern area of China and nearby regions in East Asia. Further expansion of the empire was halted after defeats in the invasions of Japan and Vietnam. Following the previous Jin dynasty, the capital of Yuan dynasty was established at Khanbaliq (also known as Dadu, modern-day Beijing). The Grand Canal was reconstructed to connect the remote capital city to economic hubs in southern part of China, setting the precedence and foundation where Beijing would largely remain as the capital of the successive regimes that unified China mainland.
After the peace treaty in 1304 that ended a series of Mongol civil wars, the emperors of the Yuan dynasty were upheld as the nominal Great Khan (Khagan) of the greater Mongol Empire over other Mongol Khanates, which nonetheless remained de facto autonomous. The era was known as Pax Mongolica, when much of the Asian continent was ruled by the Mongols. For the first and only time in history, the silk road was controlled entirely by a single state, facilitating the flow of people, trade, and cultural exchange. Network of roads and a postal system were established to connect the vast empire. Lucrative maritime trade, developed from the previous Song dynasty, continued to flourish, with Quanzhou and Hangzhou emerging as the largest ports in the world. Adventurous travelers from the far west, most notably the Venetian, Marco Polo, would have settled in China for decades. Upon his return, his detail travel record inspired generations of medieval Europeans with the splendors of the far East. The Yuan dynasty was the first ancient economy, where paper currency, known at the time as Jiaochao, was used as the predominant medium of exchange. Its unrestricted issuance in the late Yuan dynasty inflicted hyperinflation, which eventually brought the downfall of the dynasty.
While the Mongol rulers of the Yuan dynasty adopted substantially to Chinese culture, their sinicization was of lesser extent compared to earlier conquest dynasties in Chinese history. For preserving racial superiority as the conqueror and ruling class, traditional nomadic customs and heritage from the Mongolian steppe were held in high regard. On the other hand, the Mongol rulers also adopted flexibly to a variety of cultures from many advanced civilizations within the vast empire. Traditional social structure and culture in China underwent immense transform during the Mongol dominance. Large group of foreign migrants settled in China, who enjoyed elevated social status over the majority Han Chinese, while enriching Chinese culture with foreign elements. The class of scholar officials and intellectuals, traditional bearers of elite Chinese culture, lost substantial social status. This stimulated the development of culture of the common folks. There were prolific works in zaju variety shows and literary songs (sanqu), which were written in a distinctive poetry style known as qu. Novels of vernacular style gained unprecedented status and popularity.
Before the Mongol invasion, Chinese dynasties reported approximately 120 million inhabitants; after the conquest had been completed in 1279, the 1300 census reported roughly 60 million people.[49] This major decline is not necessarily due only to Mongol killings. Scholars such as Frederick W. Mote argue that the wide drop in numbers reflects an administrative failure to record rather than an actual decrease; others such as Timothy Brook argue that the Mongols created a system of enserfment among a huge portion of the Chinese populace, causing many to disappear from the census altogether; other historians including William McNeill and David Morgan consider that plague was the main factor behind the demographic decline during this period. In the 14th century China suffered additional depredations from epidemics of plague, estimated to have killed 25 million people, 30% of the population of China.[50]
Throughout the Yuan dynasty, there was some general sentiment among the populace against the Mongol dominance. Yet rather than the nationalist cause, it was mainly strings of natural disasters and incompetent governance that triggered widespread peasant uprisings since the 1340s. After the massive naval engagement at Lake Poyang, Zhu Yuanzhang prevailed over other rebel forces in the south. He proclaimed himself emperor and founded the Ming dynasty in 1368. The same year his northern expedition army captured the capital Khanbaliq. The Yuan remnants fled back to Mongolia and sustained the regime. Other Mongol Khanates in Central Asia continued to exist after the fall of Yuan dynasty in China.
The Ming dynasty was founded by Zhu Yuanzhang in 1368, who proclaimed himself as the Hongwu Emperor. The capital was initially set at Nanjing, and was later moved to Beijing from Yongle Emperor's reign onward.
Urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil.
Despite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with the voyages of Zheng He.
The Hongwu Emperor, being the only founder of a Chinese dynasty who was also of peasant origin, had laid the foundation of a state that relied fundamentally in agriculture. Commerce and trade, which flourished in the previous Song and Yuan dynasties, were less emphasized. Neo-feudal landholdings of the Song and Mongol periods were expropriated by the Ming rulers. Land estates were confiscated by the government, fragmented, and rented out. Private slavery was forbidden. Consequently, after the death of the Yongle Emperor, independent peasant landholders predominated in Chinese agriculture. These laws might have paved the way to removing the worst of the poverty during the previous regimes. Towards later era of the Ming dynasty, with declining government control, commerce, trade and private industries revived.
The dynasty had a strong and complex central government that unified and controlled the empire. The emperor's role became more autocratic, although Hongwu Emperor necessarily continued to use what he called the "Grand Secretariat" to assist with the immense paperwork of the bureaucracy, including memorials (petitions and recommendations to the throne), imperial edicts in reply, reports of various kinds, and tax records. It was this same bureaucracy that later prevented the Ming government from being able to adapt to changes in society, and eventually led to its decline.
The Yongle Emperor strenuously tried to extend China's influence beyond its borders by demanding other rulers send ambassadors to China to present tribute. A large navy was built, including four-masted ships displacing 1,500 tons. A standing army of 1 million troops was created. The Chinese armies conquered and occupied Vietnam for around 20 years, while the Chinese fleet sailed the China seas and the Indian Ocean, cruising as far as the east coast of Africa. The Chinese gained influence in eastern Moghulistan. Several maritime Asian nations sent envoys with tribute for the Chinese emperor. Domestically, the Grand Canal was expanded and became a stimulus to domestic trade. Over 100,000 tons of iron per year were produced. Many books were printed using movable type. The imperial palace in Beijing's Forbidden City reached its current splendor. It was also during these centuries that the potential of south China came to be fully exploited. New crops were widely cultivated and industries such as those producing porcelain and textiles flourished.
In 1449 Esen Tayisi led an Oirat Mongol invasion of northern China which culminated in the capture of the Zhengtong Emperor at Tumu. Since then, the Ming became on the defensive on the northern frontier, which led to the Ming Great Wall being built. Most of what remains of the Great Wall of China today was either built or repaired by the Ming. The brick and granite work was enlarged, the watchtowers were redesigned, and cannons were placed along its length.
At sea, the Ming became increasingly isolationist after the death of the Yongle Emperor. The treasure voyages which sailed Indian Ocean were discontinued, and the maritime prohibition laws were set in place banning the Chinese from sailing abroad. European traders who reached China in the midst of the Age of Discovery were repeatedly rebuked in their requests for trade, with the Portuguese being repulsed by the Ming navy at Tuen Mun in 1521 and again in 1522. Domestic and foreign demands for overseas trade, deemed illegal by the state, led to widespread wokou piracy attacking the southeastern coastline during the rule of the Jiajing Emperor (1507–1567), which only subsided after the opening of ports in Guangdong and Fujian and much military suppression.[51] The Portuguese were allowed to settle in Macau in 1557 for trade, which remained in Portuguese hands until 1999. The Dutch entry into the Chinese seas was also met with fierce resistance, with the Dutch being chased off the Penghu islands in the Sino-Dutch conflicts of 1622–1624 and were forced to settle in Taiwan instead. The Dutch in Taiwan fought with the Ming in the Battle of Liaoluo Bay in 1633 and lost, and eventually surrendered to the Ming loyalist Koxinga in 1662, after the fall of the Ming dynasty.
In 1556, during the rule of the Jiajing Emperor, the Shaanxi earthquake killed about 830,000 people, the deadliest earthquake of all time.
The Ming dynasty intervened deeply in the Japanese invasions of Korea (1592–98), which ended with the withdrawal of all invading Japanese forces in Korea, and the restoration of the Joseon dynasty, its traditional ally and tributary state. The regional hegemony of the Ming dynasty was preserved at a toll on its resources. Coincidentally, with Ming's control in Manchuria in decline, the Manchu (Jurchen) tribes, under their chieftain Nurhaci, broke away from Ming's rule, and emerged as a powerful, unified state, which was later proclaimed as the Qing dynasty. It went on to subdue the much weakened Korea as its tributary, conquered Mongolia, and expanded its territory to the outskirt of the Great Wall. The most elite army of the Ming dynasty was to station at the Shanhai Pass to guard the last stronghold against the Manchus, which weakened its suppression of internal peasants uprisings.
The Qing dynasty (1644–1912) was the last imperial dynasty in China. Founded by the Manchus, it was the second conquest dynasty to rule the entirety of China proper, and roughly doubled the territory controlled by the Ming. The Manchus were formerly known as Jurchens, residing in the northeastern part of the Ming territory outside the Great Wall. They emerged as the major threat to the late Ming dynasty after Nurhaci united all Jurchen tribes and his son, Hong Taiji, declared the founding of the Qing dynasty in 1636. The Qing dynasty set up the Eight Banners system that provided the basic framework for the Qing military conquest. Li Zicheng's peasant rebellion captured Beijing in 1644 and the Chongzhen Emperor, the last Ming emperor, committed suicide. The Manchus allied with the Ming general Wu Sangui to seize Beijing, which was made the capital of the Qing dynasty, and then proceeded to subdue the Ming remnants in the south. The decades of Manchu conquest caused enormous loss of lives and the economic scale of China shrank drastically. In total, the Qing conquest of the Ming (1618–1683) cost as many as 25 million lives.[52] The early Manchu emperors combined traditions of Central Asian rule with Confucian norms of traditional Chinese government and were considered a Chinese dynasty.
The Manchus enforced a 'queue order', forcing Han Chinese men to adopt the Manchu queue hairstyle. Officials were required to wear Manchu-style clothing Changshan (bannermen dress and Tangzhuang), but ordinary Han civilians were allowed to wear traditional Han clothing. Bannermen could not undertake trade or manual labor; they had to petition to be removed from banner status. They were considered a form of nobility and were given annual pensions, land, and allotments of cloth. The Kangxi Emperor ordered the creation of the Kangxi Dictionary, the most complete dictionary of Chinese characters that had been compiled.
Over the next half-century, all areas previously under the Ming dynasty were consolidated under the Qing. Conquests in Central Asia in the eighteenth century extended territorial control. Between 1673 and 1681, the Kangxi Emperor suppressed the Revolt of the Three Feudatories, an uprising of three generals in Southern China who had been denied hereditary rule of large fiefdoms granted by the previous emperor. In 1683, the Qing staged an amphibious assault on southern Taiwan, bringing down the rebel Kingdom of Tungning, which was founded by the Ming loyalist Koxinga (Zheng Chenggong) in 1662 after the fall of the Southern Ming, and had served as a base for continued Ming resistance in Southern China. The Qing defeated the Russians at Albazin, resulting in the Treaty of Nerchinsk.
By the end of Qianlong Emperor's long reign in 1796, the Qing Empire was at its zenith. The Qing ruled more than one-third of the world's population, and had the largest economy in the world. By area it was one of the largest empires ever.
In the 19th century the empire was internally restive and externally threatened by western powers. The defeat by the British Empire in the First Opium War (1840) led to the Treaty of Nanking (1842), under which Hong Kong was ceded to Britain and importation of opium (produced by British Empire territories) was allowed. Opium usage continued to grow in China, adversely affecting societal stability. Subsequent military defeats and unequal treaties with other western powers continued even after the fall of the Qing dynasty.
Internally the Taiping Rebellion (1851–1864), a Christian religious movement led by the "Heavenly King" Hong Xiuquan swept from the south to establish the Taiping Heavenly Kingdom and controlled roughly a third of China proper for over a decade. The court in desperation empowered Han Chinese officials such as Zeng Guofan to raise local armies. After initial defeats, Zeng crushed the rebels in the Third Battle of Nanking in 1864.[53] This was one of the largest wars in the 19th century in terms of troop involvement; there was massive loss of life, with a death toll of about 20 million.[54] A string of civil disturbances followed, including the Punti–Hakka Clan Wars, Nian Rebellion, Dungan Revolt, and Panthay Rebellion.[55] All rebellions were ultimately put down, but at enormous cost and with millions dead, seriously weakening the central imperial authority. China never rebuilt a strong central army, and many local officials used their military power to effectively rule independently in their provinces.[56]
Yet the dynasty appeared to recover in the Tongzhi Restoration (1860–1872), led by Manchu royal family reformers and Han Chinese officials such as Zeng Guofan and his proteges Li Hongzhang and Zuo Zongtang. Their Self-Strengthening Movement made effective institutional reforms, imported Western factories and communications technology, with prime emphasis on strengthening the military. However, the reform was undermined by official rivalries, cynicism, and quarrels within the imperial family. The defeat of Yuan Shikai's modernized "Beiyang Fleet" in the First Sino-Japanese War (1894–1895) led to the formation of the New Army. The Guangxu Emperor, advised by Kang Youwei, then launched a comprehensive reform effort, the Hundred Days' Reform (1898). Empress Dowager Cixi, however, feared that precipitous change would lead to bureaucratic opposition and foreign intervention and quickly suppressed it.
In the summer of 1900, the Boxer Uprising opposed foreign influence and murdered Chinese Christians and foreign missionaries. When Boxers entered Beijing, the Qing government ordered all foreigners to leave, but they and many Chinese Christians were besieged in the foreign legations quarter. An Eight-Nation Alliance sent the Seymour Expedition of Japanese, Russian, British, Italian, German, French, American, and Austrian troops to relieve the siege, but they were forced to retreat by Boxer and Qing troops at the Battle of Langfang. After the Alliance's attack on the Dagu Forts, the court declared war on the Alliance and authorized the Boxers to join with imperial armies. After fierce fighting at Tientsin, the Alliance formed the second, much larger Gaselee Expedition and finally reached Beijing; the Empress Dowager evacuated to Xi'an. The Boxer Protocol ended the war, exacting a tremendous indemnity.
The Qing court then instituted "New Policies" of administrative and legal reform, including abolition of the examination system. But young officials, military officers, and students debated reform, perhaps a constitutional monarchy, or the overthrow of the dynasty and the creation of a republic. They were inspired by an emerging public opinion formed by intellectuals such as Liang Qichao and the revolutionary ideas of Sun Yat-sen. A localised military uprising, the Wuchang Uprising, began on 10 October 1911, in Wuchang (Today part of Wuhan), and soon spread. The Republic of China was proclaimed on 1 January 1912, ending 2,000 years of dynastic rule.
The provisional government of the Republic of China was formed in Nanking on 12 March 1912. Sun Yat-sen became President of the Republic of China, but he turned power over to Yuan Shikai, who commanded the New Army. Over the next few years, Yuan proceeded to abolish the national and provincial assemblies and declared himself as the emperor of Empire of China in late 1915. Yuan's imperial ambitions were fiercely opposed by his subordinates; faced with the prospect of rebellion, he abdicated in March 1916 and died of natural causes in June.
Yuan's death in 1916 left a power vacuum; the republican government was all but shattered. This opened the way for the Warlord Era, during which much of China was ruled by shifting coalitions of competing provincial military leaders and the Beiyang government. Intellectuals, disappointed in the failure of the Republic, launched the New Culture Movement.
In 1919, the May Fourth Movement began as a response to the pro-Japanese terms imposed on China by the Treaty of Versailles following World War I. It quickly became a nationwide protest movement. The protests were a moral success as the cabinet fell and China refused to sign the Treaty of Versailles, which had awarded German holdings of Shandong to Japan. Memory of the mistreatment at Versailles fuels resentment into the 21st century.[57]
Political and intellectual ferment waxed strong throughout the 1920s and 1930s. According to Patricia Ebrey:
In the 1920s, Sun Yat-sen established a revolutionary base in Guangzhou and set out to unite the fragmented nation. He welcomed assistance from the Soviet Union (itself fresh from Lenin's Communist takeover) and he entered into an alliance with the fledgling Chinese Communist Party (CCP). After Sun's death from cancer in 1925, one of his protégés, Chiang Kai-shek, seized control of the Nationalist Party (KMT) and succeeded in bringing most of south and central China under its rule in the Northern Expedition (1926–1927). Having defeated the warlords in the south and central China by military force, Chiang was able to secure the nominal allegiance of the warlords in the North and establish the Nationalist government in Nanking. In 1927, Chiang turned on the CCP and relentlessly purged the Communists elements in his NRA. In 1934, driven from their mountain bases such as the Chinese Soviet Republic, the CCP forces embarked on the Long March across China's most desolate terrain to the northwest, where they established a guerrilla base at Yan'an in Shaanxi Province. During the Long March, the communists reorganized under a new leader, Mao Zedong (Mao Tse-tung).
The bitter Chinese Civil War between the Nationalists and the Communists continued, openly or clandestinely, through the 14-year-long Japanese occupation of various parts of the country (1931–1945). The two Chinese parties nominally formed a United Front to oppose the Japanese in 1937, during the Second Sino-Japanese War (1937–1945), which became a part of World War II. Japanese forces committed numerous war atrocities against the civilian population, including biological warfare (see Unit 731) and the Three Alls Policy (Sankō Sakusen), the three alls being: "Kill All, Burn All and Loot All".[59]
Following the defeat of Japan in 1945, the war between the Nationalist government forces and the CCP resumed, after failed attempts at reconciliation and a negotiated settlement. By 1949, the CCP had established control over most of the country. Odd Arne Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against the Japanese. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism.[60] During the civil war both the Nationalists and Communists carried out mass atrocities, with millions of non-combatants killed by both sides.[61] These included deaths from forced conscription and massacres.[62] When the Nationalist government forces were defeated by CCP forces in mainland China in 1949, the Nationalist government retreated to Taiwan with its forces, along with Chiang and a large number of their supporters; the Nationalist government had taken effective control of Taiwan at the end of WWII as part of the overall Japanese surrender, when Japanese troops in Taiwan surrendered to the Republic of China troops.[63]
Until the early 1970s, the ROC was recognized as the sole legitimate government of China by the United Nations, the United States and most Western nations, refusing to recognize the PRC on account of the Cold War. This changed in 1971 when the PRC was seated in the United Nations, replacing the ROC. The KMT ruled Taiwan under martial law until 1987, with the stated goal of being vigilant against Communist infiltration and preparing to retake mainland China. Therefore, political dissent was not tolerated during that period.
In the 1990s, the ROC underwent a major democratic reform, beginning with the 1991 resignation of the members of the Legislative Yuan and National Assembly elected in 1947. These groups were originally created to represent mainland China constituencies. Also lifted were the restrictions on the use of Taiwanese languages in the broadcast media and in schools. This culminated with the first direct presidential election in 1996 against the Democratic Progressive Party (DPP) candidate and former dissident, Peng Min-ming. In 2000, the KMT status as the ruling party ended when the DPP took power, only to regain its status in the 2008 election by Ma Ying-jeou.
Due to the controversial nature of Taiwan's political status, the ROC is currently recognized by 14 UN member states and Holy See as of 2022 as the legitimate government of "China".
Major combat in the Chinese Civil War ended in 1949 with Kuomintang (KMT) pulling out of the mainland, with the government relocating to Taipei and maintaining control only over a few islands. The CCP was left in control of mainland China. On 1 October 1949, Mao Zedong proclaimed the People's Republic of China.[64] "Communist China" and "Red China" were two common names for the PRC.[65]
The PRC was shaped by a series of campaigns and five-year plans. The economic and social plan known as the Great Leap Forward caused an estimated 45 million deaths.[66] Mao's government carried out mass executions of landowners, instituted collectivisation and implemented the Laogai camp system. Execution, deaths from forced labor and other atrocities resulted in millions of deaths under Mao. In 1966 Mao and his allies launched the Cultural Revolution, which continued until Mao's death a decade later. The Cultural Revolution, motivated by power struggles within the Party and a fear of the Soviet Union, led to a major upheaval in Chinese society.
In 1972, at the peak of the Sino-Soviet split, Mao and Zhou Enlai met US president Richard Nixon in Beijing to establish relations with the United States. In the same year, the PRC was admitted to the United Nations in place of the Republic of China, with permanent membership of the Security Council.
A power struggle followed Mao's death in 1976. The Gang of Four were arrested and blamed for the excesses of the Cultural Revolution, marking the end of a turbulent political era in China. Deng Xiaoping outmaneuvered Mao's anointed successor chairman Hua Guofeng, and gradually emerged as the de facto leader over the next few years.
Deng Xiaoping was the Paramount Leader of China from 1978 to 1992, although he never became the head of the party or state, and his influence within the Party led the country to significant economic reforms. The CCP subsequently loosened governmental control over citizens' personal lives and the communes were disbanded with many peasants receiving multiple land leases, which greatly increased incentives and agricultural production. In addition, there were many free market areas opened. The most successful free market areas was Shenzhen. It is located in Guangdong and the property tax free area still exists today. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment, a system termed by some[67] as "market socialism", and officially by the CCP as "Socialism with Chinese characteristics". The PRC adopted its current constitution on 4 December 1982.
In 1989 the death of former general secretary Hu Yaobang helped to spark the Tiananmen Square protests of that year, during which students and others campaigned for several months, speaking out against corruption and in favour of greater political reform, including democratic rights and freedom of speech. However, they were eventually put down on 4 June when Army troops and vehicles entered and forcibly cleared the square, with considerable numbers of fatalities. This event was widely reported, and brought worldwide condemnation and sanctions against the government.[68][69] A filmed incident involving the "Tank Man" was seen worldwide.
CCP general secretary and PRC President Jiang Zemin and PRC Premier Zhu Rongji, both former mayors of Shanghai, led post-Tiananmen PRC in the 1990s. Under Jiang and Zhu's ten years of administration, the PRC's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%.[70][better source needed] The country formally joined the World Trade Organization in 2001. By 1997 and 1999, former European colonies of British Hong Kong and Portuguese Macau became the Hong Kong and Macau special administrative regions of the People's Republic of China respectively.
Although the PRC needed economic growth to spur its development, the government began to worry that rapid economic growth was degrading the country's resources and environment. Another concern is that certain sectors of society are not sufficiently benefiting from the PRC's economic development; one example of this is the wide gap between urban and rural areas. As a result, under former CCP general secretary and President Hu Jintao and Premier Wen Jiabao, the PRC initiated policies to address issues of equitable distribution of resources, but the outcome was not known as of 2014[update].[71] More than 40 million farmers were displaced from their land,[72] usually for economic development, contributing to 87,000 demonstrations and riots across China in 2005.[73] For much of the PRC's population, living standards improved very substantially and freedom increased, but political controls remained tight and rural areas poor.[74]
According to the United States Department of Defense as many as 3 million Uyghurs and members of other Muslim minority groups are being held in China's internment camps which are located in the Xinjiang region and which American news reports often label as concentration camps.[75][76] The camps were established in late 2010s under Xi Jinping's administration.[77][78]
Human Rights Watch says that they have been used to indoctrinate Uyghurs and other Muslims since 2017 as part of a "people's war on terror", a policy announced in 2014.[79][80][81] The camps have been criticized by the governments of many countries and human rights organizations for alleged human rights abuses, including mistreatment, rape, and torture, with some of them alleging genocide.[82]
The outbreak of the novel coronavirus SARS coronavirus 2, which causes COVID-19 disease, originally detected in Wuhan became the global COVID-19 pandemic. The virus' origin in China has led to misinformation, including conspiracy theories that suggest the virus originated in a Chinese laboratory and was genetically engineered.[83][84][85] The World Health Organization concluded that artificial origin of the coronavirus was "extremely unlikely".[86][87]
The People's Liberation Army enters Beijing in the Pingjin Campaign
Chairman Mao Zedong proclaiming the establishment of the People's Republic of China in 1949.
People's Republic of China 10th Anniversary Parade in Beijing
The flag of the People's Republic of China since 1949.

The first human inhabitants of the Japanese archipelago have been traced to prehistoric times around 30,000 BCE. The Jōmon period, named after its cord-marked pottery, was followed by the Yayoi period in the first millennium BCE when new inventions were introduced from Asia. During this period, the first known written reference to Japan was recorded in the Chinese Book of Han in the first century CE.
Around the 3rd century BCE, the Yayoi people from the continent immigrated to the Japanese archipelago and introduced iron technology and agricultural civilization.[1]
Because they had an agricultural civilization, the population of the Yayoi began to grow rapidly and ultimately overwhelmed the Jōmon people, natives of the Japanese archipelago who were hunter-gatherers.[2]
Between the fourth to ninth century, Japan's many kingdoms and tribes gradually came to be unified under a centralized government, nominally controlled by the Emperor of Japan. The imperial dynasty established at this time continues to this day, albeit in an almost entirely ceremonial role. In 794, a new imperial capital was established at Heian-kyō (modern Kyoto), marking the beginning of the Heian period, which lasted until 1185. The Heian period is considered a golden age of classical Japanese culture. Japanese religious life from this time and onwards was a mix of native Shinto practices and Buddhism.
Over the following centuries, the power of the imperial house decreased, passing first to great clans of civilian aristocrats – most notably the Fujiwara – and then to the military clans and their armies of samurai. The Minamoto clan under Minamoto no Yoritomo emerged victorious from the Genpei War of 1180–85, defeating their rival military clan, the Taira. After seizing power, Yoritomo set up his capital in Kamakura and took the title of shōgun. In 1274 and 1281, the Kamakura shogunate withstood two Mongol invasions, but in 1333 it was toppled by a rival claimant to the shogunate, ushering in the Muromachi period. During this period, regional warlords called daimyō grew in power at the expense of the shōgun. Eventually, Japan descended into a period of civil war. Over the course of the late 16th century, Japan was reunified under the leadership of the prominent daimyō Oda Nobunaga and his successor, Toyotomi Hideyoshi. After Toyotomi's death in 1598, Tokugawa Ieyasu came to power and was appointed shōgun by the emperor. The Tokugawa shogunate, which governed from Edo (modern Tokyo), presided over a prosperous and peaceful era known as the Edo period (1600–1868). The Tokugawa shogunate imposed a strict class system on Japanese society and cut off almost all contact with the outside world.
Portugal and Japan came into contact in 1543, when the Portuguese became the first Europeans to reach Japan by landing in the southern archipelago. They had a significant impact on Japan, even in this initial limited interaction, introducing firearms to Japanese warfare. The American Perry Expedition in 1853–54 more completely ended Japan's seclusion; this contributed to the fall of the shogunate and the return of power to the emperor during the Boshin War in 1868. The new national leadership of the following Meiji period transformed the isolated feudal island country into an empire that closely followed Western models and became a great power. Although democracy developed and modern civilian culture prospered during the Taishō period (1912–26), Japan's powerful military had great autonomy and overruled Japan's civilian leaders in the 1920s and 1930s. The Japanese military invaded Manchuria in 1931, and from 1937 the conflict escalated into a prolonged war with China. Japan's attack on Pearl Harbor in 1941 led to war with the United States and its allies. Japan's forces soon became overextended, but the military held out in spite of Allied air attacks that inflicted severe damage on population centers. Emperor Hirohito announced Japan's surrender on August 15, 1945, following the atomic bombings of Hiroshima and Nagasaki and the Soviet invasion of Manchuria.
The Allies occupied Japan until 1952, during which a new constitution was enacted in 1947 that transformed Japan into a constitutional monarchy. After 1955, Japan enjoyed very high economic growth under the governance of the Liberal Democratic Party, and became a world economic powerhouse. Since the Lost Decade of the 1990s, economic growth has slowed. On March 11, 2011, Japan suffered from a magnitude 9.0 earthquake and tsunami, one of the most powerful earthquakes ever recorded, which killed almost 20,000 people and caused the Fukushima Daiichi nuclear disaster.
Hunter-gatherers arrived in Japan in Paleolithic times, though little evidence of their presence remains, as Japan's acidic soils are inhospitable to the process of fossilization. However, the discovery of unique edge-ground axes in Japan dated to over 30,000 years ago may be evidence of the first Homo sapiens in Japan.[3] Early humans likely arrived on Japan by sea on watercraft.[4] Evidence of human habitation has been dated to 32,000 years ago in Okinawa's Yamashita Cave[5] and up to 20,000 years ago on Ishigaki Island's Shiraho Saonetabaru Cave.[6]
The Jōmon period of prehistoric Japan spans from roughly 13,000 BC[7] to about 1,000 BC.[8] Japan was inhabited by a predominantly hunter-gatherer culture that reached a considerable degree of sedentism and cultural complexity.[9] The name Jōmon, meaning "cord-marked", was first applied by American scholar Edward S. Morse, who discovered shards of pottery in 1877.[10] The pottery style characteristic of the first phases of Jōmon culture was decorated by impressing cords into the surface of wet clay.[11] Jōmon pottery is generally accepted to be among the oldest in East Asia and the world.[12]
A vase from the early Jōmon period (11000–7000 BC)
Middle Jōmon vase (2000 BC)
Dogū figurine of the late Jōmon period (1000–400 BC)
The advent of the Yayoi people from the Asian continent brought fundamental transformations to the Japanese archipelago, compressing the millennial achievements of the Neolithic Revolution into a relatively short span of centuries, particularly with the development of rice cultivation[13] and metallurgy. The onset of this wave of changes was, until recently, thought to have begun around 400 BCE.[14] Radio-carbon evidence now suggests the new phase started some 500 years earlier, between 1,000 and 800 BCE.[15][16] Radiating out from northern Kyūshū, the Yayoi, endowed with bronze and iron weapons and tools initially imported from China and the Korean peninsula, gradually supplanted the Jōmon.[17] They also introduced weaving and silk production,[18] new woodworking methods,[15] glassmaking technology,[15] and new architectural styles.[19] The expansion of the Yayoi appears to have brought about a fusion with the indigenous Jōmon, resulting in a small admixture genetically.[20]
The Yayoi technologies originated on the Asian mainland. There is debate among scholars as to what extent their spread was accomplished by means of migration or simply a diffusion of ideas, or a combination of both. The migration theory is supported by genetic and linguistic studies.[15] Historian Hanihara Kazurō has suggested that the annual immigrant influx from the continent range from 350 to 3,000.[21]
The population of Japan began to increase rapidly, perhaps with a 10-fold rise over the Jōmon. Calculations of the population size have varied from 1 to 4 million by the end of the Yayoi.[22] Skeletal remains from the late Jōmon period reveal a deterioration in already poor standards of health and nutrition, in contrast to Yayoi archaeological sites where there are large structures suggestive of grain storehouses. This change was accompanied by an increase in both the stratification of society and tribal warfare, indicated by segregated gravesites and military fortifications.[15]
During the Yayoi period, the Yayoi tribes gradually coalesced into a number of kingdoms. The earliest written work of history to mention Japan, the Book of Han completed around 82 AD, states that Japan, referred to as Wa, was divided into one hundred kingdoms. A later Chinese work of history, the Wei Zhi, states that by 240 AD, one powerful kingdom had gained ascendancy over the others. According to the Wei Zhi, this kingdom was called Yamatai, though modern historians continue to debate its location and other aspects of its depiction in the Wei Zhi. Yamatai was said to have been ruled by the female monarch Himiko.[23]
During the subsequent Kofun period, most of Japan gradually unified under a single kingdom. The symbol of the growing power of Japan's new leaders was the kofun burial mounds they constructed from around 250 CE onwards.[24] Many were of massive scales, such as the Daisenryō Kofun, a 486 m-long keyhole-shaped burial mound that took huge teams of laborers fifteen years to complete. It is commonly accepted that the tomb was built for Emperor Nintoku.[25] The kofun were often surrounded by and filled with numerous haniwa clay sculptures, often in the shape of warriors and horses.[24]
The center of the unified state was Yamato in the Kinai region of central Japan.[24] The rulers of the Yamato state were a hereditary line of emperors who still reign as the world's longest dynasty. The rulers of the Yamato extended their power across Japan through military conquest, but their preferred method of expansion was to convince local leaders to accept their authority in exchange for positions of influence in the government.[26] Many of the powerful local clans who joined the Yamato state became known as the uji.[27]
These leaders sought and received formal diplomatic recognition from China, and Chinese accounts record five successive such leaders as the Five kings of Wa. Craftsmen and scholars from China and the Three Kingdoms of Korea played an important role in transmitting continental technologies and administrative skills to Japan during this period.[27]
Historians agree that there was a big struggle between the Yamato federation and the Izumo Federation centuries before written records.[28]
The Asuka period began as early as 538 CE with the introduction of the Buddhist religion from the Korean kingdom of Baekje.[29] Since then, Buddhism has coexisted with Japan's native Shinto religion, in what is today known as Shinbutsu-shūgō.[30] The period draws its name from the de facto imperial capital, Asuka, in the Kinai region.[31]
The Buddhist Soga clan took over the government in the 580s and controlled Japan from behind the scenes for nearly sixty years.[32] Prince Shōtoku, an advocate of Buddhism and of the Soga cause, who was of partial Soga descent, served as regent and de facto leader of Japan from 594 to 622. Shōtoku authored the Seventeen-article constitution, a Confucian-inspired code of conduct for officials and citizens, and attempted to introduce a merit-based civil service called the Cap and Rank System.[33] In 607, Shōtoku offered a subtle insult to China by opening his letter with the phrase, "The ruler of the land of the rising sun addresses the ruler of the land of the setting sun" as seen in the kanji characters for Japan (Nippon).[34] By 670, a variant of this expression, Nihon, established itself as the official name of the nation, which has persisted to this day.[35]
In 645, the Soga clan were overthrown in a coup launched by Prince Naka no Ōe and Fujiwara no Kamatari, the founder of the Fujiwara clan.[36] Their government devised and implemented the far-reaching Taika Reforms. The Reform began with land reform, based on Confucian ideas and philosophies from China. It nationalized all land in Japan, to be distributed equally among cultivators, and ordered the compilation of a household registry as the basis for a new system of taxation.[37] The true aim of the reforms was to bring about greater centralization and to enhance the power of the imperial court, which was also based on the governmental structure of China. Envoys and students were dispatched to China to learn about Chinese writing, politics, art, and religion. After the reforms, the Jinshin War of 672, a bloody conflict between Prince Ōama and his nephew Prince Ōtomo, two rivals to the throne, became a major catalyst for further administrative reforms.[36] These reforms culminated with the promulgation of the Taihō Code, which consolidated existing statutes and established the structure of the central government and its subordinate local governments.[38] These legal reforms created the ritsuryō state, a system of Chinese-style centralized government that remained in place for half a millennium.[36]
The art of the Asuka period embodies the themes of Buddhist art.[39] One of the most famous works is the Buddhist temple of Horyu-ji, commissioned by Prince Shōtoku and completed in 607 CE. It is now the oldest wooden structure in the world.[40]
In 710, the government constructed a grandiose new capital at Heijō-kyō (modern Nara) modeled on Chang'an, the capital of the Chinese Tang dynasty. During this period, the first two books produced in Japan appeared: the Kojiki and Nihon Shoki,[41] which contain chronicles of legendary accounts of early Japan and its creation myth, which describes the imperial line as descendants of the gods.[42] The Man'yōshū was compiled in the latter half of the eighth century, which is widely considered the finest collection of Japanese poetry.[43]
During this period, Japan suffered a series of natural disasters, including wildfires, droughts, famines, and outbreaks of disease, such as a smallpox epidemic in 735–737 that killed over a quarter of the population.[44] Emperor Shōmu (r. 724–749) feared his lack of piousness had caused the trouble and so increased the government's promotion of Buddhism, including the construction of the temple Tōdai-ji in 752.[45] The funds to build this temple were raised in part by the influential Buddhist monk Gyōki, and once completed it was used by the Chinese monk Ganjin as an ordination site.[46] Japan nevertheless entered a phase of population decline that continued well into the following Heian period.[47]
There was also a serious attempt to overthrow the Imperial house during the middle Nara period. During the 760s, monk Dōkyō tried to establish his own dynasty by the aid of Empress Shōtoku, but after her death in 770 he lost all his power and was exiled. The Fujiwara clan furthermore consolidated its power.
In 784, the capital moved briefly to Nagaoka-kyō, then again in 794 to Heian-kyō (modern Kyoto), which remained the capital until 1868.[48] Political power within the court soon passed to the Fujiwara clan, a family of court nobles who grew increasingly close to the imperial family through intermarriage.[49] Between 812 and 814 CE, a smallpox epidemic killed almost half of the Japanese population.[50]
In 858, Fujiwara no Yoshifusa had himself declared sesshō ("regent") to the underage emperor. His son Fujiwara no Mototsune created the office of kampaku, which could rule in the place of an adult reigning emperor. Fujiwara no Michinaga, an exceptional statesman who became kampaku in 996, governed during the height of the Fujiwara clan's power[51] and married four of his daughters to emperors, current and future.[49] The Fujiwara clan held on to power until 1086, when Emperor Shirakawa ceded the throne to his son Emperor Horikawa but continued to exercise political power, establishing the practice of cloistered rule,[52] by which the reigning emperor would function as a figurehead while the real authority was held by a retired predecessor behind the scenes.[51]
Throughout the Heian period, the power of the imperial court declined. The court became so self-absorbed with power struggles and with the artistic pursuits of court nobles that it neglected the administration of government outside the capital.[49] The nationalization of land undertaken as part of the ritsuryō state decayed as various noble families and religious orders succeeded in securing tax-exempt status for their private shōen manors.[51] By the eleventh century, more land in Japan was controlled by shōen owners than by the central government. The imperial court was thus deprived of the tax revenue to pay for its national army. In response, the owners of the shōen set up their own armies of samurai warriors.[53] Two powerful noble families that had descended from branches of the imperial family,[54] the Taira and Minamoto clans, acquired large armies and many shōen outside the capital. The central government began to use these two warrior clans to suppress rebellions and piracy.[55] Japan's population stabilized during the late Heian period after hundreds of years of decline.[56]
During the early Heian period, the imperial court successfully consolidated its control over the Emishi people of northern Honshu.[57] Ōtomo no Otomaro was the first man the court granted the title of seii tai-shōgun ("Great Barbarian Subduing General").[58] In 802, seii tai-shōgun Sakanoue no Tamuramaro subjugated the Emishi people, who were led by Aterui.[57] By 1051, members of the Abe clan, who occupied key posts in the regional government, were openly defying the central authority. The court requested the Minamoto clan to engage the Abe clan, whom they defeated in the Former Nine Years' War.[59] The court thus temporarily reasserted its authority in northern Japan. Following another civil war – the Later Three-Year War – Fujiwara no Kiyohira took full power; his family, the Northern Fujiwara, controlled northern Honshu for the next century from their capital Hiraizumi.[60]
In 1156, a dispute over succession to the throne erupted and the two rival claimants (Emperor Go-Shirakawa and Emperor Sutoku) hired the Taira and Minamoto clans in the hopes of securing the throne by military force. During this war, the Taira clan led by Taira no Kiyomori defeated the Minamoto clan. Kiyomori used his victory to accumulate power for himself in Kyoto and even installed his own grandson Antoku as emperor. The outcome of this war led to the rivalry between the Minamoto and Taira clans. As a result, the dispute and power struggle between both clans led to the Heiji rebellion in 1160. In 1180, Taira no Kiyomori was challenged by an uprising led by Minamoto no Yoritomo, a member of the Minamoto clan whom Kiyomori had exiled to Kamakura.[61] Though Taira no Kiyomori died in 1181, the ensuing bloody Genpei War between the Taira and Minamoto families continued for another four years. The victory of the Minamoto clan was sealed in 1185, when a force commanded by Yoritomo's younger brother, Minamoto no Yoshitsune, scored a decisive victory at the naval Battle of Dan-no-ura. Yoritomo and his retainers thus became the de facto rulers of Japan.[62]
During the Heian period, the imperial court was a vibrant center of high art and culture.[63] Its literary accomplishments include the poetry collection Kokinshū and the Tosa Diary, both associated with the poet Ki no Tsurayuki, as well as Sei Shōnagon's collection of miscellany The Pillow Book,[64] and Murasaki Shikibu's Tale of Genji, often considered the masterpiece of Japanese literature.[65]
The development of the kana written syllabaries was part of a general trend of declining Chinese influence during the Heian period. The official Japanese missions to Tang dynasty of China, which began in the year 630,[66] ended during the ninth century, though informal missions of monks and scholars continued, and thereafter the development of native Japanese forms of art and poetry accelerated.[67] A major architectural achievement, apart from Heian-kyō itself, was the temple of Byōdō-in built in 1053 in Uji.[68]
Upon the consolidation of power, Minamoto no Yoritomo chose to rule in concert with the Imperial Court in Kyoto. Though Yoritomo set up his own government in Kamakura in the Kantō region located in eastern Japan, its power was legally authorized by the Imperial court in Kyoto in several occasions. In 1192, the emperor declared Yoritomo seii tai-shōgun (征夷大将軍; Eastern Barbarian Subduing Great General), abbreviated as shōgun.[69] Yoritomo's government was called the bakufu (幕府 ("tent government")), referring to the tents where his soldiers encamped. The English term shogunate refers to the bakufu.[70] Japan remained largely under military rule until 1868.[71]
Legitimacy was conferred on the shogunate by the Imperial court, but the shogunate was the de facto rulers of the country. The court maintained bureaucratic and religious functions, and the shogunate welcomed participation by members of the aristocratic class. The older institutions remained intact in a weakened form, and Kyoto remained the official capital. This system has been contrasted with the "simple warrior rule" of the later Muromachi period.[69]
Yoritomo soon turned on Yoshitsune, who was initially harbored by Fujiwara no Hidehira, the grandson of Kiyohira and the de facto ruler of northern Honshu. In 1189, after Hidehira's death, his successor Yasuhira attempted to curry favor with Yoritomo by attacking Yoshitsune's home. Although Yoshitsune was killed, Yoritomo still invaded and conquered the Northern Fujiwara clan's territories.[72] In subsequent centuries, Yoshitsune would become a legendary figure, portrayed in countless works of literature as an idealized tragic hero.[73]
After Yoritomo's death in 1199, the office of shogun weakened. Behind the scenes, Yoritomo's wife Hōjō Masako became the true power behind the government. In 1203, her father, Hōjō Tokimasa, was appointed regent to the shogun, Yoritomo's son Minamoto no Sanetomo. Henceforth, the Minamoto shoguns became puppets of the Hōjō regents, who wielded actual power.[74]
The regime that Yoritomo had established, and which was kept in place by his successors, was decentralized and feudalistic in structure, in contrast with the earlier ritsuryō state. Yoritomo selected the provincial governors, known under the titles of shugo or jitō,[75] from among his close vassals, the gokenin. The Kamakura shogunate allowed its vassals to maintain their own armies and to administer law and order in their provinces on their own terms.[76]
In 1221, the retired Emperor Go-Toba instigated what became known as the Jōkyū War, a rebellion against the shogunate, in an attempt to restore political power to the court. The rebellion was a failure and led to Go-Toba being exiled to Oki Island, along with two other emperors, the retired Emperor Tsuchimikado and Emperor Juntoku, who were exiled to Tosa Province and Sado Island respectively.[77] The shogunate further consolidated its political power relative to the Kyoto aristocracy.[78]
The samurai armies of the whole nation were mobilized in 1274 and 1281 to confront two full-scale invasions launched by Kublai Khan of the Mongol Empire.[79] Though outnumbered by an enemy equipped with superior weaponry, the Japanese fought the Mongols to a standstill in Kyushu on both occasions until the Mongol fleet was destroyed by typhoons called kamikaze, meaning "divine wind". In spite of the Kamakura shogunate's victory, the defense so depleted its finances that it was unable to provide compensation to its vassals for their role in the victory. This had permanent negative consequences for the shogunate's relations with the samurai class.[80] Discontent among the samurai proved decisive in ending the Kamakura shogunate. In 1333, Emperor Go-Daigo launched a rebellion in the hope of restoring full power to the imperial court. The shogunate sent General Ashikaga Takauji to quell the revolt, but Takauji and his men instead joined forces with Emperor Go-Daigo and overthrew the Kamakura shogunate.[81]
Japan nevertheless entered a period of prosperity and population growth starting around 1250.[82] In rural areas, the greater use of iron tools and fertilizer, improved irrigation techniques, and double-cropping increased productivity and rural villages grew.[83] Fewer famines and epidemics allowed cities to grow and commerce to boom.[82] Buddhism, which had been largely a religion of the elites, was brought to the masses by prominent monks, such as Hōnen (1133–1212), who established Pure Land Buddhism in Japan, and Nichiren (1222–1282), who founded Nichiren Buddhism. Zen Buddhism spread widely among the samurai class.[84]
Ancient drawing depicting a samurai battling forces of the Mongol Empire
Samurai Mitsui Sukenaga (right) defeating the Mongolian invasion army (left)
Shiraishi clan
Takauji and many other samurai soon became dissatisfied with Emperor Go-Daigo's Kenmu Restoration, an ambitious attempt to monopolize power in the imperial court. Takauji rebeled after Go-Daigo refused to appoint him shōgun. In 1338, Takauji captured Kyoto and installed a rival member of the imperial family to the throne, Emperor Kōmyō, who did appoint him shogun.[85] Go-Daigo responded by fleeing to the southern city of Yoshino, where he set up a rival government. This ushered in a prolonged period of conflict between the Northern Court and the Southern Court.[86]
Takauji set up his shogunate in the Muromachi district of Kyoto. However, the shogunate was faced with the twin challenges of fighting the Southern Court and of maintaining its authority over its own subordinate governors.[86] Like the Kamakura shogunate, the Muromachi shogunate appointed its allies to rule in the provinces, but these men increasingly styled themselves as feudal lords—called daimyōs—of their domains and often refused to obey the shogun.[87] The Ashikaga shogun who was most successful at bringing the country together was Takauji's grandson Ashikaga Yoshimitsu, who came to power in 1368 and remained influential until his death in 1408. Yoshimitsu expanded the power of the shogunate and in 1392, brokered a deal to bring the Northern and Southern Courts together and end the civil war. Henceforth, the shogunate kept the emperor and his court under tight control.[86]
During the final century of the Ashikaga shogunate the country descended into another, more violent period of civil war. This started in 1467 when the Ōnin War broke out over who would succeed the ruling shogun. The daimyōs each took sides and burned Kyoto to the ground while battling for their preferred candidate. By the time the succession was settled in 1477, the shogun had lost all power over the daimyō, who now ruled hundreds of independent states throughout Japan.[88] During this Warring States period, daimyōs fought among themselves for control of the country.[89] Some of the most powerful daimyōs of the era were Uesugi Kenshin and Takeda Shingen.[90] One enduring symbol of this era was the ninja, skilled spies and assassins hired by daimyōs. Few definite historical facts are known about the secretive lifestyles of the ninja, who became the subject of many legends.[91] In addition to the daimyōs, rebellious peasants and "warrior monks" affiliated with Buddhist temples also raised their own armies.[92]
Amid this on-going anarchy, a trading ship was blown off course and landed in 1543 on the Japanese island of Tanegashima, just south of Kyushu. The three Portuguese traders on board were the first Europeans to set foot in Japan.[93] Soon European traders would introduce many new items to Japan, most importantly the musket.[94] By 1556, the daimyōs were using about 300,000 muskets in their armies.[95] The Europeans also brought Christianity, which soon came to have a substantial following in Japan reaching 350,000 believers. In 1549 the Jesuit missionary Francis Xavier disembarked in Kyushu.
Initiating direct commercial and cultural exchange between Japan and the West, the first map made of Japan in the west was represented in 1568 by the Portuguese cartographer Fernão Vaz Dourado.[96]
The Portuguese were allowed to trade and create colonies where they could convert new believers into the Christian religion. The civil war status in Japan greatly benefited the Portuguese, as well as several competing gentlemen who sought to attract Portuguese black boats and their trade to their domains. Initially, the Portuguese stayed on the lands belonging to Matsura Takanobu, Firando (Hirado),[97] and in the province of Bungo, lands of Ōtomo Sōrin, but in 1562 they moved to Yokoseura when the Daimyô there, Omura Sumitada, offered to be the first lord to convert to Christianity, adopting the name of Dom Bartolomeu. In 1564, he faced a rebellion instigated by the Buddhist clergy and Yokoseura was destroyed.
In 1561 forces under Ōtomo Sōrin attacked the castle in Moji with an alliance with the Portuguese, who provided three ships, with a crew of about 900 men and more than 50 cannons. This is thought to be the first bombardment by foreign ships on Japan.[98] The first recorded naval battle between Europeans and the Japanese occurred in 1565. In the Battle of Fukuda Bay, the daimyō Matsura Takanobu attacked two Portuguese trade vessels at Hirado port.[99] The engagement led the Portuguese traders to find a safe harbor for their ships that took them to Nagasaki.
In 1571, Dom Bartolomeu, also known as Ōmura Sumitada, guaranteed a little land in the small fishing village of "Nagasáqui" to the Jesuits, who divided it into six areas. They could use the land to receive Christians exiled from other territories, as well as for Portuguese merchants. The Jesuits built a chapel and a school under the name of São Paulo, like those in Goa and Malacca. By 1579, Nagasáqui had four hundred houses, and some Portuguese had gotten married. Fearful that Nagasaki could fall into the hands of its rival Takanobu, Omura Sumitada (Dom Bartolomeu) decided to guarantee the city directly to the Jesuits in 1580.[100] After a few years, the Jesuits came to realize that if they understood the language they would achieve more conversions to the Catholic religion. Jesuits such as João Rodrigues wrote a Japanese dictionary. Thus Portuguese became the first Western language to have such a dictionary when it was published in Nagasaki in 1603.[101]
Oda Nobunaga used European technology and firearms to conquer many other daimyōs; his consolidation of power began what was known as the Azuchi–Momoyama period (1573–1603). After Nobunaga was assassinated in 1582 by Akechi Mitsuhide, his successor Toyotomi Hideyoshi unified the nation in 1590 and launched two unsuccessful invasions of Korea in 1592 and 1597. Before the invasion, Hideyoshi tried to hire two Portuguese galleons to join the invasion but the Portuguese refused the offer.[102]
Tokugawa Ieyasu served as regent for Hideyoshi's son Toyotomi Hideyori and used his position to gain political and military support. When open war broke out, Ieyasu defeated rival clans in the Battle of Sekigahara in 1600. In 1603 the Tokugawa shogunate at Edo enacted measures including buke shohatto, as a code of conduct to control the autonomous daimyōs, and in 1639 the isolationist sakoku ("closed country") policy that spanned the two and a half centuries of tenuous political unity known as the Edo period (1603–1868), this act ended with Portuguese influence after 100 years in Japanese territory, also aiming to limit the political presence of any foreign power.[93]
In spite of the war, Japan's relative economic prosperity, which had begun in the Kamakura period, continued well into the Muromachi period. By 1450 Japan's population stood at ten million, compared to six million at the end of the thirteenth century.[82] Commerce flourished, including considerable trade with China and Korea.[103] Because the daimyōs and other groups within Japan were minting their own coins, Japan began to transition from a barter-based to a currency-based economy.[104] During the period, some of Japan's most representative art forms developed, including ink wash painting, ikebana flower arrangement, the tea ceremony, Japanese gardening, bonsai, and Noh theater.[105] Though the eighth Ashikaga shogun, Yoshimasa, was an ineffectual political and military leader, he played a critical role in promoting these cultural developments.[106] He had the famous Kinkaku-ji or "Temple of the Golden Pavilion" built in Kyoto in 1397.[107]
During the second half of the 16th century, Japan gradually reunified under two powerful warlords: Oda Nobunaga and Toyotomi Hideyoshi. The period takes its name from Nobunaga's headquarters, Azuchi Castle, and Hideyoshi's headquarters, Momoyama Castle.[70]
Nobunaga was the daimyō of the small province of Owari. He burst onto the scene suddenly, in 1560, when, during the Battle of Okehazama, his army defeated a force several times its size led by the powerful daimyō Imagawa Yoshimoto.[108] Nobunaga was renowned for his strategic leadership and his ruthlessness. He encouraged Christianity to incite hatred toward his Buddhist enemies and to forge strong relationships with European arms merchants. He equipped his armies with muskets and trained them with innovative tactics.[109] He promoted talented men regardless of their social status, including his peasant servant Toyotomi Hideyoshi, who became one of his best generals.[110]
The Azuchi–Momoyama period began in 1568, when Nobunaga seized Kyoto and thus effectively brought an end to the Ashikaga shogunate.[108] He was well on his way towards his goal of reuniting all Japan in 1582 when one of his own officers, Akechi Mitsuhide, killed him during an abrupt attack on his encampment. Hideyoshi avenged Nobunaga by crushing Akechi's uprising and emerged as Nobunaga's successor.[111] Hideyoshi completed the reunification of Japan by conquering Shikoku, Kyushu, and the lands of the Hōjō family in eastern Japan.[112] He launched sweeping changes to Japanese society, including the confiscation of swords from the peasantry, new restrictions on daimyōs, persecutions of Christians, a thorough land survey, and a new law effectively forbidding the peasants and samurai from changing their social class.[113] Hideyoshi's land survey designated all those who were cultivating the land as being "commoners", an act which effectively granted freedom to most of Japan's slaves.[114]
As Hideyoshi's power expanded, he dreamed of conquering China and launched two massive invasions of Korea starting in 1592. Hideyoshi failed to defeat the Chinese and Korean armies on the Korean Peninsula and the war ended after his death in 1598.[115] In the hope of founding a new dynasty, Hideyoshi had asked his most trusted subordinates to pledge loyalty to his infant son Toyotomi Hideyori. Despite this, almost immediately after Hideyoshi's death, war broke out between Hideyori's allies and those loyal to Tokugawa Ieyasu, a daimyō and a former ally of Hideyoshi.[116] Tokugawa Ieyasu won a decisive victory at the Battle of Sekigahara in 1600, ushering in 268 uninterrupted years of rule by the Tokugawa clan.[117]
The Edo period was characterized by relative peace and stability[118] under the tight control of the Tokugawa shogunate, which ruled from the eastern city of Edo (modern Tokyo).[119] In 1603, Emperor Go-Yōzei declared Tokugawa Ieyasu shōgun, and Ieyasu abdicated two years later to groom his son as the second shōgun of what became a long dynasty.[120] Nevertheless, it took time for the Tokugawas to consolidate their rule. In 1609, the shōgun gave the daimyō of Satsuma Domain permission to invade the Ryukyu Kingdom for perceived insults towards the shogunate; the Satsuma victory began 266 years of Ryukyu's dual subordination to Satsuma and China.[98][121] Ieyasu led the Siege of Osaka that ended with the destruction of the Toyotomi clan in 1615.[122] Soon after the shogunate promulgated the Laws for the Military Houses, which imposed tighter controls on the daimyōs,[123] and the alternate attendance system, which required each daimyō to spend every other year in Edo.[124] Even so, the daimyōs continued to maintain a significant degree of autonomy in their domains.[125] The central government of the shogunate in Edo, which quickly became the most populous city in the world,[119] took counsel from a group of senior advisors known as rōjū and employed samurai as bureaucrats.[126] The emperor in Kyoto was funded lavishly by the government but was allowed no political power.[127]
The Tokugawa shogunate went to great lengths to suppress social unrest. Harsh penalties, including crucifixion, beheading, and death by boiling, were decreed for even the most minor offenses, though criminals of high social class were often given the option of seppuku ("self-disembowelment"), an ancient form of suicide that became ritualized.[124] Christianity, which was seen as a potential threat, was gradually clamped down on until finally, after the Christian-led Shimabara Rebellion of 1638, the religion was completely outlawed.[128] To prevent further foreign ideas from sowing dissent, the third Tokugawa shogun, Iemitsu, implemented the sakoku ("closed country") isolationist policy under which Japanese people were not allowed to travel abroad, return from overseas, or build ocean-going vessels.[129] The only Europeans allowed on Japanese soil were the Dutch, who were granted a single trading post on the island of Dejima at Nagasaki from 1634 to 1854.[130] China and Korea were the only other countries permitted to trade,[131] and many foreign books were banned from import.[125]
During the first century of Tokugawa rule, Japan's population doubled to thirty million, mostly because of agricultural growth; the population remained stable for the rest of the period.[132] The shogunate's construction of roads, elimination of road and bridge tolls, and standardization of coinage promoted commercial expansion that also benefited the merchants and artisans of the cities.[133] City populations grew,[134] but almost ninety percent of the population continued to live in rural areas.[135] Both the inhabitants of cities and of rural communities would benefit from one of the most notable social changes of the Edo period: increased literacy and numeracy. The number of private schools greatly expanded, particularly those attached to temples and shrines, and raised literacy to thirty percent. This may have been the world's highest rate at the time[136] and drove a flourishing commercial publishing industry, which grew to produce hundreds of titles per year.[137] In the area of numeracy – approximated by an index measuring people's ability to report an exact rather than a rounded age (age-heaping method), and which level shows a strong correlation to later economic development of a country – Japan's level was comparable to that of north-west European countries, and moreover, Japan's index came close to the 100 percent mark throughout the nineteenth century. These high levels of both literacy and numeracy were part of the socio-economical foundation for Japan's strong growth rates during the following century.[138]
The Edo period was a time of cultural flourishing, as the merchant classes grew in wealth and began spending their income on cultural and social pursuits.[139][140] Members of the merchant class who patronized culture and entertainment were said to live hedonistic lives, which came to be called the ukiyo ("floating world").[141] This lifestyle inspired ukiyo-zōshi popular novels and ukiyo-e art, the latter of which were often woodblock prints[142] that progressed to greater sophistication and use of multiple printed colors.[143]
Forms of theater such as kabuki and bunraku puppet theater became widely popular.[144] These new forms of entertainment were (at the time) accompanied by short songs (kouta) and music played on the shamisen, a new import to Japan in 1600.[145] Haiku, whose greatest master is generally agreed to be Matsuo Bashō (1644–1694), also rose as a major form of poetry.[146] Geisha, a new profession of entertainers, also became popular. They would provide conversation, sing, and dance for customers, though they would not sleep with them.[147]
The Tokugawas sponsored and were heavily influenced by Neo-Confucianism, which led the government to divide society into four classes based on the four occupations.[148] The samurai class claimed to follow the ideology of bushido, literally "the way of the warrior".[149]
By the late eighteenth and early nineteenth centuries, the shogunate showed signs of weakening.[150] The dramatic growth of agriculture that had characterized the early Edo period had ended,[132] and the government handled the devastating Tenpō famines poorly.[150] Peasant unrest grew and government revenues fell.[151] The shogunate cut the pay of the already financially distressed samurai, many of whom worked side jobs to make a living.[152] Discontented samurai were soon to play a major role in engineering the downfall of the Tokugawa shogunate.[153]
At the same time, the people drew inspiration from new ideas and fields of study. Dutch books brought into Japan stimulated interest in Western learning, called rangaku or "Dutch learning".[154] The physician Sugita Genpaku, for instance, used concepts from Western medicine to help spark a revolution in Japanese ideas of human anatomy.[155] The scholarly field of kokugaku or "national learning", developed by scholars such as Motoori Norinaga and Hirata Atsutane, promoted what it asserted were native Japanese values. For instance, it criticized the Chinese-style Neo-Confucianism advocated by the shogunate and emphasized the Emperor's divine authority, which the Shinto faith taught had its roots in Japan's mythic past, which was referred to as the "Age of the Gods".[156]
The arrival in 1853 of a fleet of American ships commanded by Commodore Matthew C. Perry threw Japan into turmoil. The US government aimed to end Japan's isolationist policies. The shogunate had no defense against Perry's gunboats and had to agree to his demands that American ships be permitted to acquire provisions and trade at Japanese ports.[150] The Western powers imposed what became known as "unequal treaties" on Japan which stipulated that Japan must allow citizens of these countries to visit or reside on Japanese territory and must not levy tariffs on their imports or try them in Japanese courts.[157]
The shogunate's failure to oppose the Western powers angered many Japanese, particularly those of the southern domains of Chōshū and Satsuma.[158] Many samurai there, inspired by the nationalist doctrines of the kokugaku school, adopted the slogan of sonnō jōi ("revere the emperor, expel the barbarians").[159] The two domains went on to form an alliance. In August 1866, soon after becoming shogun, Tokugawa Yoshinobu, struggled to maintain power as civil unrest continued.[160] The Chōshū and Satsuma domains in 1868 convinced the young Emperor Meiji and his advisors to issue a rescript calling for an end to the Tokugawa shogunate. The armies of Chōshū and Satsuma soon marched on Edo and the ensuing Boshin War led to the fall of the shogunate.[161]
The emperor was restored to nominal supreme power,[162] and in 1869, the imperial family moved to Edo, which was renamed Tokyo ("eastern capital").[163] However, the most powerful men in the government were former samurai from Chōshū and Satsuma rather than the emperor, who was fifteen in 1868.[162] These men, known as the Meiji oligarchs, oversaw the dramatic changes Japan would experience during this period.[164] The leaders of the Meiji government desired Japan to become a modern nation-state that could stand equal to the Western imperialist powers.[165] Among them were Ōkubo Toshimichi and Saigō Takamori from Satsuma, as well as Kido Takayoshi, Ito Hirobumi, and Yamagata Aritomo from Chōshū.[162]
The Meiji government abolished the Edo class structure[166] and replaced the feudal domains of the daimyōs with prefectures.[163] It instituted comprehensive tax reform and lifted the ban on Christianity.[166] Major government priorities also included the introduction of railways, telegraph lines, and a universal education system.[167] The Meiji government promoted widespread Westernization[168] and hired hundreds of advisers from Western nations with expertise in such fields as education, mining, banking, law, military affairs, and transportation to remodel Japan's institutions.[169] The Japanese adopted the Gregorian calendar, Western clothing, and Western hairstyles.[170] One leading advocate of Westernization was the popular writer Fukuzawa Yukichi.[171] As part of its Westernization drive, the Meiji government enthusiastically sponsored the importation of Western science, above all medical science. In 1893, Kitasato Shibasaburō established the Institute for Infectious Diseases, which would soon become world-famous,[172] and in 1913, Hideyo Noguchi proved the link between syphilis and paresis.[173] Furthermore, the introduction of European literary styles to Japan sparked a boom in new works of prose fiction. Characteristic authors of the period included Futabatei Shimei and Mori Ōgai,[174] although the most famous of the Meiji era writers was Natsume Sōseki,[175] who wrote satirical, autobiographical, and psychological novels[176] combining both the older and newer styles.[177] Ichiyō Higuchi, a leading female author, took inspiration from earlier literary models of the Edo period.[178]
Government institutions developed rapidly in response to the Freedom and People's Rights Movement, a grassroots campaign demanding greater popular participation in politics. The leaders of this movement included Itagaki Taisuke and Ōkuma Shigenobu.[179] Itō Hirobumi, the first Prime Minister of Japan, responded by writing the Meiji Constitution, which was promulgated in 1889. The new constitution established an elected lower house, the House of Representatives, but its powers were restricted. Only two percent of the population were eligible to vote, and legislation proposed in the House required the support of the unelected upper house, the House of Peers. Both the cabinet of Japan and the Japanese military were directly responsible not to the elected legislature but to the emperor.[180] Concurrently, the Japanese government also developed a form of Japanese nationalism under which Shinto became the state religion and the emperor was declared a living god.[181] Schools nationwide instilled patriotic values and loyalty to the emperor.[167]
In December 1871, a Ryukyuan ship was shipwrecked on Taiwan and the crew were massacred. In 1874, using the incident as a pretext, Japan launched a military expedition to Taiwan to assert their claims to the Ryukyu Islands. The expedition featured the first instance of the Japanese military ignoring the orders of the civilian government, as the expedition set sail after being ordered to postpone.[182] Yamagata Aritomo, who was born a samurai in the Chōshū Domain, was a key force behind the modernization and enlargement of the Imperial Japanese Army, especially the introduction of national conscription.[183] The new army was put to use in 1877 to crush the Satsuma Rebellion of discontented samurai in southern Japan led by the former Meiji leader Saigo Takamori.[184]
The Japanese military played a key role in Japan's expansion abroad. The government believed that Japan had to acquire its own colonies to compete with the Western colonial powers. After consolidating its control over Hokkaido (through the Hokkaidō Development Commission) and annexing the Ryukyu Kingdom (the "Ryūkyū Disposition"), it next turned its attention to China and Korea.[185] In 1894, Japanese and Chinese troops clashed in Korea, where they were both stationed to suppress the Donghak Rebellion. During the ensuing First Sino-Japanese War, Japan's highly motivated and well-led forces defeated the more numerous and better-equipped military of Qing China.[186] The island of Taiwan was thus ceded to Japan in 1895,[187] and Japan's government gained enough international prestige to allow Foreign Minister Mutsu Munemitsu to renegotiate the "unequal treaties".[188] In 1902 Japan signed an important military alliance with the British.[189]
Japan next clashed with Russia, which was expanding its power in Asia. The Battle of Yalu River was the first time in decades that an Asian power defeated a western power.[190] The Russo-Japanese War of 1904–05 ended with the dramatic Battle of Tsushima, which was another victory for Japan's military. Japan thus laid claim to Korea as a protectorate in 1905, followed by full annexation in 1910.[191] The defeat of Russia in the war had set in motion a change in the global world order with the emergence of Japan as not only a regional power, but rather, the main Asian power.[192]
During the Meiji period, Japan underwent a rapid transition towards an industrial economy.[193] Both the Japanese government and private entrepreneurs adopted Western technology and knowledge to create factories capable of producing a wide range of goods.[194]
By the end of the period, the majority of Japan's exports were manufactured goods.[193] Some of Japan's most successful new businesses and industries constituted huge family-owned conglomerates called zaibatsu, such as Mitsubishi and Sumitomo.[195] The phenomenal industrial growth sparked rapid urbanization. The proportion of the population working in agriculture shrank from 75 percent in 1872 to 50 percent by 1920.[196] In 1927 the Tokyo Metro Ginza Line opened and it is the oldest subway line in Asia.[197]
Japan enjoyed solid economic growth at this time and most people lived longer and healthier lives. The population rose from 34 million in 1872 to 52 million in 1915.[198] Poor working conditions in factories led to growing labor unrest,[199] and many workers and intellectuals came to embrace socialist ideas.[200] The Meiji government responded with harsh suppression of dissent. Radical socialists plotted to assassinate the emperor in the High Treason Incident of 1910, after which the Tokkō secret police force was established to root out left-wing agitators.[201] The government also introduced social legislation in 1911 setting maximum work hours and a minimum age for employment.[202]
During the short reign of Emperor Taishō, Japan developed stronger democratic institutions and grew in international power. The Taishō political crisis opened the period with mass protests and riots organized by Japanese political parties, which succeeded in forcing Katsura Tarō to resign as prime minister.[203] This and the rice riots of 1918 increased the power of Japan's political parties over the ruling oligarchy.[204] The Seiyūkai and Minseitō parties came to dominate politics by the end of the so-called "Taishō democracy" era.[205] The franchise for the House of Representatives had been gradually expanded since 1890,[206] and in 1925 universal male suffrage was introduced. However, in the same year the far-reaching Peace Preservation Law also passed, prescribing harsh penalties for political dissidents.[207]
Japan's participation in World War I on the side of the Allies sparked unprecedented economic growth and earned Japan new colonies in the South Pacific seized from Germany.[208] After the war, Japan signed the Treaty of Versailles and enjoyed good international relations through its membership in the League of Nations and participation in international disarmament conferences.[209] The Great Kantō earthquake in September 1923 left over 100,000 dead, and combined with the resultant fires destroyed the homes of more than three million.[210]
The growth of popular prose fiction, which began during the Meiji period, continued into the Taishō period as literacy rates rose and book prices dropped.[211] Notable literary figures of the era included short story writer Ryūnosuke Akutagawa[212] and the novelist Haruo Satō. Jun'ichirō Tanizaki, described as "perhaps the most versatile literary figure of his day" by the historian Conrad Totman, produced many works during the Taishō period influenced by European literature, though his 1929 novel Some Prefer Nettles reflects deep appreciation for the virtues of traditional Japanese culture.[213] At the end of the Taishō period, Tarō Hirai, known by his penname Edogawa Ranpo, began writing popular mystery and crime stories.[212]
Emperor Hirohito's sixty-three-year reign from 1926 to 1989 is the longest in recorded Japanese history.[214] The first twenty years were characterized by the rise of extreme nationalism and a series of expansionist wars. After suffering defeat in World War II, Japan was occupied by foreign powers for the first time in its history, and then re-emerged as a major world economic power.[215]
Left-wing groups had been subject to violent suppression by the end of the Taishō period,[216] and radical right-wing groups, inspired by fascism and Japanese nationalism, rapidly grew in popularity.[217] The extreme right became influential throughout the Japanese government and society, notably within the Kwantung Army, a Japanese army stationed in China along the Japanese-owned South Manchuria Railroad.[218] During the Manchurian Incident of 1931, radical army officers bombed a small portion of the South Manchuria Railroad and, falsely attributing the attack to the Chinese, invaded Manchuria. The Kwantung Army conquered Manchuria and set up the puppet government of Manchukuo there without permission from the Japanese government. International criticism of Japan following the invasion led to Japan withdrawing from the League of Nations.[219]
Prime Minister Tsuyoshi Inukai of the Seiyūkai Party attempted to restrain the Kwantung Army and was assassinated in 1932 by right-wing extremists. Because of growing opposition within the Japanese military and the extreme right to party politicians, who they saw as corrupt and self-serving, Inukai was the last party politician to govern Japan in the pre-World War II era.[219] In February 1936 young radical officers of the Imperial Japanese Army attempted a coup d'état. They assassinated many moderate politicians before the coup was suppressed.[220] In its wake the Japanese military consolidated its control over the political system and most political parties were abolished when the Imperial Rule Assistance Association was founded in 1940.[221]
Japan's expansionist vision grew increasingly bold. Many of Japan's political elite aspired to have Japan acquire new territory for resource extraction and settlement of surplus population.[222] These ambitions led to the outbreak of the Second Sino-Japanese War in 1937. After their victory in the Chinese capital, the Japanese military committed the infamous Nanjing massacre. The Japanese military failed to defeat the Chinese government led by Chiang Kai-shek and the war descended into a bloody stalemate that lasted until 1945.[223] Japan's stated war aim was to establish the Greater East Asia Co-Prosperity Sphere, a vast pan-Asian union under Japanese domination.[224] Hirohito's role in Japan's foreign wars remains a subject of controversy, with various historians portraying him as either a powerless figurehead or an enabler and supporter of Japanese militarism.[225]
The United States opposed Japan's invasion of China and responded with increasingly stringent economic sanctions intended to deprive Japan of the resources to continue its war in China.[226] Japan reacted by forging an alliance with Germany and Italy in 1940, known as the Tripartite Pact, which worsened its relations with the US. In July 1941, the United States, the United Kingdom, and the Netherlands froze all Japanese assets when Japan completed its invasion of French Indochina by occupying the southern half of the country, further increasing tension in the Pacific.[227]
In late 1941, Japan's government, led by Prime Minister and General Hideki Tojo, decided to break the US-led embargo through force of arms.[228] On 7 December 1941, the Imperial Japanese Navy launched a surprise attack on the American fleet at Pearl Harbor, Hawaii. This brought the US into World War II on the side of the Allies. Japan then successfully invaded the Asian colonies of the United States, the United Kingdom, and the Netherlands, including the Philippines, Malaya, Hong Kong, Singapore, Burma, and the Dutch East Indies.[229]
In the early stages of the war, Japan scored victory after victory. The tide began to turn against Japan following the Battle of Midway in June 1942 and the subsequent Battle of Guadalcanal, in which Allied troops wrested the Solomon Islands from Japanese control.[230] During this period the Japanese military was responsible for such war crimes as mistreatment of prisoners of war, massacres of civilians, and the use of chemical and biological weapons.[231] The Japanese military earned a reputation for fanaticism, often employing banzai charges and fighting almost to the last man against overwhelming odds.[232] In 1944 the Imperial Japanese Navy began deploying squadrons of kamikaze pilots who crashed their planes into enemy ships.[233]
Life in Japan became increasingly difficult for civilians due to stringent rationing of food, electrical outages, and a brutal crackdown on dissent.[234] In 1944 the US Army captured the island of Saipan, which allowed the United States to begin widespread bombing raids on the Japanese mainland.[235] These destroyed over half of the total area of Japan's major cities.[236] The Battle of Okinawa, fought between April and June 1945, was the largest naval operation of the war and left 115,000 soldiers and 150,000 Okinawan civilians dead, suggesting that the planned invasion of mainland Japan would be even bloodier.[237] The Japanese superbattleship Yamato was sunk en route to aid in the Battle of Okinawa.[238]
However, on 6 August 1945, the US dropped an atomic bomb over Hiroshima, killing over 70,000 people. This was the first nuclear attack in history. On 9 August the Soviet Union declared war on Japan and invaded Manchukuo and other territories, and Nagasaki was struck by a second atomic bomb, killing around 40,000 people.[239] The surrender of Japan was communicated to the Allies on 14 August and broadcast by Emperor Hirohito on national radio the following day.[240]
Japan experienced dramatic political and social transformation under the Allied occupation in 1945–1952. US General Douglas MacArthur, the Supreme Commander of Allied Powers, served as Japan's de facto leader and played a central role in implementing reforms, many inspired by the New Deal of the 1930s.[241]
The occupation sought to decentralize power in Japan by breaking up the zaibatsu, transferring ownership of agricultural land from landlords to tenant farmers,[242] and promoting labor unionism.[243] Other major goals were the demilitarization and democratization of Japan's government and society. Japan's military was disarmed,[244] its colonies were granted independence,[245] the Peace Preservation Law and Tokkō were abolished,[246] and the International Military Tribunal of the Far East tried war criminals.[247] The cabinet became responsible not to the Emperor but to the elected National Diet.[248] The Emperor was permitted to remain on the throne, but was ordered to renounce his claims to divinity, which had been a pillar of the State Shinto system.[249] Japan's new constitution came into effect in 1947 and guaranteed civil liberties, labor rights, and women's suffrage,[250] and through Article 9, Japan renounced its right to go to war with another nation.[251]
The San Francisco Peace Treaty of 1951 officially normalized relations between Japan and the United States. The occupation ended in 1952, although the US continued to administer a number of the Ryukyu Islands.[252] In 1968, the Ogasawara Islands were returned from US occupation to Japanese sovereignty. Japanese citizens were allowed to return. Okinawa was the last to be returned in 1972.[253] The US continues to operate military bases throughout the Ryukyu Islands, mostly on Okinawa, as part of the US-Japan Security Treaty.[254]
Shigeru Yoshida served as prime minister in 1946–1947 and 1948–1954, and played a key role in guiding Japan through the occupation.[255] His policies, known as the Yoshida Doctrine, proposed that Japan should forge a tight relationship with the United States and focus on developing the economy rather than pursuing a proactive foreign policy.[256] Yoshida was one of the longest serving prime ministers in Japanese history.[257] Yoshida's Liberal Party merged in 1955 into the new Liberal Democratic Party (LDP),[258] which went on to dominate Japanese politics for the remainder of the Shōwa period.[259]
Although the Japanese economy was in bad shape in the immediate postwar years, an austerity program implemented in 1949 by finance expert Joseph Dodge ended inflation.[260] The Korean War (1950–1953) was a major boon to Japanese business.[261] In 1949 the Yoshida cabinet created the Ministry of International Trade and Industry (MITI) with a mission to promote economic growth through close cooperation between the government and big business. MITI sought successfully to promote manufacturing and heavy industry,[262] and encourage exports.[263] The factors behind Japan's postwar economic growth included technology and quality control techniques imported from the West, close economic and defense cooperation with the United States, non-tariff barriers to imports, restrictions on labor unionization, long work hours, and a generally favorable global economic environment.[264] Japanese corporations successfully retained a loyal and experienced workforce through the system of lifetime employment, which assured their employees a safe job.[265]
By 1955, the Japanese economy had grown beyond prewar levels,[266] and by 1968 it had become the second largest capitalist economy in the world.[267] The GNP expanded at an annual rate of nearly 10% from 1956 until the 1973 oil crisis slowed growth to a still-rapid average annual rate of just over 4% until 1991.[268] Life expectancy rose and Japan's population increased to 123 million by 1990.[269] Ordinary Japanese people became wealthy enough to purchase a wide array of consumer goods. During this period, Japan became the world's largest manufacturer of automobiles and a leading producer of electronics.[270] Japan signed the Plaza Accord in 1985 to depreciate the US dollar against the yen and other currencies. By the end of 1987, the Nikkei stock market index had doubled and the Tokyo Stock Exchange became the largest in the world. During the ensuing economic bubble, stock and real-estate loans grew rapidly.[271]
Japan became a member of the United Nations in 1956 and further cemented its international standing in 1964, when it hosted the Olympic Games in Tokyo.[272] Japan was a close ally of the United States during the Cold War, though this alliance did not have unanimous support from the Japanese people. As requested by the United States, Japan reconstituted its army in 1954 under the name Japan Self-Defense Forces (JSDF), though some Japanese insisted that the very existence of the JSDF was a violation of Article 9 of Japan's constitution.[273] In 1960, the massive Anpo protests saw hundreds of thousands of citizens take to the streets in opposition to the US-Japan Security Treaty.[274] Japan successfully normalized relations with the Soviet Union in 1956, despite an ongoing dispute over the ownership of the Kuril Islands,[275] and with South Korea in 1965, despite an ongoing dispute over the ownership of the islands of Liancourt Rocks.[276] In accordance with US policy, Japan recognized the Republic of China on Taiwan as the legitimate government of China after World War II, though Japan switched its recognition to the People's Republic of China in 1972.[277]
Among cultural developments, the immediate post-occupation period became a golden age for Japanese cinema.[278] The reasons for this include the abolition of government censorship, low film production costs, expanded access to new film techniques and technologies, and huge domestic audiences at a time when other forms of recreation were relatively scarce.[279] On 1 October 1964, Japan's first high-speed rail line was built called the Tokaido Shinkansen.[280] It is also the oldest high-speed rail system in the world.[280]
Emperor Akihito's reign began upon the death of his father, Emperor Hirohito. The economic bubble popped in 1989, and stock and land prices plunged as Japan entered a deflationary spiral. Banks found themselves saddled with insurmountable debts that hindered economic recovery.[281] Stagnation worsened as the birthrate declined far below replacement level.[282] The 1990s are often referred to as Japan's Lost Decade.[283] Economic performance was often poor in the following decades, and the stock market never returned to its pre-1989 highs.[284] Japan's system of lifetime employment largely collapsed and unemployment rates rose.[285] The faltering economy and several corruption scandals weakened the LDP's dominant political position. Japan was nevertheless governed by non-LDP prime ministers only in 1993–1996[286] and 2009–2012.[287]
Japan's dealing with its war legacy strained relations with China and Korea. Japanese officials and emperors have made over 50 formal war apologies since the 1950s. However, some politicians of China and Korea found the official apologies, such as those of the Emperor in 1990 and the Murayama Statement of 1995, inadequate or insincere.[288] Nationalist politics have exacerbated this, such as denial of the Nanjing Massacre and other war crimes,[289] revisionist history textbooks, which provoked protests in East Asia.[290] Japanese politicians make frequent visits to Yasukuni Shrine to commemorate the people who died in wars from 1868 to 1954, but convicted war criminals are among the enshrined.[291]
The population of Japan peaked at 128,083,960 in 2008.[292] It had decreased by 2,373,960 by December 2020.[292] In 2011, the economy of China became the world's second largest. Japan's economy descended to third largest by nominal GDP.[293] Despite Japan's economic difficulties, this period also saw Japanese popular culture, including video games, anime, and manga, expanding worldwide, especially among young people.[294] In March 2011, the Tokyo Skytree became the tallest tower in the world at 634 metres (2,080 ft), displacing the Canton Tower.[295][296] It is the second tallest structure in the world after the Burj Khalifa (829.8 m or 2,722 ft).[297]
On 11 March 2011, one of the largest earthquakes recorded in Japan occurred in the northeast. The resulting tsunami damaged the nuclear facilities in Fukushima, which suffered a nuclear meltdown and severe radiation leakage.[298]
Emperor Naruhito's reign began upon the abdication of his father, Emperor Akihito, on 1 May 2019.[299]
In 2020, Tokyo was due to host the Summer Olympics for the second time since 1964. Japan was the first Asian country to host the Olympics twice. However, due to the global outbreak and economic impact of COVID-19 pandemic, the Summer Olympics were postponed to 2021; they took place from 23 July to 8 August 2021.[300] Japan ranked third place, with 27 gold medals.[301]
When the 2022 Russian invasion of Ukraine began, Japan condemned and levied sanctions on Russia for its actions.[302] Ukrainian President Volodymyr Zelenskyy praised Japan as the "first Asian nation that has begun exerting pressure on Russia."[302] Japan froze the assets of Russia's central bank and other major Russian banks and assets owned by 500 Russian citizens and organizations.[302] Japan banned new investments and the export of high tech to the country. Russia's trade status as favored nation was revoked.[302] Japan's swift actions shows its becoming a leading power in the world.[302] The war in Ukraine and threats from China and North Korea caused a shift in Japan's security policy with higher defense spending which erodes its former pacifist stance.[302]
On 8 July 2022, former Prime Minister Shinzo Abe, was assassinated in the city of Nara by former Japan Self-Defense Force Marine Tetsuya Yamagami while campaigning two days before the 2022 House of Councillors election.[303] This shocked the public, because firearm fatalities are very rare in Japan. There were only 10 shooting deaths from 2017 to 2020 and 1 gun death incident in 2021.[304]
After 2022 visit by Nancy Pelosi to Taiwan, China conducted “precision missile strikes” in the ocean around Taiwan's coastline on August 4, 2022.[305] These military exercises raised tensions in the region.[305] The Japanese Ministry of Defense reported that this was the first time ballistic missiles launched by China landed in Japan's exclusive economic zone and lodged a diplomatic protest with Beijing.[306] Five Chinese missiles landed in Japan’s EEZ off Hateruma which is near Taiwan.[305] Japanese Defense Minister Nobuo Kishi said these missiles are “serious threats to Japan’s national security and the safety of the Japanese people.”[305]
Social stratification in Japan became pronounced during the Yayoi period. Expanding trade and agriculture increased the wealth of society, which was increasingly monopolized by social elites.[307] By 600 AD, a class structure had developed which included court aristocrats, the families of local magnates, commoners, and slaves.[308] Over 90% were commoners, who included farmers, merchants, and artisans.[309] During the late Heian period, the governing elite consisted of three classes. The traditional aristocracy shared power with Buddhist monks and samurai,[309] though the latter became increasingly dominant in the Kamakura and Muromachi periods.[310] These periods witnessed the rise of the merchant class, which diversified into a greater variety of specialized occupations.[311]
Women initially held social and political equality with men,[308] and archaeological evidence suggests a prehistorical preference for female rulers in western Japan. Female Emperors appear in recorded history until the Meiji Constitution declared strict male-only ascension in 1889.[312] Chinese Confucian-style patriarchy was first codified in the 7th–8th centuries with the ritsuryō system,[313] which introduced a patrilineal family register with a male head of household.[314] Women until then had held important roles in government which thereafter gradually diminished, though even in the late Heian period women wielded considerable court influence.[312] Marital customs and many laws governing private property remained gender neutral.[315]
For reasons that are unclear to historians the status of women rapidly deteriorated from the fourteenth century and onwards.[316] Women of all social classes lost the right to own and inherit property and were increasingly viewed as inferior to men.[317] Hideyoshi's land survey of the 1590s further entrenched the status of men as dominant landholders.[318] During the US occupation following World War II , women gained legal equality with men,[319] but faced widespread workplace discrimination. A movement for women's rights led to the passage of an equal employment law in 1986, but by the 1990s women held only 10% of management positions.[320]
Hideyoshi's land survey of the 1590s designated all who cultivated the land as commoners, an act which granted effective freedom to most of Japan's slaves.[321]
The Tokugawa shogunate rigidified long-existent class divisions,[322] placing most of the population into a Neo-Confucian hierarchy of four occupations, with the ruling elite at the top, followed by the peasants who made up 80% of the population, then artisans, and merchants at the bottom.[323] Court nobles,[42] clerics, outcasts, entertainers, and workers of the licensed quarters fell outside this structure.[324] Different legal codes applied to different classes, marriage between classes was prohibited, and towns were subdivided into different class areas.[322] The social stratification had little bearing on economic conditions: many samurai lived in poverty[324] and the wealth of the merchant class grew throughout the period as the commercial economy developed and urbanization grew.[325] The Edo-era social power structure proved untenable and gave way following the Meiji Restoration to one in which commercial power played an increasingly significant political role.[326]
Although all social classes were legally abolished at the start of the Meiji period,[166] income inequality greatly increased.[327] New economic class divisions were formed between capitalist business owners who formed the new middle class, small shopkeepers of the old middle class, the working class in factories, rural landlords, and tenant farmers.[328] The great disparities of income between the classes dissipated during and after World War II, eventually declining to levels that were among the lowest in the industrialized world.[327] Some postwar surveys indicated that up to 90% of Japanese self-identified as being middle class.[329]
Populations of workers in professions considered unclean, such as leatherworkers and those who handled the dead, developed in the 15th and 16th centuries into hereditary outcast communities.[330] These people, later called burakumin, fell outside the Edo-period class structure and suffered discrimination that lasted after the class system was abolished.[330] Though activism has improved the social conditions of those from burakumin backgrounds, discrimination in employment and education has lingered into the 21st century.[330]

The history of Christianity concerns the Christian religion, Christian countries, and the Christians with their various denominations, from the 1st century to the present. Christianity originated with the ministry of Jesus, a Jewish teacher and healer who proclaimed the imminent Kingdom of God and was crucified c. AD 30–33 in Jerusalem in the Roman province of Judea.[1] His followers believe that, according to the Gospels, he was the Son of God and that he died for the forgiveness of sins and was raised from the dead and exalted by God, and will return soon at the inception of God's kingdom.[1]
The earliest followers of Jesus were apocalyptic Jewish Christians.[1] The inclusion of Gentiles in the developing early Christian Church caused the separation of early Christianity from Judaism during the first two centuries of the Christian era.[2] In 313, the Roman Emperor Constantine I issued the Edict of Milan legalizing Christian worship.[3] In 380, with the Edict of Thessalonica put forth under Theodosius I, the Roman Empire officially adopted Trinitarian Christianity as its state religion, and Christianity established itself as a predominantly Roman religion in the state church of the Roman Empire.[4] Various Christological debates about the human and divine nature of Jesus consumed the Christian Church for three centuries, and seven ecumenical councils were called to resolve these debates.[5] Arianism was condemned at the First Council of Nicea (325), which supported the Trinitarian doctrine as expounded in the Nicene Creed.[5]
In the Early Middle Ages, missionary activities spread Christianity towards the west and the north among Germanic peoples;[6] towards the east among Armenians, Georgians, and Slavic peoples;[7] in the Middle East among Syrians and Egyptians;[8] in Eastern Africa among the Ethiopians;[9] and further into Central Asia, China, and India.[10] During the High Middle Ages, Eastern and Western Christianity grew apart, leading to the East–West Schism of 1054. Growing criticism of the Roman Catholic ecclesiastical structure and its corruption led to the Protestant Reformation and its related reform movements in the 15th and 16th centuries, which concluded with the European wars of religion that set off the split of Western Christianity. Since the Renaissance era, with the European colonization of the Americas and other continents actively instigated by the Christian churches,[11][12][13][14] Christianity has expanded throughout the world.[15] Today, there are more than two billion Christians worldwide[16] and Christianity has become the world's largest religion.[17] Within the last century, as the influence of Christianity has progressively waned in the Western world, Christianity continues to be the predominant religion in Europe (including Russia) and the Americas, and has rapidly grown in Asia as well as in the Global South and Third World countries, most notably in Latin America, China, South Korea, and much of Sub-Saharan Africa.[18][19][20][21]
The religious, social, and political climate of 1st-century Roman Judea and its neighbouring provinces was extremely diverse and constantly characterized by socio-political turmoil,[1][22][23] with numerous Judaic movements that were both religious and political.[24] The ancient Roman-Jewish historian Josephus described the four most prominent sects within Second Temple Judaism: Pharisees, Sadducees, Essenes, and an unnamed "fourth philosophy",[25] which modern historians recognize to be the Zealots and Sicarii.[26] The 1st century BC and 1st century AD had numerous charismatic religious leaders contributing to what would become the Mishnah of Rabbinic Judaism, including the Jewish sages Yohanan ben Zakkai and Hanina ben Dosa. Jewish messianism, and the Jewish Messiah concept, has its roots in the apocalyptic literature produced between the 2nd century BC and the 1st century BC,[27] promising a future "anointed" leader (messiah or king) from the Davidic line to resurrect the Israelite Kingdom of God, in place of the foreign rulers of the time.[1]
The main sources of information regarding Jesus' life and teachings are the four canonical gospels, and to a lesser extent the Acts of the Apostles and the Pauline epistles. According to the Gospels, Jesus is the Son of God, who was crucified c. AD 30–33 in Jerusalem.[1] His followers believed that he was raised from the dead and exalted by God, heralding the coming Kingdom of God.[1]

Early Christianity is generally reckoned by church historians to begin with the ministry of Jesus (c. 27–30) and end with the First Council of Nicaea (325). It is typically divided into two periods: the Apostolic Age (c. 30–100, when the first apostles were still alive) and the Ante-Nicene Period (c. 100–325).[28]
The Apostolic Age is named after the Apostles and their missionary activities. It holds special significance in Christian tradition as the age of the direct apostles of Jesus. A primary source for the Apostolic Age is the Acts of the Apostles, but its historical accuracy has been debated and its coverage is partial, focusing especially from Acts 15[29] onwards on the ministry of Paul, and ending around 62 AD with Paul preaching in Rome under house arrest.
The earliest followers of Jesus were a sect of apocalyptic Jewish Christians within the realm of Second Temple Judaism.[1][30][31][32][33] The early Christian groups were strictly Jewish, such as the Ebionites,[30] and the early Christian community in Jerusalem, led by James the Just, brother of Jesus.[34] According to Acts 9,[35] they described themselves as "disciples of the Lord" and [followers] "of the Way", and according to Acts 11,[36] a settled community of disciples at Antioch were the first to be called "Christians". Some of the early Christian communities attracted God-fearers, i.e. Greco-Roman sympathizers which made an allegiance to Judaism but refused to convert and therefore retained their Gentile (non-Jewish) status, who already visited Jewish synagogues.[37][38] The inclusion of Gentiles posed a problem, as they could not fully observe the Halakha. Saul of Tarsus, commonly known as Paul the Apostle, persecuted the early Jewish Christians, then converted and started his mission among the Gentiles.[37] The main concern of Paul's letters is the inclusion of Gentiles into God's New Covenant, sending the message that faith in Christ is sufficient for salvation.[37][39][40] Because of this inclusion of Gentiles, early Christianity changed its character and gradually grew apart from Judaism during the first two centuries of the Christian Era.[37] The fourth-century church fathers Eusebius and Epiphanius of Salamis cite a tradition that before the destruction of Jerusalem in AD 70 the Jerusalem Christians had been warned to flee to Pella in the region of the Decapolis across the Jordan River.[41]
The Gospels and New Testament epistles contain early creeds and hymns, as well as accounts of the Passion, the empty tomb, and Resurrection appearances.[43] Early Christianity spread to pockets of believers among Aramaic-speaking peoples along the Mediterranean coast and also to the inland parts of the Roman Empire and beyond, into the Parthian Empire and the later Sasanian Empire, including Mesopotamia, which was dominated at different times and to varying extent by these empires.[44]
The ante-Nicene period (literally meaning "before Nicaea") was the period following the Apostolic Age down to the First Council of Nicaea in 325. By the beginning of the Nicene period, the Christian faith had spread throughout Western Europe and the Mediterranean Basin, and to North Africa and the East. A more formal Church structure grew out of the early communities, and various Christian doctrines developed. Christianity grew apart from Judaism, creating its own identity by an increasingly harsh rejection of Judaism and of Jewish practices. 
The number of Christians grew by approximately 40% per decade during the first and second centuries.[45] In the post-Apostolic church a hierarchy of clergy gradually emerged as overseers of urban Christian populations took on the form of episkopoi (overseers, the origin of the terms bishop and episcopal) and presbyters (elders; the origin of the term priest) and then deacons (servants). But this emerged slowly and at different times in different locations. Clement, a 1st-century bishop of Rome, refers to the leaders of the Corinthian church in his epistle to Corinthians as bishops and presbyters interchangeably. The New Testament writers also use the terms overseer and elders interchangeably and as synonyms.[46]
The Ante-Nicene period saw the rise of a great number of Christian sects, cults, and movements with strong unifying characteristics which were lacking in the apostolic period. They had different interpretations of the Bible, particularly regarding theological doctrines such as the divinity of Jesus and the nature of the Trinity. Many of the variations which existed during this time defy neat categorizations, because various forms of Christianity interacted in a complex fashion in order to form the dynamic character of Christianity which existed during this era. The Post-Apostolic period was diverse both in terms of beliefs and practices. In addition to the broad spectrum of general branches of Christianity, there was constant change and diversity that variably resulted in both internecine conflicts and syncretic adoption.[47][48][49][50]
The letters of the Apostle Paul sent to the early Christian communities in Rome, Greece, and Asia Minor were circulating in collected form by the end of the 1st century.[51] By the early 3rd century, there existed a set of early Christian writings similar to the current New Testament,[52] though there were still disputes over the canonicity of texts such as the Epistle to the Hebrews, the Epistle of James, the First and Second Epistle of Peter, the First Epistle of John, and the Book of Revelation.[53][54] By the 4th century, there existed unanimity in the Latin Church concerning the canonical texts included in the New Testament canon,[55] and by the 5th century the Eastern Churches, with a few exceptions, had come to accept the Book of Revelation and thus had come into harmony on the matter of the canon.[56]
As Christianity spread throughout the provinces of the Roman Empire and beyond its borders, it acquired certain members from high-ranking social classes and well-educated circles of the Hellenistic world; they sometimes became bishops. They produced two sorts of works, theological and apologetic, the latter being works aimed at defending the Christian faith by using reason, philosophy, and sacred scriptures to refute arguments against the veracity of Christianity. These authors are known as the Church Fathers, and the study of their lives and writings is called "patristics". Notable early Church Fathers include Ignatius of Antioch, Polycarp, Justin Martyr, Irenaeus, Clement of Alexandria, Tertullian, and Origen.
Early Christian art and architecture emerged relatively late and the first known Christian images emerge from about 200 AD,[57] although there is some literary evidence that small domestic images were used earlier.[citation needed] The oldest known Christian paintings are from the Roman catacombs, dated to about 200, and the oldest Christian sculptures are from sarcophagi, dating to the beginning of the 3rd century.[58] The early rejection of images, and the necessity to hide Christian practice from persecution, left behind few written records regarding early Christianity and its evolution.[58]
There was no empire-wide persecution of Christians until the reign of Decius in the 3rd century.[59] The last and most severe persecution organised by the imperial Roman authorities was the Diocletianic Persecution, 303–311.[60]
The Edict of Serdica was issued in 311 by the Roman Emperor Galerius, officially ending the persecution of Christians in the East. With the promulgation of the Edict of Milan (313), in which the Roman Emperors Constantine the Great and Licinius legalized the Christian religion, persecution of Christians by the Roman state ceased.[61]
The Kingdom of Armenia became the first country in the world to establish Christianity as its state religion when, in an event traditionally dated to the year 301, Gregory the Illuminator convinced Tiridates III, the King of Armenia, to convert to Christianity.
How much Christianity the Roman Emperor Constantine adopted at this point is difficult to discern,[62] but his accession was a turning point for the Christian Church. He supported the Church financially, built various basilicas, granted privileges (e.g., exemption from certain taxes) to clergy, promoted Christians to some high offices, and returned confiscated property.[63] Constantine played an active role in the leadership of the Church. In 316, he acted as a judge in a North African dispute concerning the Donatist controversy. More significantly, in 325 he summoned the Council of Nicaea, the first ecumenical council. He thus established a precedent for the emperor as responsible to God for the spiritual health of his subjects, and thus with a duty to maintain orthodoxy. He was to enforce doctrine, root out heresy, and uphold ecclesiastical unity.[64]
The successor of Constantine's son, his nephew Julian, under the influence of his adviser Mardonius, renounced Christianity and embraced a Neoplatonic and mystical form of Greco-Roman Paganism, shocking the Christian establishment.[65] He attempted to revive Greco-Roman Paganism in the Roman Empire and began by reopening the Pagan temples, modifying them to resemble Christian traditions, such as the episcopal structure and public charity (previously unknown in the Greco-Roman religion). Julian's short reign ended when he was wounded in the Battle of Samarra and died days later during the expedition against the Sasanian Empire (363).
An increasingly popular Nontrinitarian Christological doctrine that spread throughout the Roman Empire from the 4th century onwards was Arianism,[66][67] founded by the Christian presbyter Arius from Alexandria, Egypt, which taught that Jesus Christ is a creature distinct from and subordinate to God the Father.[66][67]
Although the Arian doctrine was condemned as heresy and eventually eliminated by the State church of the Roman Empire, it remained popular underground for some time. In the late 4th century, Ulfilas, a Roman Arian bishop, was appointed as the first Christian missionary to the Goths, the Germanic peoples in much of Europe at the borders of and within the Roman Empire.[66][67] Ulfilas spread Arian Christianity among the Goths, firmly establishing the faith among many of the Germanic tribes, thus helping to keep them culturally and religiously distinct from Chalcedonian Christians.[66][67][68]
During this age, the first ecumenical councils were convened. They were mostly concerned with Christological and theological disputes. The First Council of Nicaea (325) and the First Council of Constantinople (381) resulted in condemnation of Arian teachings as heresy and produced the Nicene Creed.
On 27 February 380, with the Edict of Thessalonica put forth under Theodosius I, Gratian, and Valentinian II, the Roman Empire officially adopted Trinitarian Christianity as its state religion. Prior to this date, Constantius II and Valens had personally favoured Arian or Semi-Arian forms of Christianity, but Valens' successor Theodosius I supported the Trinitarian doctrine as expounded in the Nicene Creed.
After its establishment, the Church adopted the same organisational boundaries as the Empire: geographical provinces, called dioceses, corresponding to imperial government territorial divisions. The bishops, who were located in major urban centres as in pre-legalisation tradition, thus oversaw each diocese. The bishop's location was his "seat", or "see". Among the sees, five came to hold special eminence: Rome, Constantinople, Jerusalem, Antioch, and Alexandria. The prestige of most of these sees depended in part on their apostolic founders, from whom the bishops were therefore the spiritual successors. Though the bishop of Rome was still held to be the First among equals, Constantinople was second in precedence as the new capital of the empire.
Theodosius I decreed that others not believing in the preserved "faithful tradition", such as the Trinity, were to be considered to be practitioners of illegal heresy,[69] and in 385, this resulted in the first case of the state, not Church, infliction of capital punishment on a heretic, namely Priscillian.[70][71]
During the early 5th century, the School of Edessa had taught a Christological perspective stating that Christ's divine and human nature were distinct persons. A particular consequence of this perspective was that Mary could not be properly called the mother of God but could only be considered the mother of Christ. The most widely known proponent of this viewpoint was the Patriarch of Constantinople Nestorius. Since referring to Mary as the mother of God had become popular in many parts of the Church this became a divisive issue.
The Roman Emperor Theodosius II called for the Council of Ephesus (431), with the intention of settling the issue. The council ultimately rejected Nestorius' view. Many churches who followed the Nestorian viewpoint broke away from the Roman Church, causing a major schism. The Nestorian churches were persecuted, and many followers fled to the Sasanian Empire where they were accepted. The Sasanian (Persian) Empire had many Christian converts early in its history, tied closely to the Syriac branch of Christianity. The Sasanian Empire was officially Zoroastrian and maintained a strict adherence to this faith, in part to distinguish itself from the religion of the Roman Empire (originally the Greco-Roman Paganism and then Christianity). Christianity became tolerated in the Sasanian Empire, and as the Roman Empire increasingly exiled heretics during the 4th and 6th centuries, the Sasanian Christian community grew rapidly.[72] By the end of the 5th century, the Persian Church was firmly established and had become independent of the Roman Church. This church evolved into what is today known as the Church of the East.
In 451, the Council of Chalcedon was held to further clarify the Christological issues surrounding Nestorianism. The council ultimately stated that Christ's divine and human nature were separate but both part of a single entity, a viewpoint rejected by many churches who called themselves miaphysites. The resulting schism created a communion of churches, including the Armenian, Syrian, and Egyptian churches.[73] Though efforts were made at reconciliation in the next few centuries, the schism remained permanent, resulting in what is today known as Oriental Orthodoxy.
Monasticism is a form of asceticism whereby one renounces worldly pursuits and goes off alone as a hermit or joins a tightly organized community. It began early in the Christian Church as a family of similar traditions, modelled upon Scriptural examples and ideals, and with roots in certain strands of Judaism. John the Baptist is seen as an archetypical monk, and monasticism was inspired by the organisation of the Apostolic community as recorded in Acts 2:42–47.[74]
Notable Christian authors of Late Antiquity such as Origen, St Jerome, John Chrysostom, and Augustine of Hippo, interpreted meanings of the Biblical texts within a highly asceticized religious environment.[75] Scriptural examples of asceticism could be found in the lives of John the Baptist, Jesus Christ, the twelve apostles, and Paul the Apostle.[75] The Dead Sea Scrolls revealed ascetic practices of the ancient Jewish sect of Essenes who took vows of abstinence to prepare for a holy war. An emphasis on an ascetic religious life was evident in both early Christian writings (see: Philokalia) and practices (see: Hesychasm). Other Christian practitioners of asceticism include saints such as Paul the Hermit, Simeon Stylites, David of Wales, John of Damascus, and Francis of Assisi.[75]
The deserts of the Middle East were at one time inhabited by thousands of male and female Christian ascetics, hermits and anchorites,[76] including St. Anthony the Great (otherwise known as St. Anthony of the Desert), St. Mary of Egypt, and St. Simeon Stylites, collectively known as the Desert Fathers and Desert Mothers. In 963 an association of monasteries called Lavra was formed on Mount Athos, in Eastern Orthodox tradition.[77] This became the most important center of orthodox Christian ascetic groups in the centuries that followed.[77] In the modern era, Mount Athos and Meteora have remained a significant center.[78]
Eremitic monks, or hermits, live in solitude, whereas cenobitics live in communities, generally in a monastery, under a rule (or code of practice) and are governed by an abbot. Originally, all Christian monks were hermits, following the example of Anthony the Great. However, the need for some form of organised spiritual guidance lead Pachomius in 318 to organise his many followers in what was to become the first monastery. Soon, similar institutions were established throughout the Egyptian desert as well as the rest of the eastern half of the Roman Empire. Women were especially attracted to the movement.[79] Central figures in the development of monasticism were Basil the Great in the East and, in the West, Benedict, who created the Rule of Saint Benedict, which would become the most common rule throughout the Middle Ages and the starting point for other monastic rules.[80]
The transition into the Early Middle Ages was a gradual and localised process. Rural areas rose as power centres whilst urban areas declined. Although a greater number of Christians remained in the East (Greek areas), important developments were underway in the West (Latin areas), and each took on distinctive shapes. The bishops of Rome, the popes, were forced to adapt to drastically changing circumstances. Maintaining only nominal allegiance to the emperor, they were forced to negotiate balances with the "barbarian rulers" of the former Roman provinces. In the East, the Church maintained its structure and character and evolved more slowly.
The stepwise loss of Western Roman Empire dominance, replaced with foederati and Germanic kingdoms, coincided with early missionary efforts into areas not controlled by the collapsing empire.[81] As early as in the 5th century, missionary activities from Roman Britain into the Celtic areas (Scotland, Ireland, and Wales) produced competing early traditions of Celtic Christianity, that was later reintegrated under the Church in Rome. Prominent missionaries in Northwestern Europe of the time were the Christian saints Patrick, Columba, and Columbanus. The Anglo-Saxon tribes that invaded Southern Britain some time after the Roman abandonment were initially Pagans but were converted to Christianity by Augustine of Canterbury on the mission of Pope Gregory the Great. Soon becoming a missionary centre, missionaries such as Wilfrid, Willibrord, Lullus, and Boniface converted their Saxon relatives in Germania.
The largely Christian Gallo-Roman inhabitants of Gaul (modern France and Belgium) were overrun by the Franks in the early 5th century. The native inhabitants were persecuted until the Frankish King Clovis I converted from Paganism to Roman Catholicism in 496. Clovis insisted that his fellow nobles follow suit, strengthening his newly established kingdom by uniting the faith of the rulers with that of the ruled.[82] After the rise of the Frankish Kingdom and the stabilizing political conditions, the Western part of the Church increased the missionary activities, supported by the Merovingian dynasty as a means to pacify troublesome neighbour peoples. After the foundation of a church in Utrecht by Willibrord, backlashes occurred when the Pagan Frisian King Radbod destroyed many Christian centres between 716 and 719. In 717, the English missionary Boniface was sent to aid Willibrord, re-establishing churches in Frisia and continuing missions in Germany.[82] During the late 8th century, Charlemagne used mass killings in order to subjugate the Pagan Saxons and forcibly compel them to accept Christianity.[83]
Since they are considered "People of the Book" in the Islamic religion, Christians under Muslim rule were subjected to the status of dhimmi (along with Jews, Samaritans, Gnostics, Mandeans, and Zoroastrians), which was inferior to the status of Muslims.[84][85][86] Christians and other religious minorities thus faced religious discrimination and persecution in that they were banned from proselytising (for Christians, it was forbidden to evangelize or spread Christianity) in the lands invaded by the Arab Muslims on pain of death, they were banned from bearing arms, undertaking certain professions, and were obligated to dress differently in order to distinguish themselves from Arabs.[85] Under the Islamic law (sharīʿa), Non-Muslims were obligated to pay the jizya and kharaj taxes,[84][85][86] together with periodic heavy ransom levied upon Christian communities by Muslim rulers in order to fund military campaigns, all of which contributed a significant proportion of income to the Islamic states while conversely reducing many Christians to poverty, and these financial and social hardships forced many Christians to convert to Islam.[85] Christians unable to pay these taxes were forced to surrender their children to the Muslim rulers as payment who would sell them as slaves to Muslim households where they were forced to convert to Islam.[85]
According to the tradition of the Syriac Orthodox Church, the Muslim conquest of the Levant was a relief for Christians oppressed by the Western Roman Empire.[86] Michael the Syrian, patriarch of Antioch, wrote later that the Christian God had "raised from the south the children of Ishmael to deliver us by them from the hands of the Romans".[86] Various Christian communities in the regions of Palestine, Syria, Lebanon, and Armenia resented either towards the governance of the Western Roman Empire or that of the Byzantine Empire, and therefore preferred to live under more favourable economic and political conditions as dhimmi under the Muslim rulers.[86] However, modern historians also recognize that the Christian populations living in the lands invaded by the Arab Muslim armies between the 7th and 10th centuries AD suffered religious persecution, religious violence, and martyrdom multiple times at the hands of Arab Muslim officials and rulers;[86][87][88][89] many were executed under the Islamic death penalty for defending their Christian faith through dramatic acts of resistance such as refusing to convert to Islam, repudiation of the Islamic religion and subsequent reconversion to Christianity, and blasphemy towards Muslim beliefs.[87][88][89]
According to the Ḥanafī school of Islamic law (sharīʿa), the testimony of a Non-Muslim (such as a Christian or a Jew) was not considered valid against the testimony of a Muslim in legal or civil matters. Historically, in Islamic culture and traditional Islamic law Muslim women have been forbidden from marrying Christian or Jewish men, whereas Muslim men have been permitted to marry Christian or Jewish women[90][91] (see: Interfaith marriage in Islam). Christians under Islamic rule had the right to convert to Islam or any other religion, while conversely a murtad, or an apostate from Islam, faced severe penalties or even hadd, which could include the Islamic death penalty.[87][88][89]
In general, Christians subject to Islamic rule were allowed to practice their religion with some notable limitations stemming from the apocryphal Pact of Umar. This treaty, supposedly enacted in 717 AD, forbade Christians from publicly displaying the cross on church buildings, from summoning congregants to prayer with a bell, from re-building or repairing churches and monasteries after they had been destroyed or damaged, and imposed other restrictions relating to occupations, clothing, and weapons.[92] The Umayyad Caliphate persecuted many Berber Christians in the 7th and 8th centuries AD, who slowly converted to Islam.[93]
In Umayyad al-Andalus (the Iberian Peninsula), the Mālikī school of Islamic law was the most prevalent.[88] The martyrdoms of forty-eight Christian martyrs that took place in the Emirate of Córdoba between 850 and 859 AD[94] are recorded in the hagiographical treatise written by the Iberian Christian and Latinist scholar Eulogius of Córdoba.[87][88][89] The Martyrs of Córdoba were executed under the rule of Abd al-Rahman II and Muhammad I, and Eulogius' hagiography describes in detail the executions of the martyrs for capital violations of Islamic law, including apostasy and blasphemy.[87][88][89]
Eastern Christian scientists and scholars of the medieval Islamic world (particularly Jacobite and Nestorian Christians) contributed to the Arab Islamic civilization during the reign of the Umayyad and the Abbasid, by translating works of Greek philosophers to Syriac and afterwards, to Arabic.[95][96][97] They also excelled in philosophy, science, theology, and medicine.[98][99][100] And the personal physicians of the Abbasid Caliphs were often Assyrian Christians such as the long serving Bukhtishu dynast
The Abbasid Caliphate was less tolerant of Christianity than had been the Umayyad caliphs.[86] Nonetheless, Christian officials continued to be employed in the government, and the Christians of the Church of the East were often tasked with the translation of Ancient Greek philosophy and Greek mathematics.[86] The writings of al-Jahiz attacked Christians for being too prosperous, and indicates they were able to ignore even those restrictions placed on them by the state.[86] In the late 9th century, the patriarch of Jerusalem, Theodosius, wrote to his colleague the patriarch of Constantinople Ignatios that "they are just and do us no wrong nor show us any violence".[86]
Elias of Heliopolis, having moved to Damascus from Heliopolis (Ba'albek), was accused of apostasy from Christianity after attending a party held by a Muslim Arab, and was forced to flee Damascus for his hometown, returning eight years later, where he was recognized and imprisoned by the "eparch", probably the jurist al-Layth ibn Sa'd.[101]: 34  After refusing to convert to Islam under torture, he was brought before the Damascene emir and relative of the caliph al-Mahdi (r. 775–785), Muhammad ibn-Ibrahim, who promised good treatment if Elias would convert.[101]: 34  On his repeated refusal, Elias was tortured and beheaded and his body burnt, cut up, and thrown into the river Chrysorrhoes (the Barada) in 779 AD.[101]: 34 
According to the Synaxarion of Constantinople, the hegumenos Michael of Zobe and thirty-six of his monks at the Monastery of Zobe near Sebasteia (Sivas) were killed by a raid on the community.[101]: 70  The perpetrator was the "emir of the Hagarenes", "Alim", probably Ali ibn-Sulayman, an Abbasid governor who raided Roman territory in 785 AD.[101]: 70  Bacchus the Younger was beheaded in Jerusalem in 786–787 AD. Bacchus was Palestinian, whose family, having been Christian, had been converted to Islam by their father.[101]: 29–30  Bacchus however, remained crypto-Christian and undertook a pilgrimage to Jerusalem, upon which he was baptized and entered the monastery of Mar Saba.[101]: 29–30  Reunion with his family prompted their reconversion to Christianity and Bacchus's trial and execution for apostasy under the governing emir Harthama ibn A'yan.[101]: 29–30 
After the 838 Sack of Amorium, the hometown of the emperor Theophilos (r. 829–842) and his Amorian dynasty, the caliph al-Mu'tasim (r. 833–842) took more than forty Roman prisoners.[101]: 41–42  These were taken to the capital, Samarra, where after seven years of theological debates and repeated refusals to convert to Islam, they were put to death in March 845 under the caliph al-Wathiq (r. 842–847).[101]: 41–42  Within a generation they were venerated as the 42 Martyrs of Amorium. According to their hagiographer Euodius, probably writing within a generation of the events, the defeat at Amorium was to be blamed on Theophilos and his iconoclasm.[101]: 41–42  According to some later hagiographies, including one by one of several Middle Byzantine writers known as Michael the Synkellos, among the forty-two were Kallistos, the doux of the Koloneian thema, and the heroic martyr Theodore Karteros.[101]: 41–42 
During the 10th-century phase of the Arab–Byzantine wars, the victories of the Romans over the Arabs resulted in mob attacks on Christians, who were believed to sympathize with the Roman state.[86] According to Bar Hebraeus, the catholicus of the Church of the East, Abraham III (r. 906–937), wrote to the grand vizier that "we Nestorians are the friends of the Arabs and pray for their victories".[86] The attitude of the Nestorians "who have no other king but the Arabs", he contrasted with the Greek Orthodox Church, whose emperors he said "had never cease to make war against the Arabs.[86] Between 923 and 924 AD, several Orthodox churches were destroyed in mob violence in Ramla, Ashkelon, Caesarea Maritima, and Damascus.[86] In each instance, according to the Arab Melkite Christian chronicler Eutychius of Alexandria, the caliph al-Muqtadir (r. 908–932) contributed to the rebuilding of ecclesiastical property.[86]
Following a series of heavy military reverses against the Muslims, Iconoclasm emerged within the provinces of the Byzantine Empire in the early 8th century. In the years 720s, the Byzantine Emperor Leo III the Isaurian banned the pictorial representation of Christ, saints, and biblical scenes. In the Latin West, Pope Gregory III held two synods at Rome and condemned Leo's actions. The Byzantine Iconoclast Council, held at Hieria in 754 AD, ruled that holy portraits were heretical.[102] The iconoclastic movement destroyed much of the Christian Church's early artistic history. The iconoclastic movement was later defined as heretical in 787 AD under the Second Council of Nicaea (the seventh ecumenical council) but had a brief resurgence between 815 and 842 AD.
The Carolingian Renaissance was a period of intellectual and cultural revival of literature, arts, and scriptural studies during the late 8th and 9th centuries under the rule of the Carolingian dynasty, mostly during the reigns of the Frankish kings Charlemagne, founder and first Emperor of the Carolingian Empire, and his son, Louis the Pious. To address the problems of illiteracy among clergy and court scribes, Charlemagne founded schools and attracted the most learned men from all of Europe to his court.
Tensions in Christian unity started to become evident in the 4th century. Two basic problems were involved: the nature of the primacy of the bishop of Rome and the theological implications of adding a clause to the Nicene Creed, known as the filioque clause. These doctrinal issues were first openly discussed in Photius's patriarchate. The Eastern churches viewed Rome's understanding of the nature of episcopal power as being in direct opposition to the Church's essentially conciliar structure and thus saw the two ecclesiologies as mutually antithetical.[103]
Another issue developed into a major irritant to Eastern Christendom, the gradual introduction into the Nicene Creed in the West of the Filioque clause – meaning "and the Son" – as in "the Holy Spirit ... proceeds from the Father and the Son", where the original Creed, sanctioned by the councils and still used today by the Eastern Orthodox, simply states "the Holy Spirit, ... proceeds from the Father." The Eastern Church argued that the phrase had been added unilaterally and therefore illegitimately, since the East had never been consulted.[104] In addition to this ecclesiological issue, the Eastern Church also considered the Filioque clause unacceptable on dogmatic grounds.[105]
In the 9th century, a controversy arose between Eastern (Byzantine, Greek Orthodox) and Western (Latin, Roman Catholic) Christianity that was precipitated by the opposition of the Roman Pope John VII to the appointment by the Byzantine Emperor Michael III of Photios I to the position of patriarch of Constantinople. Photios was refused an apology by the pope for previous points of dispute between the East and West. Photios refused to accept the supremacy of the pope in Eastern matters or accept the Filioque clause. The Latin delegation at the council of his consecration pressed him to accept the clause in order to secure their support. The controversy also involved Eastern and Western ecclesiastical jurisdictional rights in the Bulgarian church. Photios did provide concession on the issue of jurisdictional rights concerning Bulgaria, and the papal legates made do with his return of Bulgaria to Rome. This concession, however, was purely nominal, as Bulgaria's return to the Byzantine rite in 870 had already secured for it an autocephalous church. Without the consent of Boris I of Bulgaria, the papacy was unable to enforce any of its claims.
The East–West Schism, also known as the "Great Schism", separated the Church into Western (Latin) and Eastern (Greek) branches, i.e., Western Catholicism and Eastern Orthodoxy. It was the first major division since certain groups in the East rejected the decrees of the Council of Chalcedon (see Oriental Orthodoxy) and was far more significant. Though normally dated to 1054, the East–West Schism was actually the result of an extended period of estrangement between Latin and Greek Christendom over the nature of papal primacy and certain doctrinal matters regarding the Filioque, but intensified from cultural, geographical, geopolitical, and linguistic differences.
From the 6th century onward, most of the monasteries in the Catholic West belonged to the Benedictine Order. Owing to the stricter adherence to a reformed Benedictine rule, the Abbey of Cluny became the acknowledged leading centre of Western monasticism from the later 10th century. Cluny created a large, federated order in which the administrators of subsidiary houses served as deputies of the abbot of Cluny and answered to him. The Cluniac spirit was a revitalising influence on the Norman Church, at its height from the second half of the 10th century through the early 12th century.
The next wave of monastic reform came with the Cistercian movement. The first Cistercian abbey was founded in 1098, at Cîteaux Abbey. The keynote of Cistercian life was a return to a literal observance of the Benedictine rule, rejecting the developments of the Benedictines. The most striking feature in the reform was the return to manual labour, and especially to field-work. Inspired by Bernard of Clairvaux, the primary builder of the Cistercians, they became the main force of technological advancement and diffusion in medieval Europe. By the end of the 12th century, the Cistercian houses numbered 500, and at its height in the 15th century the order claimed to have close to 750 houses. Most of these were built in wilderness areas, and played a major part in bringing such isolated parts of Europe into economic cultivation.
A third level of monastic reform was provided by the establishment of the Mendicant orders. Commonly known as "friars", mendicants live under a monastic rule with traditional vows of poverty, chastity, and obedience but they emphasise preaching, missionary activity, and education, in a secluded monastery. Beginning in the 12th century, the Franciscan Order was instituted by the followers of Francis of Assisi, and thereafter the Dominican Order was begun by St. Dominic.
Modern western universities have their origins directly in the Medieval Church.[106][107][108][109][110] They began as cathedral schools, and all students were considered clerics.[111] This was a benefit as it placed the students under ecclesiastical jurisdiction and thus imparted certain legal immunities and protections. The cathedral schools eventually became partially detached from the cathedrals and formed their own institutions, the earliest being the University of Bologna (1088), the University of Oxford (1096), and the University of Paris (c. 1150).[112][113][114]
The Investiture controversy, otherwise referred to as the "Lay Investiture controversy", was the most significant conflict between secular and religious powers that took place in medieval Europe. It began as a dispute in the 11th century between the Holy Roman Emperor Henry IV and Pope Gregory VII concerning who would appoint bishops (investiture). The end of lay investiture threatened to undercut the power of the Holy Roman Empire and the ambitions of the European nobility. Bishoprics being merely lifetime appointments, a king could better control their powers and revenues than those of hereditary noblemen. Even better, he could leave the post vacant and collect the revenues, theoretically in trust for the new bishop, or give a bishopric to pay a helpful noble. The Roman Catholic Church wanted to end lay investiture to end this and other abuses, to reform the episcopate and provide better pastoral care. Pope Gregory VII issued the Dictatus Papae, which declared that the pope alone could appoint bishops. Henry IV's rejection of the decree led to his excommunication and a ducal revolt. Eventually, Henry IV received absolution after a dramatic public penance, though the Great Saxon Revolt and conflict of investiture continued.
A similar controversy occurred in England between King Henry I and St. Anselm, Archbishop of Canterbury, over investiture and episcopal vacancy. The English dispute was resolved by the Concordat of London (1107), where the king renounced his claim to invest bishops but continued to require an oath of fealty. This was a partial model for the Concordat of Worms (Pactum Calixtinum), which resolved the Imperial investiture controversy with a compromise that allowed secular authorities some measure of control but granted the selection of bishops to their cathedral canons. As a symbol of the compromise, both ecclesiastical and lay authorities invested bishops with the staff and the ring, respectively.
Generally, the Crusades (1095–1291) refer to the European Christian campaigns in the Holy Land sponsored by the Papacy against Muslims in order to reconquer the region of Palestine.[115][116][117] There were other Crusader expeditions against the Islamic forces in the Mediterranean, primarily in Southern Spain, Southern Italy, and the islands of Cyprus, Malta, and Sicily.[116] The Papacy also sponsored numerous Crusades against the Pagan peoples of Northeastern Europe in order to subjugate and forcibly convert them to Christianity,[115] against its political enemies in Western Europe, and against heretical or schismatic religious minorities within European Christendom.[118]
The Holy Land had been part of the Roman Empire, and thus subsequently of the Byzantine Empire, until the Arab Muslim invasions of the 7th and 8th centuries. Thereafter, Christians had generally been permitted to visit the sacred places in the Holy Land until 1071, when the Seljuk Turks closed Christian pilgrimages and assailed the Byzantines, defeating them at the Battle of Manzikert. Emperor Alexius I asked for aid from Pope Urban II against Islamic aggression. He probably expected money from the pope for the hiring of mercenaries. Instead, Urban II called upon the knights of Christendom in a speech made at the Council of Clermont on 27 November 1095, combining the idea of pilgrimage to the Holy Land with that of waging a holy war against infidels.[119]
The First Crusade captured Antioch in 1099 and then Jerusalem. The Second Crusade occurred in 1145 when Edessa was taken by Islamic forces. Jerusalem was held until 1187 and the Third Crusade, after battles between Richard the Lionheart and Saladin. The Fourth Crusade, begun by Innocent III in 1202, intended to retake the Holy Land but was soon subverted by the Venetians. When the crusaders arrived in Constantinople, they sacked the city and other parts of Asia Minor and established the Latin Empire of Constantinople in Greece and Asia Minor. Five numbered crusades to the Holy Land, culminating in the siege of Acre of 1219, essentially ending the Western presence in the Holy Land.[120]
Jerusalem was held by the crusaders for nearly a century, while other strongholds in the Near East remained in Christian possession much longer. The crusades in the Holy Land ultimately failed to establish permanent Christian kingdoms. Islamic expansion into Europe remained a threat for centuries, culminating in the campaigns of Suleiman the Magnificent in the 16th century. Crusades in Iberia (the Reconquista), southern Italy, and Sicily eventually lead to the demise of Islamic power in Europe. The Albigensian Crusade targeted the heretical Cathars of southern France; in combination with the Inquisition set up in its aftermath, it succeeded in exterminating them. The Wendish Crusade succeeded in subjugating and forcibly converting the pagan Slavs of modern eastern Germany. The Livonian Crusade, carried out by the Teutonic Knights and other orders of warrior-monks, similarly conquered and forcibly converted the pagan Balts of Livonia and Old Prussia. However, the pagan Grand Duchy of Lithuania successfully resisted the Knights and converted only voluntarily in the 14th century.[121]
The Medieval Inquisition was a series of inquisitions (Roman Catholic ecclesiastical bodies charged with suppressing Christian movements that they regarded as heretical) from around 1184, including the Episcopal Inquisition (1184–1230) and later the Papal Inquisition (1230s–1240s). It was established in response to the Christian movements within Europe considered apostate or heretical to Western Catholicism, in particular the Bogomils,[122] Cathars (or Albigensians),[123] Waldensians,[124] Beguines and Beghards,[125] Lollards,[126] Hussites,[127] and European Jews,[128] which were respectively disseminated in the Bulgarian Empire,[122] Southern France,[123] Northern Italy,[124] the Flanders and Rhineland,[125] England,[126] the Lands of the Bohemian Crown,[127] and the territories united under the Crown of Aragon.[128] These were the first inquisition movements of many that would follow in European Christendom.
Early evangelization of Scandinavia was carried out by the Christianized Anglo-Saxons throughout their missions in the Scandinavian Peninsula; the most notable of the Anglo-Saxon missionaries was Ansgar, Archbishop of Bremen, nicknamed "Apostle of the North".[129] Ansgar, a native of Amiens, was sent with a group of monks to Jutland, Denmark around the year 820, at the time of the pro-Christian King Harald Klak.[129] The mission was only partially successful, and Ansgar returned two years later to Germany, after Harald had been driven out of his kingdom. In 829, Ansgar went to Birka on Lake Mälaren, Sweden, with his aide friar Witmar, and a small congregation was formed in 831 which included the king's steward Hergeir. Conversion was slow, however, and most Scandinavian lands were only completely Christianised at the time of rulers such as Saint Canute IV of Denmark and Olaf I of Norway in the years following AD 1000.
The Christianization of the Slavs was initiated by one of Byzantium's most learned churchmen—the patriarch Photios I of Constantinople. The Byzantine Emperor Michael III chose Cyril and Methodius in response to a request from King Rastislav of Moravia, who wanted missionaries that could minister to the Moravians in their own language. The two brothers spoke the local Slavonic vernacular and translated the Bible and many of the prayer books.[130] As the translations prepared by them were copied by speakers of other dialects, the hybrid literary language Old Church Slavonic was created, which later evolved into Church Slavonic and is the common liturgical language still used by the Russian Orthodox Church and other Slavic Orthodox Christians. Methodius went on to convert the Serbs.[131]
Bulgaria was a Pagan country since its establishment in 681 until 864, when Boris I converted to Christianity. The reasons for that decision were complex; the most important factors were that Bulgaria was situated between two powerful Christian empires, Byzantium and East Francia; Christian doctrine particularly favoured the position of the monarch as God's representative on Earth, while Boris also saw it as a way to overcome the differences between Bulgars and Slavs.[132][133] Bulgaria was officially recognized as a patriarchate by Constantinople in 927, Serbia in 1346, and Russia in 1589. All of these nations had been converted long before these dates.
The Avignon Papacy, sometimes referred to as the Babylonian Captivity, was a period from 1309 to 1378 during which seven popes resided in Avignon, in modern-day France.[134] In 1309, Pope Clement V moved to Avignon in southern France. Confusion and political animosity waxed, as the prestige and influence of Rome waned without a resident pontiff. Troubles reached their peak in 1378 when Gregory XI died while visiting Rome. A papal conclave met in Rome and elected Urban VI, an Italian. Urban soon alienated the French cardinals, and they held a second conclave electing Robert of Geneva to succeed Gregory XI, beginning the Western Schism.
John Wycliffe, an English scholastic philosopher and Christian theologian best known for denouncing the abuses and corruption of the Catholic Church, was a precursor of the Protestant Reformation.[135] He emphasized the supremacy of the Bible and called for a direct relationship between God and the human person, without interference by priests and bishops.[135] The Lollards, a Proto-Protestant Christian movement that followed the teachings of Wycliffe, played a role in the English Reformation.[135][136][137] Jan Hus, a Czech Christian theologian based in Prague, was influenced by Wycliffe and spoke out against the abuses and corruption he saw in the Catholic Church.[127] His followers became known as the Hussites, a Proto-Protestant Christian movement that followed the teachings of Jan Hus, who became the best known representative of the Bohemian Reformation.[127][135] He was a forerunner of the Protestant Reformation,[127][135] and his legacy has become a powerful symbol of Czech culture in Bohemia.[138] Both Wycliffe and Hus were accused of heresy and subsequently condemned to the death penalty for their outspoken views about the Catholic Church.[126][127][135]
The Renaissance was a period of great cultural change and achievement, marked in Italy by a classical orientation and an increase of wealth through mercantile trade. The city of Rome, the papacy, and the papal states were all affected by the Renaissance. On the one hand, it was a time of great artistic patronage and architectural magnificence, where the Church commissioned such artists as Michelangelo, Brunelleschi, Bramante, Raphael, Fra Angelico, Donatello, and Leonardo da Vinci. On the other hand, wealthy Italian families often secured episcopal offices, including the papacy, for their own members, some of whom were known for immorality, such as Alexander VI and Sixtus IV.
In addition to being the head of the Church, the pope became one of Italy's most important secular rulers, and pontiffs such as Julius II often waged campaigns to protect and expand their temporal domains. Furthermore, the popes, in a spirit of refined competition with other Italian lords, spent lavishly both on private luxuries but also on public works, repairing or building churches, bridges, and a magnificent system of aqueducts in Rome that still function today.
In 1453, Constantinople fell to the Ottoman Empire. Eastern Christians fleeing Constantinople, and the Greek manuscripts they carried with them, is one of the factors that prompted the literary renaissance in the West at about this time. The Ottoman government followed Islamic law when dealing with the conquered Christian population. Christians were officially tolerated as people of the Book. As such, the Church's canonical and hierarchical organisation were not significantly disrupted, and its administration continued to function. One of the first things that Mehmet the Conqueror did was to allow the Church to elect a new patriarch, Gennadius Scholarius. However, these rights and privileges, including freedom of worship and religious organisation, were often established in principle but seldom corresponded to reality. Christians were viewed as second-class citizens, and the legal protections they depended upon were subject to the whims of the sultan and the sublime porte.[139][140] The Hagia Sophia and the Parthenon, which had been Christian churches for nearly a millennium, were converted into mosques. Violent persecutions of Christians were common and reached their climax in the Armenian, Assyrian, and Greek genocides.
Beginning with the first wave of European colonization, the religious discrimination, persecution, and violence toward the Indigenous peoples' native religions was systematically perpetrated by the European Christian colonists and settlers from the 15th-16th centuries onwards.[11][12][13][14][141][142]
During the Age of Discovery and the following centuries, the Spanish and Portuguese colonial empires were the most active in attempting to convert the Indigenous peoples of the Americas to the Christian religion.[11][12] Pope Alexander VI issued the Inter caetera bull in May 1493 that confirmed the lands claimed by the Kingdom of Spain, and mandated in exchange that the Indigenous peoples be converted to Catholic Christianity. During Columbus's second voyage, Benedictine friars accompanied him, along with twelve other priests. With the Spanish conquest of the Aztec empire, evangelization of the dense Indigenous populations was undertaken in what was called the "spiritual conquest."[143] Several mendicant orders were involved in the early campaign to convert the Indigenous peoples. Franciscans and Dominicans learned Indigenous languages, such as Nahuatl, Mixtec, and Zapotec.[144] One of the first schools for Indigenous peoples in Mexico was founded by Pedro de Gante in 1523. The friars aimed at converting Indigenous leaders, with the hope and expectation that their communities would follow suit.[145] In densely populated regions, friars mobilized Indigenous communities to build churches, making the religious change visible; these churches and chapels were often in the same places as old temples, often using the same stones. "Native peoples exhibited a range of responses, from outright hostility to active embrace of the new religion."[146] In central and southern Mexico where there was an existing Indigenous tradition of creating written texts, the friars taught Indigenous scribes to write their own languages in Latin letters. There is significant body of texts in Indigenous languages created by and for Indigenous peoples in their own communities for their own purposes. In frontier areas where there were no settled Indigenous populations, friars and Jesuits often created missions, bringing together dispersed Indigenous populations in communities supervised by the friars in order to more easily preach the gospel and ensure their adherence to the faith. These missions were established throughout the Spanish colonies which extended from the southwestern portions of current-day United States through Mexico and to Argentina and Chile.
As slavery was prohibited between Christians and could only be imposed upon non-Christian prisoners of war and/or men already sold as slaves, the debate on Christianization was particularly acute during the early 16th century, when Spanish conquerors and settlers sought to mobilize Indigenous labor. Later, two Dominican friars, Bartolomé de Las Casas and the philosopher Juan Ginés de Sepúlveda, held the Valladolid debate, with the former arguing that Native Americans were endowed with souls like all other human beings, while the latter argued to the contrary to justify their enslavement. In 1537, the papal bull Sublimis Deus definitively recognized that Native Americans possessed souls, thus prohibiting their enslavement, without putting an end to the debate. Some claimed that a native who had rebelled and then been captured could be enslaved nonetheless.
When the first Franciscans arrived in Mexico in 1524, they burned the sacred places dedicated to the Indigenous peoples' native religions.[147] However, in pre-Columbian Mesoamerica, burning the temple of a conquered group was standard practice, shown in Indigenous manuscripts, such as Codex Mendoza. Conquered Indigenous groups expected to take on the gods of their new overlords, adding them to the existing pantheon. They likely were unaware that their conversion to Christianity entailed the complete and irrevocable renunciation of their ancestral religious beliefs and practices. In 1539, Mexican bishop Juan de Zumárraga oversaw the trial and execution of the Indigenous nobleman Carlos of Texcoco for apostasy from Christianity.[148] Following that, the Catholic Church removed Indigenous converts from the jurisdiction of the Inquisition, since it had a chilling effect on evangelization. In creating a protected group of Christians, Indigenous men no longer could aspire to be ordained Christian priests.[149]
Throughout the Americas, the Jesuits were active in attempting to convert the Indigenous peoples to Christianity. They had considerable success on the frontiers in New France,[150] Portuguese Brazil, and Antonio de Vieira, S.J;[151] and in Paraguay, almost an autonomous state within a state.[152]
In the early 16th century, attempts were made by the Christian theologians Martin Luther and Huldrych Zwingli, along with many others, to reform the Catholic Church. They considered the roots of corruption within the Catholic Church and its ecclesiastical structure to be doctrinal, rather than simply a matter of depravity, moral weakness, or lack of ecclesiastical discipline, and thus advocated for God's autonomy in redemption, and against voluntaristic notions that salvation could be earned by people. The Reformation is usually considered to have started with the publication of the Ninety-five Theses by Luther in 1517, although there was no schism until the 1521 Diet of Worms. The edicts of the Diet condemned Luther and officially banned citizens of the Holy Roman Empire from defending or propagating his ideas.[153]
The word Protestant is derived from the Latin protestatio, meaning declaration, which refers to the letter of protestation by Lutheran princes against the decision of the Diet of Speyer in 1529, which reaffirmed the edict of the Diet of Worms ordering the seizure of all property owned by persons guilty of advocating Lutheranism.[154] The term "Protestant" was not originally used by Reformation era leaders; instead, they called themselves "evangelical", emphasising the "return to the true gospel (Greek: euangelion)."[155]
Early protest was against corruptions such as simony, the holding of multiple church offices by one person at the same time, episcopal vacancies, and the sale of indulgences. The Protestant position also included the Five solae (sola scriptura, sola fide, sola gratia, solus Christus, soli Deo gloria), the priesthood of all believers, Law and Gospel, and the two kingdoms doctrine. The three most important traditions to emerge directly from the Reformation were the Lutheran, Reformed, and Anglican traditions, though the latter group identifies as both "Reformed" and "Catholic", and some subgroups reject the classification as "Protestant".
Unlike other reform movements, the English Reformation began by royal influence. Henry VIII considered himself a thoroughly Catholic king, and in 1521 he defended the papacy against Luther in a book he commissioned entitled, The Defence of the Seven Sacraments, for which Pope Leo X awarded him the title Fidei Defensor (Defender of the Faith). However, the king came into conflict with the papacy when he wished to annul his marriage with Catherine of Aragon, for which he needed papal sanction. Catherine, among many other noble relations, was the aunt of Emperor Charles V, the papacy's most significant secular supporter. The ensuing dispute eventually lead to a break from Rome and the declaration of the King of England as head of the English Church, which saw itself as a Protestant church navigating a middle way between Lutheranism and Reformed Christianity, but leaning more towards the latter.[156] Consequently, England experienced periods of reform and also Counter-Reformation. Monarchs such as Edward VI, Lady Jane Grey, Mary I, Elizabeth I, and Archbishops of Canterbury such as Thomas Cranmer and William Laud, pushed the Church of England in different directions over the course of only a few generations. What emerged was the Elizabethan Religious Settlement and a state church that considered itself both "Reformed" and "Catholic" but not "Roman", and other unofficial more radical movements, such as the Puritans. In terms of politics, the English Reformation included heresy trials, the exiling of Roman Catholic populations to Spain and other Roman Catholic lands, and censorship and prohibition of books.[157]
The Radical Reformation represented a response to corruption both in the Catholic Church and in the expanding Magisterial Protestant movement led by Martin Luther and many others. Beginning in Germany and Switzerland in the 16th century, the Radical Reformation gave birth to many radical Protestant groups throughout Europe. The term covers radical reformers like Thomas Müntzer and Andreas Karlstadt, the Zwickau prophets, and Anabaptist Christians, most notably the Amish, Mennonites, Hutterites, the Bruderhof Communities, and Schwarzenau Brethren.
The Counter-Reformation was the response of the Catholic Church to the Protestant Reformation. In terms of meetings and documents, it consisted of the Confutatio Augustana, the Council of Trent, the Roman Catechism, and the Defensio Tridentinæ fidei. In terms of politics, the Counter-Reformation included heresy trials, the exiling of Protestant populations from Catholic lands, the seizure of children from their Protestant parents for institutionalized Catholic upbringing, a series of wars, the Index Librorum Prohibitorum (the list of prohibited books), and the Spanish Inquisition.
Although Protestant Christians were excommunicated in an attempt to reduce their influence within the Catholic Church, at the same time they were persecuted during the Counter-Reformation, prompting some to live as crypto-Protestants (also termed Nicodemites), against the urging of John Calvin who urged them to live their faith openly.[158] Crypto-Protestants were documented as late as the 19th century in Latin America.[159]
The Council of Trent (1545–1563) initiated by Pope Paul III addressed issues of certain ecclesiastical corruptions such as simony, absenteeism, nepotism, the holding of multiple church offices by one person, and other abuses. It also reasserted traditional practices and doctrines of the Church, such as the episcopal structure, clerical celibacy, the seven Sacraments, transubstantiation (the belief that during mass the consecrated bread and wine truly become the body and blood of Christ), the veneration of relics, icons, and saints (especially the Blessed Virgin Mary), the necessity of both faith and good works for salvation, the existence of purgatory and the issuance (but not the sale) of indulgences. In other words, all Protestant doctrinal objections and changes were uncompromisingly rejected. The council also fostered an interest in education for parish priests to increase pastoral care. Milan's Archbishop Saint Charles Borromeo set an example by visiting the remotest parishes and instilling high standards.
Simultaneous to the Counter-Reformation, the Catholic Reformation consisted of improvements in art and culture, anti-corruption measures, the founding of the Jesuits, the establishment of seminaries, a reassertion of traditional doctrines and the emergence of new religious orders aimed at both moral reform and new missionary activity. Also part of this was the development of new yet orthodox forms of spirituality, such as that of the Spanish mystics and the French school of spirituality.
The papacy of St. Pius V was known not only for its focus on halting heresy and worldly abuses within the Church, but also for its focus on improving popular piety in a determined effort to stem the appeal of Protestantism. Pius began his pontificate by giving large alms to the poor, charity, and hospitals, and the pontiff was known for consoling the poor and sick as well as supporting missionaries. These activities coincided with a rediscovery of the ancient Christian catacombs in Rome. As Diarmaid MacCulloch states, "Just as these ancient martyrs were revealed once more, Catholics were beginning to be martyred afresh, both in mission fields overseas and in the struggle to win back Protestant northern Europe: the catacombs proved to be an inspiration for many to action and to heroism."[160]
Catholic missions were carried to new places beginning with the new Age of Discovery, and the Roman Catholic Church established missions in the Americas.
The Galileo affair, in which Galileo Galilei came into conflict with the Roman Catholic Church over his support of heliocentrism, is often considered a defining moment in the history of the relationship between religion and science. In 1610, Galileo published his Sidereus Nuncius (Starry Messenger), describing the surprising observations that he had made with the new telescope. These and other discoveries exposed major difficulties with the understanding of the heavens that had been held since antiquity, and raised new interest in radical teachings such as the heliocentric theory of Copernicus. In reaction, many scholars maintained that the motion of the earth and immobility of the sun were heretical, as they contradicted some accounts given in the Bible as understood at that time. Galileo's part in the controversies over his theological and philosophical positions culminated in his trial and sentencing in 1633, on a grave suspicion of heresy.
The colonization with the most impact in the New World was that of Protestant English Puritans in North America. Unlike the Spanish or French, the English colonists made surprisingly little effort to evangelize the native peoples.[161] The Puritans, or Pilgrims, left England so that they could live in an area with Puritanism established as the exclusive civic religion. Though they had left England because of the suppression of their religious practice, most Puritans had thereafter originally settled in the Low Countries but found the licentiousness there, where the state hesitated from enforcing religious practice, as unacceptable, and thus they set out for the New World and the hopes of a Puritan utopia.
Christian revivalism refers to the Calvinist and Wesleyan revival, called the "Great Awakening" in North America, which saw the development of evangelical Congregationalist, Presbyterian, Baptist, and new Methodist churches.
The First Great Awakening was a wave of religious enthusiasm among Protestants in the American colonies c. 1730–1740, emphasising the traditional Reformed virtues of Godly preaching, rudimentary liturgy, and a deep sense of personal guilt and redemption by Christ Jesus. Historian Sydney E. Ahlstrom saw it as part of a "great international Protestant upheaval" that also created pietism in Germany, the Evangelical Revival, and Methodism in England.[162] It centred on reviving the spirituality of established congregations and mostly affected Congregational, Presbyterian, Dutch Reformed, German Reformed, Baptist, and Methodist churches, while also spreading within the slave population. The Second Great Awakening (1800–1830s), unlike the first, focused on the unchurched and sought to instill in them a deep sense of personal salvation as experienced in revival meetings. It also sparked the beginnings of groups such as the Mormons, the Restoration Movement and the Holiness movement. The Third Great Awakening began from 1857 and was most notable for taking the movement throughout the world, especially in English speaking countries. The final group to emerge from the "great awakenings" in North America was Pentecostalism, which had its roots in the Methodist, Wesleyan, and Holiness movements, and began in 1906 on Azusa Street in Los Angeles. Pentecostalism would later lead to the Charismatic movement.
Restorationism refers to the belief that a purer form of Christianity should be restored using the early church as a model.[163]: 635 [164]: 217  In many cases, restorationist groups believed that contemporary Christianity, in all its forms, had deviated from the true, original Christianity, which they then attempted to "reconstruct", often using the Book of Acts as a "guidebook" of sorts. Restorationists do not usually describe themselves as "reforming" a Christian church continuously existing from the time of Jesus, but as restoring the Church that they believe was lost at some point. "Restorationism" is often used to describe the Stone-Campbell Restoration Movement.
The term "restorationist" is also used to describe the Jehovah's Witness movement, founded in the late 1870s by Charles Taze Russell. The term can also be used to describe the Latter Day Saint movement, including The Church of Jesus Christ of Latter-day Saints (LDS Church), the Community of Christ and numerous other Latter Day Saints sects. Latter Day Saints, also known as Mormons, believe that Joseph Smith was chosen to restore the original organization established by Jesus, now "in its fullness", rather than to reform the church.[165][166]
The Russian Orthodox Church held a privileged position in the Russian Empire, expressed in the motto of the late empire from 1833: Orthodoxy, Autocracy, and Populism. Nevertheless, the Church reform of Peter I in the early 18th century had placed the Orthodox authorities under the control of the tsar. An ober-procurator appointed by the tsar ran the committee which governed the Church between 1721 and 1918: the Most Holy Synod.
The Church became involved in the various campaigns of russification,[167] and was accused of involvement in Russian anti-semitism,[168] despite the lack of an official position on Judaism as such.[169]
The Bolsheviks and other Russian revolutionaries saw the Church, like the tsarist state, as an enemy of the people. Criticism of atheism was strictly forbidden and sometimes lead to imprisonment.[170][171][172] Some actions against Orthodox priests and believers included torture, being sent to prison camps, labour camps or mental hospitals, as well as execution.[173][174]
In the first five years after the Bolshevik revolution, 28 bishops and 1,200 priests were executed.[176] This included people like the Grand Duchess Elizabeth Fyodorovna who was at this point a monastic. Executed along with her were: Grand Duke Sergei Mikhailovich Romanov; the Princes Ioann Konstantinvich, Konstantin Konstantinovich, Igor Konstantinovich and Vladimir Pavlovich Paley; Grand Duke Sergei's secretary, Fyodor Remez; and Varvara Yakovleva, a sister from the Grand Duchess Elizabeth's convent.
Liberal Christianity, sometimes called liberal theology, is an umbrella term covering diverse, philosophically informed religious movements and moods within late 18th, 19th and 20th-century Christianity. The word "liberal" in liberal Christianity does not refer to a leftist political agenda or set of beliefs, but rather to the freedom of dialectic process associated with continental philosophy and other philosophical and religious paradigms developed during the Age of Enlightenment.
Fundamentalist Christianity is a movement that arose mainly within British and American Protestantism in the late 19th century and early 20th century in reaction to modernism and certain liberal Protestant groups that denied doctrines considered fundamental to Christianity yet still called themselves "Christian." Thus, fundamentalism sought to re-establish tenets that could not be denied without relinquishing a Christian identity, the "fundamentals": inerrancy of the Bible, the principle of sola scriptura, the Virgin Birth of Jesus, the doctrine of substitutionary atonement, the bodily resurrection of Jesus, and the imminent return of Jesus Christ.
Under the state atheism of countries in the Soviet Union and the Eastern Bloc, Christians of many denominations experienced persecution, with many churches and monasteries being destroyed, as well as clergy being executed.[177][178][179]
The position of Christians affected by Nazism is highly complex.[180] Pope Pius XI declared – Mit brennender Sorge – that Fascist governments had hidden "pagan intentions" and expressed the irreconcilability of the Catholic position and totalitarian fascist state worship, which placed the nation above God, fundamental human rights, and dignity. His declaration that "Spiritually, [Christians] are all Semites" prompted the Nazis to give him the title "Chief Rabbi of the Christian World."[181]
Catholic priests were executed in concentration camps alongside Jews; for example, 2,600 Catholic priests were imprisoned in Dachau, and 2,000 of them were executed (cf. Priesterblock). A further 2,700 Polish priests were executed (a quarter of all Polish priests), and 5,350 Polish nuns were either displaced, imprisoned, or executed.[182] Many Catholic laymen and clergy played notable roles in sheltering Jews during the Holocaust, including Pope Pius XII. The head rabbi of Rome became a Catholic in 1945 and, in honour of the actions the pope undertook to save Jewish lives, he took the name Eugenio (the pope's first name).[183] A former Israeli consul in Italy claimed: "The Catholic Church saved more Jewish lives during the war than all the other churches, religious institutions, and rescue organisations put together."[184]
The relationship between Nazism and Protestantism, especially the German Lutheran Church, was complex. Though many[185] Protestant church leaders in Germany supported the Nazis' growing anti-Jewish activities, some such as Dietrich Bonhoeffer (a Lutheran pastor) of the Confessing Church, a movement within Protestantism that strongly opposed Nazism, were strongly opposed to the Third Reich. Bonhoeffer was later found guilty in the conspiracy to assassinate Hitler and executed.
On 11 October 1962, Pope John XXIII opened the Second Vatican Council, the 21st ecumenical council of the Catholic Church. The council was "pastoral" in nature, interpreting dogma in terms of its scriptural roots, revising liturgical practices, and providing guidance for articulating traditional Church teachings in contemporary times. The council is perhaps best known for its instructions that the Mass may be celebrated in the vernacular as well as in Latin.
Ecumenism broadly refers to movements between Christian groups to establish a degree of unity through dialogue. Ecumenism is derived from Greek οἰκουμένη (oikoumene), which means "the inhabited world", but more figuratively something like "universal oneness." The movement can be distinguished into Catholic and Protestant movements, with the latter characterised by a redefined ecclesiology of "denominationalism" (which the Catholic Church, among others, rejects).
Over the last century, moves have been made to reconcile the schism between the Catholic Church and the Eastern Orthodox churches. Although progress has been made, concerns over papal primacy and the independence of the smaller Orthodox churches has blocked a final resolution of the schism. On 30 November 1894, Pope Leo XIII published Orientalium Dignitas. On 7 December 1965, a Joint Catholic-Orthodox Declaration of Pope Paul VI and the Ecumenical Patriarch Athenagoras I was issued lifting the mutual excommunications of 1054.
Some of the most difficult questions in relations with the ancient Eastern Churches concern some doctrine (i.e. Filioque, scholasticism, functional purposes of asceticism, the essence of God, Hesychasm, Fourth Crusade, establishment of the Latin Empire, Uniatism to note but a few) as well as practical matters such as the concrete exercise of the claim to papal primacy and how to ensure that ecclesiastical union would not mean mere absorption of the smaller Churches by the Latin component of the much larger Catholic Church (the most numerous single religious denomination in the world) and the stifling or abandonment of their own rich theological, liturgical and cultural heritage.
With respect to Catholic relations with Protestant communities, certain commissions were established to foster dialogue and documents have been produced aimed at identifying points of doctrinal unity, such as the Joint Declaration on the Doctrine of Justification produced with the Lutheran World Federation in 1999. Ecumenical movements within Protestantism have focused on determining a list of doctrines and practices essential to being Christian and thus extending to all groups which fulfill these basic criteria a (more or less) co-equal status, with perhaps one's own group still retaining a "first among equal" standing. This process involved a redefinition of the idea of "the Church" from traditional theology. This ecclesiology, known as denominationalism, contends that each group (which fulfills the essential criteria of "being Christian") is a sub-group of a greater "Christian Church", itself a purely abstract concept with no direct representation, i.e., no group, or "denomination", claims to be "the Church." This ecclesiology is at variance with other groups that indeed consider themselves to be "the Church." The "essential criteria" generally consist of belief in the Trinity, belief that Jesus Christ is the only way to bring forgiveness and eternal life, and that Jesus died and rose again bodily.
In reaction to these developments, Christian fundamentalism emerged as a socio-political-religious movement in rejection of what many Christians perceived as radical influences of philosophical humanism that were affecting the Christian religion, according to them. Especially targeting critical approaches to the interpretation of the Bible, and trying to blockade the inroads made into their churches by secular scientific assumptions, fundamentalist Christians began to appear in various Christian denominations as numerous independent movements of resistance to the developments that they regarded as a drift away from historical Christianity. Over time, the Evangelical movement has divided into two main wings, with the label Fundamentalist following one branch, while the term Evangelical has become the preferred banner of the more moderate side. Although both strands of Evangelicalism primarily originated in the English-speaking world, the majority of Evangelicals today live elsewhere in the world.
World Christianity, otherwise known as "global Christianity", has been defined both as a term that attempts to convey the global nature of the Christian religion[18][21][188] and an academic field of study that encompasses analysis of the histories, practices, and discourses of Christianity as a world religion and its various forms as they are found on the six continents.[189] However, the term often focuses on "non-Western Christianity" which "comprises (usually the exotic) instances of Christian faith in 'the global South', in Asia, Africa, and Latin America."[190] It also includes Indigenous or diasporic forms of Christianity in the Caribbean,[187] South America,[187] Western Europe,[191] and North America.[191]
Printed sources
Web-sources
The following links give an overview of the history of Christianity:
The following links provide quantitative data related to Christianity and other major religions, including rates of adherence at different points in time:

The history of slavery spans many cultures, nationalities, and religions from ancient times to the present day. Likewise, its victims have come from many different ethnicities and religious groups.  The social, economic, and legal positions of enslaved people have differed vastly in different systems of slavery in different times and places.[1]
Slavery was relatively rare in pre-civilisation hunter-gatherer populations,[2] as it develops under conditions of social stratification.[3] Slavery operated in the first civilizations (such as Sumer in Mesopotamia,[4] which dates back as far as 3500 BCE). Slavery features in the Mesopotamian Code of Hammurabi (c. 1750 BCE), which refers to it as an established institution.[5]
Slavery was widespread in the ancient world.  It was found in almost every ancient civilization, including the Roman Empire. It became less common throughout Europe during the Early Middle Ages, although it continued to be practised in some areas. Both Christians and Muslims captured and enslaved each other during centuries of warfare in the Mediterranean.[6] Islamic slavery encompassed mainly Western and Central Asia, Northern and Eastern Africa, India, and Europe from the 7th to the 20th century. 
Beginning in the 16th century, European merchants initiated the transatlantic slave trade, purchasing enslaved Africans from West African kingdoms and transporting them to Europe's colonies in the Americas. The transatlantic slave trade was eventually curtailed due to European and American governments passing legislation abolishing their nation's involvement in it.
Although slavery is no longer legal anywhere in the world, human trafficking remains an international problem. An estimated 25-40 million people were enslaved as of 2013[update], the majority of these in Asia.[7] During the 1983–2005 Second Sudanese Civil War, people were taken into slavery.[8] Evidence emerged in the late 1990s of systematic child slavery and trafficking on cacao plantations in West Africa.[9]
Slavery in the 21st century continues and generates $150 billion in annual profits.[10] Populations in regions with armed conflict are especially vulnerable, and modern transportation has made human trafficking easier.[11] In 2019, there were an estimated 40 million people worldwide subject to some form of slavery - 25% were children.[10] Sixty-one percent[nb 1] are used for forced labor, mostly in the private sector. Thirty-eight percent[nb 2] live in forced marriages.[10] Other types of modern slavery are child soldiers, sex trafficking, and sexual slavery.
Evidences of slavery predate written records; the practice has existed in many[12]—if not most—cultures.[13]
Mass slavery requires economic surpluses and a high population density to be viable.[14] Because of this, the practice of slavery would have only proliferated after the invention of agriculture during the Neolithic Revolution, about 11,000 years ago.[15][failed verification]
Slavery occurred in civilizations as old as Sumer, as well as in almost every other ancient civilization, including ancient Egypt, ancient China, the Akkadian Empire, Assyria, Babylonia, Persia, ancient Israel and Judah[16][17][18]ancient Greece, ancient India, the Roman Empire, the Arab Islamic Caliphate and Sultanate, Nubia and the pre-Columbian civilizations of the Americas.[19] Ancient slavery consists of a mixture of debt-slavery, punishment for crime, prisoners of war, child abandonment, and children born to slaves.[20]
C. 1480 BC, fugitive-slave treaty between Idrimi of Alakakh (now Tell Atchana) and Pillia of Kizzuwatna (now Cilicia).
Slaves in chains during the period of Roman rule at Smyrna (present-day İzmir), 200 CE.
13th-century CE slave market in Yemen.[21]
Writing in 1984, French historian Fernand Braudel noted that slavery had been endemic in Africa and part of the structure of everyday life throughout the 15th to the 18th century. "Slavery came in different guises in different societies: there were court slaves, slaves incorporated into princely armies, domestic and household slaves, slaves working on the land, in industry, as couriers and intermediaries, even as traders".[22] During the 16th century, Europe began to outpace the Arab world in the export traffic, with its trafficking of enslaved people from Africa to the Americas.[citation needed] The Dutch imported enslaved people from Asia into their colony at the Cape of Good Hope (now Cape Town) in the 17th century.[citation needed] In 1807 Britain (which already held a small coastal territory, intended for the resettlement of formerly enslaved people, in Freetown, Sierra Leone) made the slave trade within its empire illegal with the Slave Trade Act 1807, and worked to extend the prohibition to other territory,[23]: 42  as did the United States in 1808.[24]
In Senegambia, between 1300 and 1900, close to one-third of the population was enslaved. In early Islamic states of the Western Sudan, including Ghana (750–1076), Mali (1235–1645), Segou (1712–1861), and Songhai (1275–1591), about a third of the population was enslaved. The earliest Akan state of Bonoman which had third of its population being enslaved in the 17th century. In Sierra Leone in the 19th century about half of the population consisted of enslaved people. In the 19th century at least half the population was enslaved among the Duala of the Cameroon, the Igbo and other peoples of the lower Niger, the Kongo, and the Kasanje kingdom and Chokwe of Angola. Among the Ashanti and Yoruba a third of the population consisted of enslaved people as well as Bono.[25] The population of the Kanem was about one third enslaved. It was perhaps 40% in Bornu (1396–1893). Between 1750 and 1900 from one- to two-thirds of the entire population of the Fulani jihad states consisted of enslaved people. The population of the Sokoto caliphate formed by Hausas in northern Nigeria and Cameroon was half-slave in the 19th century. It is estimated that up to 90% of the population of Arab-Swahili Zanzibar was enslaved. Roughly half the population of Madagascar was enslaved.[26][27][page needed][28][29][30]
Slavery in Ethiopia persisted until 1942. The Anti-Slavery Society estimated that there were 2,000,000 enslaved people in the early 1930s, out of an estimated population of between 8 and 16 million.[31] It was finally abolished by order of emperor Haile Selassie on 26 August 1942.[32]
When British rule was first imposed on the Sokoto Caliphate and the surrounding areas in northern Nigeria at the turn of the 20th century, approximately 2 million to 2.5 million people living there were enslaved.[33] Slavery in northern Nigeria was finally outlawed in 1936.[34]
Writing in 1998 about the extent of trade coming through and from Africa, the Congolese journalist Elikia M'bokolo wrote "The African continent was bled of its human resources via all possible routes. Across the Sahara, through the Red Sea, from the Indian Ocean ports and across the Atlantic. At least ten centuries of slavery for the benefit of the Muslim countries (from the ninth to the nineteenth)." He continues: "Four million slaves exported via the Red Sea, another four million through the Swahili ports of the Indian Ocean, perhaps as many as nine million along the trans-Saharan caravan route, and eleven to twenty million (depending on the author) across the Atlantic Ocean"[35]
Zanzibar was once East Africa's main slave-trading port, during the East African slave trade and under Omani Arabs in the 19th century, as many as 50,000 enslaved people were passing through the city each year.[36]
Prior to the 16th century, the bulk of enslaved people exported from Africa were shipped from East Africa to the Arabian peninsula. Zanzibar became a leading port in this trade.[37] Arab traders of enslaved people differed from European ones in that they would often conduct raiding expeditions themselves, sometimes penetrating deep into the continent. They also differed in that their market greatly preferred the purchase of enslaved females over male.[38]
The increased presence of European rivals along the East coast led Arab traders to concentrate on the overland slave caravan routes across the Sahara from the Sahel to North Africa. The German explorer Gustav Nachtigal reported seeing slave caravans departing from Kukawa in Bornu bound for Tripoli and Egypt in 1870. The trade of enslaved people represented the major source of revenue for the state of Bornu as late as 1898. The eastern regions of the Central African Republic have never recovered demographically from the impact of 19th-century raids from the Sudan and still have a population density of less than 1 person/km2.[39] During the 1870s, European initiatives against the trade of enslaved people caused an economic crisis in northern Sudan, precipitating the rise of Mahdist forces. Mahdi's victory created an Islamic state, one that quickly reinstituted slavery.[40][41]
European involvement in the East African trade of enslaved people began when Portugal established Estado da Índia in the early 16th century. From then until the 1830s, c. 200 enslaved people were exported from Portuguese Mozambique annually and similar figures has been estimated for enslaved people brought from Asia to the Philippines during the Iberian Union (1580–1640).[42][43]
The Middle Passage, the crossing of the Atlantic to the Americas, endured by enslaved people laid out in rows in the holds of ships, was only one element of the well-known triangular trade engaged in by Portuguese, American, Dutch, Danish-Norwegians,[44] French, British and others. Ships having landed with enslaved people in Caribbean ports would take on sugar, indigo, raw cotton, and later coffee, and make for Liverpool, Nantes, Lisbon or Amsterdam. Ships leaving European ports for West Africa would carry printed cotton textiles, some originally from India, copper utensils and bangles, pewter plates and pots, iron bars more valued than gold, hats, trinkets, gunpowder and firearms and alcohol. Tropical shipworms were eliminated in the cold Atlantic waters, and at each unloading, a profit was made.[citation needed]
The Atlantic slave trade peaked in the late 18th century when the largest number of people were captured and enslaved on raiding expeditions into the interior of West Africa. These expeditions were typically carried out by African states, such as the Bono State, Oyo empire (Yoruba), Kong Empire, Kingdom of Benin, Imamate of Futa Jallon, Imamate of Futa Toro, Kingdom of Koya, Kingdom of Khasso, Kingdom of Kaabu, Fante Confederacy, Ashanti Confederacy, Aro Confederacy and the kingdom of Dahomey.[45][46] Europeans rarely entered the interior of Africa, due to fear of disease and moreover fierce African resistance. The enslaved people were brought to coastal outposts where they were traded for goods. The people captured on these expeditions were shipped by European traders to the colonies of the New World. It is estimated that over the centuries, twelve to twenty million enslaved people were shipped from Africa by European traders, of whom some 15 percent died during the terrible voyage, many during the arduous journey through the Middle Passage. The great majority were shipped to the Americas, but some also went to Europe and Southern Africa.[citation needed]

While talking about the trade of enslaved people in East Africa in his journals, David Livingstone said To overdraw its evil is a simple impossibility.[47]While travelling in the African Great Lakes Region in 1866, Livingstone described a trail of slaves:
19th June 1866 – We passed a woman tied by the neck to a tree and dead, the people of the country explained that she had been unable to keep up with the other slaves in a gang, and her master had determined that she should not become anyone's property if she recovered.26th June. – ...We passed a slave woman shot or stabbed through the body and lying on the path: a group of men stood about a hundred yards off on one side, and another of the women on the other side, looking on; they said an Arab who passed early that morning had done it in anger at losing the price he had given for her, because she was unable to walk any longer.
27th June 1866 – To-day we came upon a man dead from starvation, as he was very thin. One of our men wandered and found many slaves with slave-sticks on, abandoned by their masters from want of food; they were too weak to be able to speak or say where they had come from; some were quite young.[48]The strangest disease I have seen in this country seems really to be broken-heartedness, and it attacks free men who have been captured and made slaves... Twenty one were unchained, as now safe; however all ran away at once; but eight with many others still in chains, died in three days after the crossing. They described their only pain in the heart, and placed the hand correctly on the spot, though many think the organ stands high up in the breast-bone.[49]African states played a key role in the trade of enslaved people, and slavery was a common practice among Sub Saharan Africans even before the involvement of the Arabs, Berbers and Europeans. There were three types: those who were enslaved through conquest, in lieu of unpaid debts, or those whose parents gave them as property to tribal chiefs. Chieftains would barter their enslaved people to Arab, Berber, Ottoman or European buyers for rum, spices, cloth or other goods.[50] Selling captives or prisoners was a common practice among Africans, Turks, Berbers and Arabs during that era. However, as the Atlantic trade of enslaved people increased its demand, local systems which primarily serviced indentured servitude expanded. European trading of enslaved people, as a result, was the most pivotal change in the social, economic, cultural, spiritual, religious, political dynamics of the concept of trading in enslaved people. It ultimately undermined local economies and political stability as villages' vital labour forces were shipped overseas as slave raids and civil wars became commonplace. Crimes which were previously punishable by some other means became punishable by enslavement.[51]
Slavery already existed in Kingdom of Kongo prior to the arrival of the Portuguese. Because it had been established within his kingdom, Afonso I of Kongo believed that the slave trade should be subject to Kongo law. When he suspected the Portuguese of receiving illegally enslaved persons to sell, he wrote letters to the King João III of Portugal in 1526 imploring him to put a stop to the practice.[52]
The kings of Dahomey sold their war captives into transatlantic slavery, who otherwise may have been killed in a ceremony known as the Annual Customs. As one of West Africa's principal slave states, Dahomey became extremely unpopular with neighbouring peoples.[53][54][55] Like the Bambara Empire to the east, the Khasso kingdoms depended heavily on the slave trade for their economy. A family's status was indicated by the number of enslaved people it owned, leading to wars for the sole purpose of taking more captives. This trade led the Khasso into increasing contact with the European settlements of Africa's west coast, particularly the French.[56] Benin grew increasingly rich during the 16th and 17th centuries on the trade of enslaved people with Europe; enslaved people from enemy states of the interior were sold, and carried to the Americas in Dutch and Portuguese ships. The Bight of Benin's shore soon came to be known as the "Slave Coast".[57]
In the 1840s, King Gezo of Dahomey said:[9][58]
"The slave trade is the ruling principle of my people. It is the source and the glory of their wealth...the mother lulls the child to sleep with notes of triumph over an enemy reduced to slavery."In 1807 the United Kingdom made the international trade of enslaved people illegal with the Slave Trade Act. The Royal Navy was deployed to prevent slavers from the United States, France, Spain, Portugal, Holland, West Africa and Arabia. The King of Bonny (now in Nigeria) allegedly became dissatisfied of the British intervention in stopping the trade of enslaved people:[59]
"We think this trade must go on. That is the verdict of our oracle and the priests. They say that your country, however great, can never stop a trade ordained by God himself."Joseph Miller states that African buyers would prefer males, but in reality, women and children would be more easily captured as men fled. Those captured would be sold for various reasons such as food, debts, or servitude. Once captured, the journey to the coast killed many and weakened others. Disease engulfed many, and insufficient food damaged those who made it to the coasts. Scurvy was so common that it was known as mal de Luanda (Luanda sickness).[60] The assumption for those who died on the journey died from malnutrition. As food was limited, water may have been just as bad. Dysentery was widespread and poor sanitary conditions at ports did not help. Since supplies were poor, enslaved people were not equipped with the best clothing, meaning they were even more exposed to diseases.[60]
On top of the fear of disease, people were afraid of why they were being captured. The popular assumption was that Europeans were cannibals. Stories and rumours spread that whites captured Africans to eat them.[60] Olaudah Equiano accounts his experience about the sorrow enslaved people encountered at the ports. He talks about his first moment on a slave ship and asked if he was going to be eaten.[61] Yet, the worst for slaves has only begun, and the journey on the water proved to be more harrowing. For every 100 Africans captured, only 64 would reach the coast, and only about 50 would reach the New World.[60]
Others believe that slavers had a vested interest in capturing rather than killing, and in keeping their captives alive; and that this coupled with the disproportionate removal of males and the introduction of new crops from the Americas (cassava, maize) would have limited general population decline to particular regions of western Africa around 1760–1810, and in Mozambique and neighbouring areas half a century later. There has also been speculation that within Africa, females were most often captured as brides, with their male protectors being a "bycatch" who would have been killed if there had not been an export market for them.
British explorer Mungo Park encountered a group of enslaved people when traveling through Mandinka country:
They were all very inquisitive, but they viewed me at first with looks of horror, and repeatedly asked if my countrymen were cannibals. They were very desirous to know what became of the slaves after they had crossed the salt water. I told them that they were employed in cultivation the land; but they would not believe me ... A deeply-rooted idea that the whites purchase negroes for the purpose of devouring them, or of selling them to others that they may be devoured hereafter, naturally makes the slaves contemplate a journey towards the coast with great terror, insomuch that the slatees are forced to keep them constantly in irons, and watch them very closely, to prevent their escape.[62]During the period from the late 19th century and early 20th century, demand for the labour-intensive harvesting of rubber drove frontier expansion and forced labour. The personal monarchy of Belgian King Leopold II in the Congo Free State saw mass killings and slavery to extract rubber.[63]

Stephanie Smallwood in her book Saltwater Slavery uses Equiano's account on board ships to describe the general thoughts of most slaves:"Then," said I, "how comes it in all our country we never heard of them?" They told me because they lived so very far off. I then asked where were their women? Had they any like themselves? I was told that they had. "And why," said I, "do we not see them?" They answered, because they were left behind. I asked how the vessel could go? They told me they could not tell; but that there was cloth put upon the masts by the help of the ropes I saw, and then the vessel went on; and the white men had some spell or magic they put in the water when they liked, in order to stop the vessel. I was exceedingly amazed at this account, and really thought they were spirits. I there-fore wished much to be from amongst them, for I expected they would sacrifice me; but my wishes were vain—for we were so quartered that it was impossible for any of us to make our escape.[64]These accounts raised many questions as some slaves grew philosophical with their journey. Smallwood points out the challenges for slaves were physical and metaphysical. The physical would be obvious as the challenge to overcome capacity, lack of ship room, and food. The metaphysical was unique, as the open sea would challenge African slaves' vision of the ocean as habitable.[64] The journey on the ocean would prove to be an African's biggest fear that would keep them in awe. Because of the lack of knowledge of the sea, Africans would experience feelings of extreme anxiety. Europeans were also fearful of the sea due to diseases, but not to the extent of Africans. Part of their fear came with the lack of a sense of time, as Africans used seasonal weather to predict time and days. The moon was a sense of time, but used like in other cultures and not very accurate.  Africans used the moon to count their days on the sea, but this did not provide seasonal changes.[64]
Surviving the voyage was the main struggle. Close quarters meant everyone was infected by any diseases that spread, including the crew. Death was so common that ships were called tumbeiros, or floating tombs.[64] What shocked Africans the most was how death was handled in the ships. Smallwood says the traditions for an African death was delicate and community-based. On ships, bodies would be thrown into the sea. Because the sea represented bad omens, bodies in the sea represented a form of purgatory and the ship a form of hell. Any Africans who made the journey would have survived extreme disease and malnutrition, as well as trauma from being on the open ocean and the death of their friends.
In Algiers during the time of the Regency of Algiers in North Africa in the 19th century, up to 1.5 million Christians and Europeans were captured and forced into slavery.[65] This eventually led to the Bombardment of Algiers in 1816 by the British and Dutch, forcing the Dey of Algiers to free many slaves.[66]
The trading of children has been reported in modern Nigeria and Benin. In parts of Ghana, a family may be punished for an offense by having to turn over a virgin female to serve as a sex slave within the offended family. In this instance, the woman does not gain the title or status of "wife". In parts of Ghana, Togo, and Benin, shrine slavery persists, despite being illegal in Ghana since 1998. In this system of ritual servitude, sometimes called trokosi (in Ghana) or voodoosi in Togo and Benin, young virgin girls are given as slaves to traditional shrines and are used sexually by the priests in addition to providing free labor for the shrine.[citation needed]
An article in the Middle East Quarterly in 1999 reported that slavery is endemic in Sudan.[67] Estimates of abductions during the Second Sudanese Civil War range from 14,000 to 200,000 people.[68]
During the Second Sudanese Civil War people were taken into slavery; estimates of abductions range from 14,000 to 200,000. Abduction of Dinka women and children was common.[8] In Mauritania it is estimated that up to 600,000 men, women and children, or 20% of the population, are currently enslaved, many of them used as bonded labor.[69] Slavery in Mauritania was criminalized in August 2007.[70]
During the Darfur conflict that began in 2003, many people were kidnapped by Janjaweed and sold into slavery as agricultural labor, domestic servants and sex slaves.[71][72][73]
In Niger, slavery is also a current phenomenon. A Nigerien study has found that more than 800,000 people are enslaved, almost 8% of the population.[74][75][76] Niger installed an anti-slavery provision in 2003.[77][78] In a landmark ruling in 2008, the ECOWAS Community Court of Justice declared that the Republic of Niger failed to protect Hadijatou Mani Koraou from slavery, and awarded Mani CFA 10,000,000 (approximately US$20,000) in reparations.[79]
Sexual slavery and forced labor are common in the Democratic Republic of Congo.[80][81][82]
Many pygmies in the Republic of Congo and Democratic Republic of Congo belong from birth to Bantus in a system of slavery.[83][84]
Evidence emerged in the late 1990s of systematic slavery in cacao plantations in West Africa; see the chocolate and slavery article.[9]
According to the U.S. State Department, more than 109,000 children were working on cocoa farms alone in Ivory Coast in "the worst forms of child labour" in 2002.[85]
On the night of 14–15 April 2014, a group of militants attacked the Government Girls Secondary School in Chibok, Nigeria. They broke into the school, pretending to be guards,[86] telling the girls to get out and come with them.[87] A large number of students were taken away in trucks, possibly into the Konduga area of the Sambisa Forest where Boko Haram were known to have fortified camps.[87] Houses in Chibok were also burned down in the incident.[88] According to police, approximately 276 children were taken in the attack, of whom 53 had escaped as of 2 May.[89] Other reports said that 329 girls were kidnapped, 53 had escaped and 276 were still missing.[90][91][92] The students have been forced to convert to Islam[93] and into marriage with members of Boko Haram, with a reputed "bride price" of ₦2,000 each ($12.50/£7.50).[94][95] Many of the students were taken to the neighbouring countries of Chad and Cameroon, with sightings reported of the students crossing borders with the militants, and sightings of the students by villagers living in the Sambisa Forest, which is considered a refuge for Boko Haram.[95][96]
On 5 May 2014 a video in which Boko Haram leader Abubakar Shekau claimed responsibility for the kidnappings emerged. Shekau claimed that "Allah instructed me to sell them...I will carry out his instructions"[97] and "[s]lavery is allowed in my religion, and I shall capture people and make them slaves."[98] He said the girls should not have been in school and instead should have been married since girls as young as nine are suitable for marriage.[97][98]
During the Second Libyan Civil War Libyans started capturing[99] some of the Sub-Saharan African migrants trying to get to Europe through Libya and selling them on slave markets.[100][101] Slaves are often ransomed to their families and in the meantime until ransom can be paid, they may be tortured, forced to work, sometimes worked to death, and eventually they may be executed or left to starve if the payment has not been made after a period of time. Women are often raped and used as sex slaves and sold to brothels.[102][103][104][105]
Many child migrants also suffer from abuse and child rape in Libya.[106][107]
To participate in the slave trade in Spanish America, bankers and trading companies had to pay the Spanish king for the license, called the Asiento de Negros, but an unknown amount of the trade was illegal. After 1670 when the Spanish Empire declined substantially they outsourced part of the slave trade to the Dutch (1685-1687), the Portuguese, the French (1698-1713) and the English (1713-1750), also providing organized depots in the Caribbean islands to the Dutch, British and French America. As a result of the War of the Spanish Succession, the British government obtained the monopoly (asiento de negros) of selling African slaves in Spanish America, which was granted to the South Sea Company. Meanwhile, slave trading became a core business for privately owned enterprises in the Americas.
In Pre-Columbian Mesoamerica the most common forms of slavery were those of prisoners of war and debtors. People unable to pay back debts could be sentenced to work as slaves to the persons owed until the debts were worked off, as a form of indentured servitude. Warfare was important to Maya society, because raids on surrounding areas provided the victims required for human sacrifice, as well as slaves for the construction of temples.[108] Most victims of human sacrifice were prisoners of war or slaves.[109] Slavery was not usually hereditary; children of slaves were born free. In the Inca Empire, workers were subject to a mita in lieu of taxes which they paid by working for the government. Each ayllu, or extended family, would decide which family member to send to do the work. It is unclear if this labor draft or corvée counts as slavery. The Spanish adopted this system, particularly for their silver mines in Bolivia.[110]
Other slave-owning societies and tribes of the New World were, for example, the Tehuelche of Patagonia, the Comanche of Texas, the Caribs of Dominica, the Tupinambá of Brazil, the fishing societies, such as the Yurok, that lived along the west coast of North America from what is now Alaska to California, the Pawnee and Klamath.[111] Many of the indigenous peoples of the Pacific Northwest Coast, such as the Haida and Tlingit, were traditionally known as fierce warriors and slave-traders, raiding as far as California. Slavery was hereditary, the slaves being prisoners of war. Among some Pacific Northwest tribes about a quarter of the population was enslaved.[112][113] One slave narrative was composed by an Englishman, John R. Jewitt, who had been taken alive when his ship was captured in 1802; his memoir provides a detailed look at life as an enslaved person, and asserts that a large number were held.
Slavery was a mainstay of the Brazilian colonial economy, especially in mining and sugarcane production.[114] 35.3% of all enslaved people from the Atlantic Slave trade went to Colonial Brazil. 4 million enslaved people were obtained by Brazil, 1.5 million more than any other country.[115] Starting around 1550, the Portuguese began to trade enslaved Africans to work the sugar plantations, once the native Tupi people deteriorated. Although Portuguese Prime Minister Sebastião José de Carvalho e Melo, 1st Marquis of Pombal prohibited the importation of enslaved people into Continental Portugal on 12 February 1761, slavery continued in her overseas colonies. Slavery was practiced among all classes. Enslaved people were owned by upper and middle classes, by the poor, and even by other enslaved people.[116]
From São Paulo, the Bandeirantes, adventurers mostly of mixed Portuguese and native ancestry, penetrated steadily westward in their search for Indians to enslave. Along the Amazon river and its major tributaries, repeated slaving raids and punitive attacks left their mark. One French traveler in the 1740s described hundreds of miles of river banks with no sign of human life and once-thriving villages that were devastated and empty. In some areas of the Amazon Basin, and particularly among the Guarani of southern Brazil and Paraguay, the Jesuits had organized their Jesuit Reductions along military lines to fight the slavers. In the mid-to-late 19th century, many Amerindians were enslaved to work on rubber plantations.[117][118][119]
Enslaved people that escaped formed Maroon communities which played an important role in the histories of Brazil and other countries such as Suriname, Puerto Rico, Cuba, and Jamaica. In Brazil, the Maroon villages were called palenques or quilombos. Maroons survived by growing vegetables and hunting. They also raided plantations. At these attacks, the maroons would burn crops, steal livestock and tools, kill slavemasters, and invite other enslaved people to join their communities.[120]
Jean-Baptiste Debret, a French painter who was active in Brazil in the first decades of the 19th century, started out with painting portraits of members of the Brazilian Imperial family, but soon became concerned with the slavery of both blacks and indigenous inhabitants. His paintings on the subject (two appear on this page) helped bring attention to the subject in both Europe and Brazil itself.
The Clapham Sect, a group of evangelical reformers, campaigned during much of the 19th century for Britain to use its influence and power to stop the traffic of enslaved people to Brazil. Besides moral qualms, the low cost of slave-produced Brazilian sugar meant that the British West Indies were unable to match the market prices of Brazilian sugar, and each Briton was consuming 16 pounds (7 kg) of sugar a year by the 19th century. This combination led to intensive pressure from the British government for Brazil to end this practice, which it did by steps over several decades.[121]
First, foreign trade of enslaved people was banned in 1850. Then, in 1871, the sons of the enslaved people were freed. In 1885, enslaved people aged over 60 years were freed. The Paraguayan War contributed to ending slavery as many enslaved people enlisted in exchange for freedom. In Colonial Brazil, slavery was more a social than a racial condition. Some of the greatest figures of the time, like the writer Machado de Assis and the engineer André Rebouças had black ancestry.
Brazil's 1877–78 Grande Seca (Great Drought) in the cotton-growing northeast led to major turmoil, starvation, poverty and internal migration. As wealthy plantation holders rushed to sell their enslaved people south, popular resistance and resentment grew, inspiring numerous emancipation societies. They succeeded in banning slavery altogether in the province of Ceará by 1884.[122] Slavery was legally ended nationwide on 13 May by the Lei Áurea ("Golden Law") of 1888. It was an institution in decadence at these times, as since the 1880s the country had begun to use European immigrant labor instead. Brazil was the last nation in the Western Hemisphere to abolish slavery.[123] The Republic of Ragusa became the first European country to ban the trade of enslaved people in 1416.[citation needed] In modern times Denmark-Norway abolished the trade in 1802.
Slavery was commonly used in the parts of the Caribbean controlled by France and the British Empire. The Lesser Antilles islands of Barbados, St. Kitts, Antigua, Martinique and Guadeloupe, which were the first important societies of enslaved people in the Caribbean, began the widespread use of enslaved Africans by the end of the 17th century, as their economies converted from sugar production.[124]
England had multiple sugar colonies in the Caribbean, especially Jamaica, Barbados, Nevis, and Antigua, which provided a steady flow of sugar sales; forced labor of enslaved people produced the sugar.[125] By the 1700s, there were more enslaved persons in Barbados than in all the English colonies on the mainland combined. Since Barbados did not have many mountains, English planters were able to clear land for sugarcane. Indentured servants were initially sent to Barbados to work in the sugar fields. These indentured servants were treated so poorly that future indentured servants stopped going to Barbados, and there were not enough people to work the fields. This is when the British started bringing in enslaved Africans. For the English planters in Barbados, reliance on enslaved labor was necessary for them to be able to profit from production of cane-origin sugar for the growing market for sugar in Europe and other markets.[citation needed]
In the Treaty of Utrecht, which ended the War of the Spanish Succession (1702–1714), the various European powers negotiating the terms of the treaty also discussed colonial issues as well.[126] Of special importance in the negotiations at Utrecht was the successful negotiation between the British and French delegations for Britain to obtain a thirty-year monopoly on the right to sell slaves in Spanish America, called the Asiento de Negros. Queen Anne also allowed her North American colonies like Virginia to make laws that promoted the importation of slaves. Anne had secretly negotiated with France to get its approval regarding the Asiento.[127] In 1712, she delivered a speech which included a public announcement of her success in taking the Asiento away from France; many London merchants celebrated her economic coup.[128] Most of the trade of enslaved people involved sales to Spanish colonies in the Caribbean, and to Mexico, as well as sales to European colonies in the Caribbean and in North America.[129] Historian Vinita Ricks says the agreement allotted Queen Anne "22.5% (and King Philip V, of Spain 28%) of all profits collected for the Asiento monopoly. Ricks concludes that the Queen's "connection to slave trade revenue meant that she was no longer a neutral observer. She had a vested interest in what happened on slave ships."[130]
By 1778, the French were importing approximately 13,000 Africans for enslavement yearly to the French West Indies.[131]
To regularise slavery, in 1685 Louis XIV had enacted the Code Noir, a slave code accorded certain human rights to enslaved people and responsibilities to the master, who was obliged to feed, clothe and provide for the general well-being of his human property. Free people of color owned one-third of the plantation property and one-quarter of the enslaved people in Saint Domingue (later Haiti).[132] Slavery in the First Republic was abolished on 4 February 1794. When it became clear that Napoleon intended to re-establish slavery in Saint-Domingue (Haiti), Jean-Jacques Dessalines and Alexandre Pétion switched sides, in October 1802. On 1 January 1804, Dessalines, the new leader under the dictatorial 1801 constitution, declared Haiti a free republic.[133] Thus Haiti became the second independent nation in the Western Hemisphere, after the United States, as a result of the only successful slave rebellion in world history.[134]
Whitehall in England announced in 1833 that enslaved people in British colonies would be completely freed by 1838. In the meantime, the government told enslaved people they had to remain on their plantations and would have the status of "apprentices" for the next six years.
In Port-of-Spain, Trinidad, on 1 August 1834, an unarmed group of mainly elderly Negroes being addressed by the Governor at Government House about the new laws, began chanting: "Pas de six ans. Point de six ans" ("Not six years. No six years"), drowning out the voice of the Governor. Peaceful protests continued until a resolution to abolish apprenticeship was passed and de facto freedom was achieved. Full emancipation for all was legally granted ahead of schedule on 1 August 1838, making Trinidad the first British colony with enslaved people to completely abolish slavery.[135]
After Great Britain abolished slavery, it began to pressure other nations to do the same. France, too, abolished slavery. By then Saint-Domingue had already won its independence and formed the independent Republic of Haiti, though France still controlled Guadeloupe, Martinique and a few smaller islands.
Slavery in Canada was practised by First Nations and continued during the European colonization of Canada.[136] It is estimated that there were 
4,200 enslaved people in the French colony of Canada and later British North America between 1671 and 1831.[137] Two-thirds of these were of indigenous ancestry 
(typically called panis)[138] whereas the other third were of African descent.[137] They were house servants and farm workers.[139]  The number of enslaved people of color increased during British rule, especially with the arrival of United Empire Loyalists after 1783.[140] A small portion of Black Canadians today are descended from these slaves.[141]
The practice of slavery in the Canadas ended through case law; having died out in the early 19th century through judicial actions litigated on behalf of enslaved people seeking manumission.[142] The courts, to varying degrees, rendered slavery unenforceable in both Lower Canada and Nova Scotia. In Lower Canada, for example, after court decisions in the late 1790s, the "slave could not be compelled to serve longer than he would, and ... might leave his master at will."[143] Upper Canada passed the  Act Against Slavery in 1793, one of the earliest anti-slavery acts in the world.[144] The institution was formally banned throughout most of the British Empire, including the Canadas in 1834, after the passage of the Slavery Abolition Act 1833 in the British parliament. These measures resulted in a number of Black people (free and slaves) from the United States moving to Canada after the American Revolution, known as the Black Loyalists; and again after the War of 1812, with a number of Black Refugees settling in Canada. During the mid-19th century, British North America served as a terminus for the Underground Railroad, a network of routes used by enslaved African-Americans to escape a slave state.
During the period from the late 19th century and early 20th century, demand for the labor-intensive harvesting of rubber drove frontier expansion and slavery in Latin America and elsewhere. Indigenous peoples were enslaved as part of the rubber boom in Ecuador, Peru, Colombia, and Brazil.[145] In Central America, rubber tappers participated in the enslavement of the indigenous Guatuso-Maleku people for domestic service.[146]
In late August 1619, the frigate White Lion, a privateer ship owned by Robert Rich, 2nd Earl of Warwick, but flying a Dutch flag arrived at Point Comfort, Virginia (several miles downstream from the colony of Jamestown, Virginia) with the first recorded enslaved people from Africa to Virginia. The approximately 20 Africans were from the present-day Angola. They had been removed by the White Lion's crew from a Portuguese cargo ship, the São João Bautista.[147][148]
Historians are undecided if the legal practice of slavery began in the colony because at least some of them had the status of indentured servant. Alden T. Vaughn says most agree that both black slaves and indentured servants existed by 1640.[149]
Only a small fraction of the enslaved Africans brought to the New World came to British North America, perhaps as little as 5% of the total. The vast majority of enslaved people were sent to the Caribbean sugar colonies, Brazil, or Spanish America.
By the 1680s, with the consolidation of England's Royal African Company, enslaved Africans were arriving in English colonies in larger numbers, and the institution continued to be protected by the British government. Colonists now began purchasing slaves in larger numbers.
The shift from indentured servants to enslaved African was prompted by a dwindling class of former servants who had worked through the terms of their indentures and thus became competitors to their former masters. These newly freed servants were rarely able to support themselves comfortably, and the tobacco industry was increasingly dominated by large planters. This caused domestic unrest culminating in Bacon's Rebellion. Eventually, chattel slavery became the norm in regions dominated by plantations.
The Fundamental Constitutions of Carolina established a model in which a rigid social hierarchy placed enslaved people under the absolute authority of their master. With the rise of a plantation economy in the Carolina Lowcountry based on rice cultivation, a society of enslaved people was created that later became the model for the King Cotton economy across the Deep South. The model created by South Carolina was driven by the emergence of a majority enslaved population that required repressive and often brutal force to control. Justification for such an enslaved society developed into a conceptual framework of white supremacy in the American colonies.[158]
Several local slave rebellions took place during the 17th and 18th centuries: Gloucester County, Virginia Revolt (1663);[159] New York Slave Revolt of 1712; Stono Rebellion (1739); and New York Slave Insurrection of 1741.[160]
Within the British Empire, the Massachusetts courts began to follow England when, in 1772, England became the first country in the world to outlaw the slave trade within its borders (see Somerset v Stewart) followed by the Knight v. Wedderburn decision in Scotland in 1778. Between 1764 and 1774, seventeen enslaved people appeared in Massachusetts courts to sue their owners for freedom.[161] In 1766, John Adams' colleague Benjamin Kent won the first trial in the present-day United States to free an enslaved person (Slew vs. Whipple).[162][163][164][165][166][167]
The Republic of Vermont banned slavery in its constitution of 1777 and continued the ban when it entered the United States in 1791.[168] Through the Northwest Ordinance of 1787 under the Congress of the Confederation, slavery was prohibited in the territories north west of the Ohio River. In 1794, Congress banned American vessels from being used in the slave trade, and also banned the export of enslaved people from America to other countries.[169] However, little effort was made to enforce this legislation. The slave ship owners of Rhode Island were able to continue in trade, and the USA's slaving fleet in 1806 was estimated to be nearly 75% as large as that of Britain, with dominance of the transportation of enslaved people into Cuba.[23]: 63  By 1804, abolitionists succeeded in passing legislation that ended legal slavery in every northern state (with slaves above a certain age legally transformed to indentured servants).[170] Congress passed an Act Prohibiting Importation of Slaves as of 1 January 1808; but not the internal slave trade.[171]
Despite the actions of abolitionists, free blacks were subject to racial segregation in the Northern states.[172] While the United Kingdom did not ban slavery throughout most of the empire, including British North America till 1833, free blacks found refuge in the Canadas after the American Revolutionary War and again after the War of 1812. Refugees from slavery fled the South across the Ohio River to the North via the Underground Railroad. Midwestern state governments asserted States Rights arguments to refuse federal jurisdiction over fugitives. Some juries exercised their right of jury nullification and refused to convict those indicted under the Fugitive Slave Act of 1850.
After the passage of the Kansas–Nebraska Act in 1854, armed conflict broke out in Kansas Territory, where the question of whether it would be admitted to the Union as a slave state or a free state had been left to the inhabitants. The radical abolitionist John Brown was active in the mayhem and killing in "Bleeding Kansas." The true turning point in public opinion is better fixed at the Lecompton Constitution fraud. Pro-slavery elements in Kansas had arrived first from Missouri and quickly organized a territorial government that excluded abolitionists. Through the machinery of the territory and violence, the pro-slavery faction attempted to force the unpopular pro-slavery Lecompton Constitution through the state. This infuriated Northern Democrats, who supported popular sovereignty, and was exacerbated by the Buchanan administration reneging on a promise to submit the constitution to a referendum—which would surely fail. Anti-slavery legislators took office under the banner of the newly formed Republican Party. The Supreme Court in the Dred Scott decision of 1857 asserted that one could take one's property anywhere, even if one's property was chattel and one crossed into a free territory. It also asserted that African Americans could not be federal citizens. Outraged critics across the North denounced these episodes as the latest of the Slave Power (the politically organized slave owners) taking more control of the nation.[173]
The enslaved population in the United States stood at four million.[174] Ninety-five percent of blacks lived in the South, comprising one third of the population there as opposed to 1% of the population of the North. The central issue in politics in the 1850s involved the extension of slavery into the western territories, which settlers from the Northern states opposed. The Whig Party split and collapsed on the slavery issue, to be replaced in the North by the new Republican Party, which was dedicated to stopping the expansion of slavery. Republicans gained a majority in every northern state by absorbing a faction of anti-slavery Democrats, and warning that slavery was a backward system that undercut liberal democracy and economic modernization.[175] Numerous compromise proposals were put forward, but they all collapsed. A majority of Northern voters were committed to stopping the expansion of slavery, which they believed would ultimately end slavery. Southern voters were overwhelmingly angry that they were being treated as second-class citizens. In the election of 1860, the Republicans swept Abraham Lincoln into the Presidency and his party took control with legislators into the United States Congress. The states of the Deep South, convinced that the economic power of what they called "King Cotton" would overwhelm the North and win support from Europe voted to secede from the U.S. (the Union). They formed the Confederate States of America, based on the promise of maintaining slavery. War broke out in April 1861, as both sides sought wave after wave of enthusiasm among young men volunteering to form new regiments and new armies. In the North, the main goal was to preserve the union as an expression of American nationalism.
 Rebel leaders Jefferson Davis, Robert E. Lee, Nathan Bedford Forrest and others were slavers and slave-traders.
By 1862 most northern leaders realized that the mainstay of Southern secession, slavery, had to be attacked head-on. All the border states rejected President Lincoln's proposal for compensated emancipation. However, by 1865 all had begun the abolition of slavery, except Kentucky and Delaware. The Emancipation Proclamation was an executive order issued by Lincoln on 1 January 1863. In a single stroke, it changed the legal status, as recognized by the U.S. government, of 3 million enslaved people in designated areas of the Confederacy from "slave" to "free." It had the practical effect that as soon as an enslaved person escaped the control of the Confederate government, by running away or through advances of the Union Army, the enslaved person became legally and actually free. Plantation owners, realizing that emancipation would destroy their economic system, sometimes moved their human property as far as possible out of reach of the Union Army. By June 1865, the Union Army controlled all of the Confederacy and liberated all of the designated enslaved people. The owners were never compensated.[176] About 186,000 free blacks and newly freed people fought for the Union in the Army and Navy, thereby validating their claims to full citizenship.[177]
The severe dislocations of war and Reconstruction had a severe negative impact on the black population, with a large amount of sickness and death.[178][179] After liberation, many of the Freedmen remained on the same plantation. Others fled or crowded into refugee camps operated by the Freedmen's Bureau. The Bureau provided food, housing, clothing, medical care, church services, some schooling, legal support, and arranged for labor contracts.[180] Fierce debates about the rights of the Freedmen, and of the defeated Confederates, often accompanied by killings of black leaders, marked the Reconstruction Era, 1863–77.[181]
Slavery was never reestablished, but after President Ulysses S. Grant left the White House in 1877, white-supremacist "Redeemer" Southern Democrats took control of all the southern states, and blacks lost nearly all the political power they had achieved during Reconstruction. By 1900, they also lost the right to vote – they had become second class citizens. The great majority lived in the rural South in poverty working as laborers, sharecroppers or tenant farmers; a small proportion owned their own land. The black churches, especially the Baptist Church, was the center of community activity and leadership.[182]
Slavery has existed all throughout Asia, and forms of slavery still exist today. In the ancient Near East and Asia Minor slavery was common practice, dating back to the very earliest recorded civilisations in the world such as Sumer, Elam, Ancient Egypt, Akkad, Assyria, Ebla and Babylonia, as well as amongst the Hattians, Hittites, Hurrians, Mycenaean Greece, Luwians, Canaanites, Israelites, Amorites, Phoenicians, Arameans, Ammonites, Edomites, Moabites, Byzantines, Philistines, Medes, Phrygians, Lydians, Mitanni, Kassites, Parthians, Urartians, Colchians, Chaldeans and Armenians.[183][184][185]
Slavery in the Middle East first developed out of the slavery practices of the Ancient Near East,[186] and these practices were radically different at times, depending on social-political factors such as the Muslim slave trade. Two rough estimates by scholars of the number of slaves held over twelve centuries in Muslim lands are 11.5 million[187]
and 14 million.[188][189]
Under Sharia (Islamic law),[186][190] children of slaves or prisoners of war could become slaves but only non-Muslims.[191] Manumission of a slave was encouraged as a way of expiating sins.[192] Many early converts to Islam, such as Bilal ibn Rabah al-Habashi, were poor and former slaves.[193][194][195][196] In theory, slavery in Islamic law does not have a racial or color component, although this has not always been the case in practice.[197]
Bernard Lewis writes: "In one of the sad paradoxes of human history, it was the humanitarian reforms brought by Islam that resulted in a vast development of the slave trade inside, and still more outside, the Islamic empire." He notes that the Islamic injunctions against the enslavement of Muslims led to massive importation of slaves from the outside.[198] According to Patrick Manning, Islam by recognizing and codifying slavery seems to have done more to protect and expand slavery than the reverse.[199]
Slavery was a legal and important part of the economy of the Ottoman Empire and Ottoman society[200] until the slavery of Caucasians was banned in the early 19th century, although slaves from other groups were allowed.[201] In Constantinople (present-day Istanbul), the administrative and political center of the Empire, about a fifth of the population consisted of slaves in 1609.[202] Even after several measures to ban slavery in the late 19th century, the practice continued largely unfazed into the early 20th century. As late as 1908, female slaves were still sold in the Ottoman Empire. Sexual slavery was a central part of the Ottoman slave system throughout the history of the institution.[203][204]
A member of the Ottoman slave class, called a kul in Turkish, could achieve high status. Harem guards and janissaries are some of the better-known positions a slave could hold, but slaves were actually often at the forefront of Ottoman politics. The majority of officials in the Ottoman government were bought slaves, raised free, and integral to the success of the Ottoman Empire from the 14th century into the 19th. Many officials themselves owned a large number of slaves, although the Sultan himself owned by far the largest amount.[205] By raising and specially training slaves as officials in palace schools such as Enderun, the Ottomans created administrators with intricate knowledge of government and fanatic loyalty.
Ottomans practiced devşirme, a sort of "blood tax" or "child collection", young Christian boys from the Balkans and Anatolia were taken from their homes and families, brought up as Muslims, and enlisted into the most famous branch of the kapıkulu, the Janissaries, a special soldier class of the Ottoman army that became a decisive faction in the Ottoman invasions of Europe.[206]
During the various 18th and 19th century persecution campaigns against Christians as well as during the culminating Assyrian, Armenian and Greek genocides of World War I, many indigenous Armenian, Assyrian and Greek Christian women and children were carried off as slaves by the Ottoman Turks and their Kurdish allies. Henry Morgenthau, Sr., U.S. Ambassador in Constantinople from 1913 to 1916, reports in his Ambassador Morgenthau's Story that there were gangs trading white slaves during his term in Constantinople.[207] He also reports that Armenian girls were sold as slaves during the Armenian Genocide.[208][209]
According to Ronald Segal, the male:female gender ratio in the Atlantic slave trade was 2:1, whereas in Islamic lands the ratio was 1:2. Another difference between the two was, he argues, that slavery in the west had a racial component, whereas the Qur'an explicitly condemned racism. This, in Segal's view, eased assimilation of freed slaves into society.[210] Men would often take their female slaves as concubines; in fact, most Ottoman sultans were sons of such concubines.[210]
Scholars differ as to whether or not slaves and the institution of slavery existed in ancient India. These English words have no direct, universally accepted equivalent in Sanskrit or other Indian languages, but some scholars translate the word dasa, mentioned in texts like Manu Smriti,[211] as slaves.[212] Ancient historians who visited India offer the closest insights into the nature of Indian society and slavery in other ancient civilizations. For example, the Greek historian Arrian, who chronicled India about the time of Alexander the Great, wrote in his Indika,[213]
The Indians do not even use aliens as slaves, much less a countryman of their own.During the millennium long Chinese domination of Vietnam, Vietnam was a great source of slave girls who were used as sex slaves in China.[219][220] The slave girls of Viet were even eroticized in Tang dynasty poetry.[219]
The Islamic invasions, starting in the 8th century, also resulted in hundreds of thousands of Indians being enslaved by the invading armies, one of the earliest being the armies of the Umayyad commander Muhammad bin Qasim.[221][222][223][224][225] Qutb-ud-din Aybak, a Turkic slave of Muhammad Ghori rose to power following his master's death. For almost a century, his descendants ruled North-Central India in form of Slave Dynasty. Several slaves were also brought to India by the Indian Ocean trades; for example, the Siddi are descendants of Bantu slaves brought to India by Arab and Portuguese merchants.[226]
Andre Wink summarizes the slavery in 8th and 9th century India as follows,
(During the invasion of Muhammad al-Qasim), invariably numerous women and children were enslaved. The sources insist that now, in dutiful conformity to religious law, 'the one-fifth of the slaves and spoils' were set apart for the caliph's treasury and despatched to Iraq and Syria. The remainder was scattered among the army of Islam. At Rūr, a random 60,000 captives reduced to slavery. At Brahamanabad 30,000 slaves were allegedly taken. At Multan 6,000. Slave raids continued to be made throughout the late Umayyad period in Sindh, but also much further into Hind, as far as Ujjain and Malwa. The Abbasid governors raided Punjab, where many prisoners and slaves were taken.In the early 11th century Tarikh al-Yamini, the Arab historian Al-Utbi recorded that in 1001 the armies of Mahmud of Ghazna conquered Peshawar and Waihand (capital of Gandhara) after Battle of Peshawar (1001), "in the midst of the land of Hindustan", and captured some 100,000 youths.[222][223] Later, following his twelfth expedition into India in 1018–19, Mahmud is reported to have returned with such a large number of slaves that their value was reduced to only two to ten dirhams each. This unusually low price made, according to Al-Utbi, "merchants [come] from distant cities to purchase them, so that the countries of Central Asia, Iraq and Khurasan were swelled with them, and the fair and the dark, the rich and the poor, mingled in one common slavery". Elliot and Dowson refer to "five hundred thousand slaves, beautiful men and women.".[224][228][229] Later, during the Delhi Sultanate period (1206–1555), references to the abundant availability of low-priced Indian slaves abound. Levi attributes this primarily to the vast human resources of India, compared to its neighbors to the north and west (India's Mughal population being approximately 12 to 20 times that of Turan and Iran at the end of the 16th century).[230]
Slavery and empire-formation tied in particularly well with iqta and it is within this context of Islamic expansion that elite slavery was later commonly found. It became the predominant system in North India in the thirteenth century and retained considerable importance in the fourteenth century. Slavery was still vigorous in fifteenth-century Bengal, while after that date it shifted to the Deccan where it persisted until the seventeenth century. It remained present to a minor extent in the Mughal provinces throughout the seventeenth century and had a notable revival under the Afghans in North India again in the eighteenth century.The Delhi sultanate obtained thousands of slaves and eunuch servants from the villages of Eastern Bengal (a widespread practice which Mughal emperor Jahangir later tried to stop). Wars, famines, pestilences drove many villagers to sell their children as slaves. The Muslim conquest of Gujarat in Western India had two main objectives. The conquerors demanded and more often forcibly wrested both land owned by Hindus and Hindu women. Enslavement of women invariably led to their conversion to Islam.[232] In battles waged by Muslims against Hindus in Malwa and Deccan plateau, a large number of captives were taken. Muslim soldiers were permitted to retain and enslave POWs as plunder.[233]
The first Bahmani sultan, Alauddin Bahman Shah is noted to have captured 1,000 singing and dancing girls from Hindu temples after he battled the northern Carnatic chieftains. The later Bahmanis also enslaved civilian women and children in wars; many of them were converted to Islam in captivity.[234][235] About the Mughal empire, W.H. Moreland observed, "it became a fashion to raid a village or group of villages without any obvious justification, and carry off the inhabitants as slaves."[236][237][238]
During the rule of Shah Jahan, many peasants were compelled to sell their women and children into slavery to meet the land revenue demand.[239] Slavery was officially abolished in British India by the Indian Slavery Act, 1843. However, in modern India, Pakistan and Nepal, there are millions of bonded laborers, who work as slaves to pay off debts.[240][241][242]
The Tang dynasty purchased Western slaves from the Radhanite Jews.[243] Tang Chinese soldiers and pirates enslaved Koreans, Turks, Persians, Indonesians, and people from Inner Mongolia, Central Asia, and northern India.[244][245][246][247] The greatest source of slaves came from southern tribes, including Thais and aboriginals from the southern provinces of Fujian, Guangdong, Guangxi, and Guizhou. Malays, Khmers, Indians, and black Africans were also purchased as slaves in the Tang dynasty.[248] Slavery was prevalent until the late 19th century and early 20th century China.[249] All forms of slavery have been illegal in China since 1910.[250]
Reginald Dyer, recalling operations against tribes in Iranian Baluchistan in 1916, stated in a 1921 memoir that the local Balochi tribes would regularly carry out raids against travellers and small towns. During these raids, women and children would often be abducted to become slaves, and would be sold for prices varying based on quality, age and looks. He stated that the average price for a young woman was 300 rupees, and the average price for a small child 25 rupees. The slaves, it was noted, were often half starved.[251]
Slavery in Japan was, for most of its history, indigenous, since the export and import of slaves was restricted by Japan being a group of islands. In late-16th-century Japan, slavery was officially banned; but forms of contract and indentured labor persisted alongside the period penal codes' forced labor. During the Second Sino-Japanese War and the Pacific War, the Imperial Japanese Armed Forces used millions of civilians and prisoners of war from several countries as forced laborers.[252][253][254]
In Korea, slavery was officially abolished with the Gabo Reform of 1894. During the Joseon period, in times of poor harvest and famine, many peasants voluntarily sold themselves into the nobi system in order to survive.[255]
In Southeast Asia, there was a large slave class in Khmer Empire who built the enduring monuments in Angkor Wat and did most of the heavy work.[256] Between the 17th and the early 20th centuries one-quarter to one-third of the population of some areas of Thailand and Burma were slaves.[257] By the 19th century, Bhutan had developed a slave trade with Sikkim and Tibet, also enslaving British subjects and Brahmins.[258][259] According to the International Labour Organization (ILO), during the early 21st century an estimated 800,000
people are subject to forced labor in Myanmar.[260]
Slavery in pre-Spanish Philippines was practiced by the tribal Austronesian peoples who inhabited the culturally diverse islands. The neighbouring Muslim states conducted slave raids from the 1600s into the 1800s in coastal areas of the Gulf of Thailand and the Philippine islands.[261][262] Slaves in Toraja society in Indonesia were family property. People would become slaves when they incurred a debt. Slaves could also be taken during wars, and slave trading was common. Torajan slaves were sold and shipped out to Java and Siam. Slaves could buy their freedom, but their children still inherited slave status. Slavery was abolished in 1863 in all Dutch colonies.[263][264]
According to media reports from late 2014, the Islamic State of Iraq and the Levant (ISIL) was selling Yazidi and Christian women as slaves.[265] According to Haleh Esfandiari of the Woodrow Wilson International Center for Scholars, after ISIL militants have captured an area "[t]hey usually take the older women to a makeshift slave market and try to sell them."[266] In mid-October 2014, the UN estimated that 5,000 to 7,000 Yazidi women and children were abducted by ISIL and sold into slavery.[267] In the digital magazine Dabiq, ISIL claimed religious justification for enslaving Yazidi women whom they consider to be from a heretical sect. ISIL claimed that the Yazidi are idol worshipers and their enslavement is part of the old shariah practice of spoils of war.[268][269][270][271][272] According to The Wall Street Journal, ISIL appeals to apocalyptic beliefs and claims "justification by a Hadith that they interpret as portraying the revival of slavery as a precursor to the end of the world".[273]
ISIL announced the revival of slavery as an institution.[274] In 2015 the official slave prices set by ISIL were following:[275][276]
However some slaves have been sold for as little as a pack of cigarettes.[277]
Sex slaves were sold to Saudi Arabia, other Persian Gulf states and Turkey.[278]
Records of slavery in Ancient Greece go as far back as Mycenaean Greece. The origins are not known, but it appears that slavery became an important part of the economy and society only after the establishment of cities.[279] Slavery was common practice and an integral component of ancient Greece, as it was in other societies of the time. It is estimated that in Athens, the majority of citizens owned at least one slave. Most ancient writers considered slavery not only natural but necessary, but some isolated debate began to appear, notably in Socratic dialogues. The Stoics produced the first condemnation of slavery recorded in history.[18]
During the 8th and the 7th centuries BC, in the course of the two Messenian Wars, the Spartans reduced an entire population to a pseudo-slavery called helotry.[280] According to Herodotus (IX, 28–29), helots were seven times as numerous as Spartans. Following several helot revolts around the year 600 BC, the Spartans restructured their city-state along authoritarian lines, for the leaders decided that only by turning their society into an armed camp could they hope to maintain control over the numerically dominant helot population.[281] In some Ancient Greek city-states, about 30% of the population consisted of slaves, but paid and slave labor seem to have been equally important.[282]
Romans inherited the institution of slavery from the Greeks and the Phoenicians.[283] As the Roman Republic expanded outward, it enslaved entire populations, thus ensuring an ample supply of laborers to work in Rome's farms, quarries and households. The people subjected to Roman slavery came from all over Europe and the Mediterranean. Slaves were used for labor, and also for amusement (e.g. gladiators and sex slaves). In the late Republic, the widespread use of recently enslaved groups on plantations and ranches led to slave revolts on a large scale; the Third Servile War led by Spartacus was the most famous and most threatening to Rome.
Various tribes of Europe are recorded by Roman sources as owning slaves.[284] Strabo records slaves as an export commodity from Britannia,[285] From Llyn Cerrig Bach in Anglesey, an iron gang chain dated to 100 BCE-50 CE was found, over 3 metres long with neck-rings for five captives.[286]
The chaos of invasion and frequent warfare also resulted in victorious parties taking slaves throughout Europe in the early Middle Ages. St. Patrick, himself captured and sold as a slave, protested against an attack that enslaved newly baptized Christians in his "Letter to the Soldiers of Coroticus". As a commonly traded commodity, like cattle, slaves could become a form of internal or trans-border currency.[287]
Slavery during the Early Middle Ages had several distinct sources.
The Vikings raided across Europe, but took the most slaves in raids on the British Isles and in Eastern Europe. While the Vikings kept some slaves as servants, known as thralls, they sold most captives in the Byzantine or Islamic markets. In the West, their target populations were primarily English, Irish, and Scottish, while in the East they were mainly Slavs. The Viking slave-trade slowly ended in the 11th century, as the Vikings settled in the European territories they had once raided. They converted serfs to Christianity and themselves merged with the local populace.[288]
In central Europe, specifically the Frankish/German/Holy Roman Empire of Charlemagne, raids and wars to the east generated a steady supply of slaves from the Slavic captives of these regions. Because of high demand for slaves in the wealthy Muslim empires of Northern Africa, Spain, and the Near East, especially for slaves of European descent, a market for these slaves rapidly emerged. So lucrative was this market that it spawned an economic boom in central and western Europe, today known as the Carolingian Renaissance.[289][290][291] This boom period for slaves stretched from the early Muslim conquests to the High Middle Ages but declined in the later Middle Ages as the Islamic Golden Age waned.
Medieval Spain and Portugal saw almost constant warfare between Muslims and Christians. Al-Andalus sent periodic raiding expeditions to loot the Iberian Christian kingdoms, bringing back booty and slaves. In a raid against Lisbon, Portugal in 1189, for example, the Almohad caliph Yaqub al-Mansur took 3,000 female and child captives. In a subsequent attack upon Silves, Portugal in 1191, his governor of Córdoba took 3,000 Christian slaves.[292]
The Byzantine-Ottoman wars and the Ottoman wars in Europe resulted in the taking of large numbers of Christian slaves and using or selling them in the Islamic world too.[293] After the battle of Lepanto the victors freed approximately 12,000 Christian galley slaves from the Ottoman fleet.[294]
Similarly, Christians sold Muslim slaves captured in war. The Order of the Knights of Malta attacked pirates and Muslim shipping, and their base became a centre for slave trading, selling captured North Africans and Turks. Malta remained a slave market until well into the late 18th century. One thousand slaves were required to man the galleys (ships) of the Order.[295][page needed][296]
Poland banned slavery in the 15th century; in Lithuania, slavery was formally abolished in 1588; the institution was replaced by the second enserfment. Slavery remained a minor institution in Russia until 1723, when Peter the Great converted the household slaves into house serfs. Russian agricultural slaves were formally converted into serfs earlier, in 1679.[297] The escaped Russian serfs and kholops formed autonomous communities in the southern steppes, where they became known as Cossacks (meaning "outlaws").[298]
Capture in war, voluntary servitude and debt slavery became common within the British Isles before 1066. The Bodmin manumissions show both that slavery existed in 9th and 10th Century Cornwall and that many Cornish slave owners did set their slaves free. Slaves were routinely bought and sold. Running away was also common and slavery was never a major economic factor in the British Isles during the Middle Ages. Ireland and Denmark provided markets for captured Anglo-Saxon and Celtic slaves. Pope Gregory I reputedly made the pun, Non Angli, sed Angeli ("Not Angles, but Angels"), after a response to his query regarding the identity of a group of fair-haired Angles, slave children whom he had observed in the marketplace. After the Norman Conquest, the law no longer supported chattel slavery and slaves became part of the larger body of serfs.[299][300]
In the early Middle Ages, the city of Verdun was the centre of the thriving European slave trade in young boys who were sold to the Islamic emirates of Iberia where they were enslaved as eunuchs.[301] The Italian ambassador Liutprand of Cremona, as one example in the 10th century, presented a gift of four eunuchs to Emperor Constantine VII.[302]
Barbary pirates and Maltese corsairs both raided for slaves and purchased slaves from European merchants, often the Radhanites, one of the few groups who could easily move between the Christian and Islamic worlds.[303][304]
In the late Middle Ages, from 1100 to 1500, the European slave-trade continued, though with a shift from being centered among the Western Mediterranean Islamic nations to the Eastern Christian and Muslim states. The city-states of Venice and Genoa controlled the Eastern Mediterranean from the 12th century and the Black Sea from the 13th century. They sold both Slavic and Baltic slaves, as well as Georgians, Turks, and other ethnic groups of the Black Sea and Caucasus. The sale of European slaves by Europeans slowly ended as the Slavic and Baltic ethnic groups Christianized by the Late Middle Ages.[305] European slaves did not pass on an inherited status and was thus more akin to forced labor, or indentured servitude.[original research?]
From the 1440s into the 18th century, Europeans from Italy, Spain, Portugal, France, and England were sold into slavery by North Africans. It has been suggested that "white slavery had been minimised or ignored because academics preferred to treat Europeans as evil colonialists rather than as victims."[306][307] In 1575, the Tatars captured over 35,000 Ukrainians; a 1676 raid took almost 40,000. About 60,000 Ukrainians were captured in 1688; some were ransomed, but most were sold into slavery.[308][309] Some of the Roma people were enslaved over five centuries in Romania until abolition in 1864 (see Slavery in Romania).[310]
The Mongol invasions and conquests in the 13th century also resulted in taking numerous captives into slavery.[311] The Mongols enslaved skilled individuals, women and children and marched them to Karakorum or Sarai, whence they were sold throughout Eurasia. Many of these slaves were shipped to the slave market in Novgorod.[312][313][314]
Slave commerce during the Late Middle Ages was mainly in the hands of Venetian and Genoese merchants and cartels, who were involved in the slave trade with the Golden Horde.[315]  In 1382 the Golden Horde under Khan Tokhtamysh sacked Moscow, burning the city and carrying off thousands of inhabitants as slaves. Between 1414 and 1423, some 10,000 eastern European slaves were sold in Venice.[316] Genoese merchants organized the slave trade from the Crimea to Mamluk Egypt. For years, the Khanates of Kazan and Astrakhan routinely made raids on Russian principalities for slaves and to plunder towns. Russian chronicles record about 40 raids by Kazan Khans on the Russian territories in the first half of the 16th century.[317]
In 1441 Haci I Giray declared independence from the Golden Horde and established the Crimean Khanate.[318] For a long time, until the early 18th century, the khanate maintained an extensive slave-trade with the Ottoman Empire and the Middle East. In a process called the "harvesting of the steppe" they enslaved many Slavic peasants. Muscovy recorded about 30 major Tatar raids into Muscovite territories between 1558 and 1596.[319]
Moscow was repeatedly a target.[320] In 1521, the combined forces of Crimean Khan Mehmed Giray and his Kazan allies attacked the city and captured thousands of slaves.[321] In 1571, the Crimean Tatars attacked and sacked Moscow, burning everything but the Kremlin and taking thousands of captives as slaves.[322] In Crimea, about 75% of the population consisted of slaves.[323]
In the Viking era beginning circa 793, the Norse raiders often captured and enslaved militarily weaker peoples they encountered. The Nordic countries called their slaves thralls (Old Norse: Þræll).[288] The thralls were mostly from Western Europe, among them many Franks, Frisians, Anglo-Saxons, and both Irish and Britonnic Celts. Many Irish slaves travelled in expeditions for the colonization of Iceland.[324] The Norse also took German, Baltic, Slavic and Latin slaves. The slave trade was one of the pillars of Norse commerce during the 9th through 11th centuries. The 10th-century Persian traveller Ibn Rustah described how Swedish Vikings, the Varangians or Rus, terrorized and enslaved the Slavs taken in their raids along the Volga River. The thrall system was finally abolished in the mid-14th century in Scandinavia.[325]
Mediterranean powers frequently sentenced convicted criminals to row in the war-galleys of the state (initially only in time of war).[326] After the revocation of the Edict of Nantes in 1685 and Camisard rebellion, the French Crown filled its galleys with French Huguenots, Protestants condemned for resisting the state.[327] Galley-slaves lived and worked in such harsh conditions that many did not survive their terms of sentence, even if they survived shipwreck and slaughter or torture at the hands of enemies or of pirates.[328] Naval forces often turned 'infidel' prisoners-of-war into galley-slaves. Several well-known historical figures served time as galley slaves after being captured by the enemy—the Ottoman corsair and admiral Turgut Reis and the Knights Hospitaller Grand Master Jean Parisot de la Valette among them.[329]
Denmark-Norway was the first European country to ban the slave trade.[330] This happened with a decree issued by King Christian VII of Denmark in 1792, to become fully effective by 1803. Slavery as an institution was not banned until 1848. At this time Iceland was a part of Denmark-Norway but slave trading had been abolished in Iceland in 1117 and had never been reestablished.[331]
Slavery in the French Republic was abolished on 4 February 1794, including in its colonies. The lengthy Haitian Revolution by its slaves and free people of color established Haiti as a free republic in 1804 ruled by blacks, the first of its kind.[133] At the time of the revolution, Haiti was known as Saint-Domingue and was a colony of France.[332] Napoleon Bonaparte gave up on Haiti in 1803, but reestablished slavery in Guadeloupe and Martinique in 1804, at the request of planters of the Caribbean colonies. Slavery was permanently abolished in the French empire during the French Revolution of 1848.[333]
The 15th-century Portuguese exploration of the African coast is commonly regarded as the harbinger of European colonialism. In 1452, Pope Nicholas V issued the papal bull Dum Diversas, granting Afonso V of Portugal the right to reduce any "Saracens, pagans and any other unbelievers" to hereditary slavery which legitimized slave trade under Catholic beliefs of that time. This approval of slavery was reaffirmed and extended in his Romanus Pontifex bull of 1455. These papal bulls came to serve as a justification for the subsequent era of the slave trade and European colonialism, although for a short period as in 1462 Pius II declared slavery to be "a great crime".[334] Unlike Portugal, Protestant nations did not use the papal bull as a justification for their involvement in the slave trade. The position of the church was to condemn the slavery of Christians, but slavery was regarded as an old established and necessary institution which supplied Europe with the necessary workforce. In the 16th century, African slaves had replaced almost all other ethnicities and religious enslaved groups in Europe.[335] Within the Portuguese territory of Brazil, and even beyond its original borders, the enslavement of Native Americans was carried out by the Bandeirantes.
Among many other European slave markets, Genoa, and Venice were some well-known markets, their importance and demand growing after the great plague of the 14th century which decimated much of the European workforce.[336]
The maritime town of Lagos, Portugal, was the first slave market created in Portugal for the sale of imported African slaves, the Mercado de Escravos, which opened in 1444.[337][338] In 1441, the first slaves were brought to Portugal from northern Mauritania.[338] Prince Henry the Navigator, major sponsor of the Portuguese African expeditions, as of any other merchandise, taxed one fifth of the selling price of the slaves imported to Portugal.[338] By the year 1552 African slaves made up 10 percent of the population of Lisbon.[339][340]
In the second half of the 16th century, the Crown gave up the monopoly on slave trade and the focus of European trade in African slaves shifted from import to Europe to slave transports directly to tropical colonies in the Americas—in the case of Portugal, especially Brazil.[338] In the 15th century, one-third of the slaves were resold to the African market in exchange of gold.[335]
Importation of black slaves was prohibited in mainland Portugal and Portuguese India in 1761, but slavery continued in Portuguese overseas colonies.[341] At the same time, was stimulated the trade of black slaves ("the pieces", in the terms of that time) to Brazil and two companies were founded, with the support and direct involvement of the Marquis of Pombal - the Company of Grão-Pará and Maranhão and the General Company of Pernambuco and Paraíba - whose main activity was precisely the trafficking of slaves, mostly black Africans, to Brazilian lands.[342][341]
Slavery was finally abolished in all Portuguese colonies in 1869.
The Spaniards were the first Europeans to use African slaves in the New World on islands such as Cuba and Hispaniola, due to a shortage of labor caused by the spread of diseases, and so the Spanish colonists gradually became involved in the Atlantic slave trade. The first African slaves arrived in Hispaniola in 1501;[343] by 1517, the natives had been "virtually annihilated" mostly to diseases.[344]
The problem of the justness of Native American's slavery was a key issue for the Spanish Crown. It was Charles V who gave a definite answer to this complicated and delicate matter. To that end, on 25 November 1542, the Emperor abolished slavery by decree in his Leyes Nuevas. This bill was based on the arguments given by the best Spanish theologists and jurists who were unanimous in the condemnation of such slavery as unjust; they declared it illegitimate and outlawed it from America—not just the slavery of Spaniards over Natives—but also the type of slavery practiced among the Natives themselves[345] Thus, Spain became the first country to officially abolish slavery.
However, in the Spanish colonies of Cuba and Puerto Rico, where sugarcane production was highly profitable based on slave labor, African slavery persisted until 1873 in Puerto Rico "with provisions for periods of apprenticeship",[346] and 1886 in Cuba.[347]
Although slavery was illegal inside the Netherlands it flourished throughout the Dutch Empire in the Americas, Africa, Ceylon and Indonesia.[348] The Dutch Slave Coast (Dutch: Slavenkust) referred to the trading posts of the Dutch West India Company on the Slave Coast, which lie in contemporary Ghana, Benin, Togo and Nigeria. Initially the Dutch shipped slaves to Dutch Brazil, and during the second half of the 17th century they had a controlling interest in the trade to the Spanish colonies. Today's Suriname and Guyana became prominent markets in the 18th century. Between 1612 and 1872, the Dutch operated from some 10 fortresses along the Gold Coast (now Ghana), from which slaves were shipped across the Atlantic. Dutch involvement on the Slave Coast increased with the establishment of a trading post in Offra in 1660. Willem Bosman writes in his Nauwkeurige beschrijving van de Guinese Goud- Tand- en Slavekust (1703) that Allada was also called Grand Ardra, being the larger cousin of Little Ardra, also known as Offra. From 1660 onward, Dutch presence in Allada and especially Offra became more permanent.[349] A report from this year asserts Dutch trading posts, apart from Allada and Offra, in Benin City, Grand-Popo, and Savi.
The Offra trading post soon became the most important Dutch office on the Slave Coast. According to a 1670 report, annually 2,500 to 3,000 slaves were transported from Offra to the Americas. These numbers were only feasible in times of peace, however, and dwindled in time of conflict. From 1688 onward, the struggle between the Aja king of Allada and the peoples on the coastal regions, impeded the supply of slaves. The Dutch West India Company chose the side of the Aja king, causing the Offra office to be destroyed by opposing forces in 1692. By 1650 the Dutch had the pre-eminent slave trade in Europe and South East Asia. Later, trade shifted to Ouidah. On the instigation of Governor-General of the Dutch Gold Coast Willem de la Palma, Jacob van den Broucke was sent in 1703 as "opperkommies" (head merchant) to the Dutch trading post at Ouidah, which according to sources was established around 1670.[350][351] Political unrest caused the Dutch to abandon their trading post at Ouidah in 1725, and they then moved to Jaquim, at which place they built Fort Zeelandia.[352] The head of the post, Hendrik Hertog, had a reputation for being a successful slave trader. In an attempt to extend his trading area, Hertog negotiated with local tribes and mingled in local political struggles. He sided with the wrong party, however, leading to a conflict with Director-General Jan Pranger and to his exile to the island of Appa in 1732. The Dutch trading post on this island was extended as the new centre of the slave trade. In 1733, Hertog returned to Jaquim, this time extending the trading post into Fort Zeelandia. The revival of the slave trade at Jaquim was only temporary, however, as his superiors at the Dutch West India Company noticed that Hertog's slaves were more expensive than at the Gold Coast. From 1735, Elmina became the preferred spot to trade slaves.[353] As of 1778, it was estimated that the Dutch were shipping approximately 6,000 Africans for enslavement in the Dutch West Indies each year.[131] Slavery also characterised the Dutch possessions in Indonesia, Ceylon, and South Africa, where Indonesians have made a significant contribution to the Cape Coloured population of that country. The Dutch part in the Atlantic slave trade is estimated at 5–7 percent, as they shipped about 550,000–600,000 African slaves across the Atlantic, about 75,000 of whom died on board before reaching their destinations. From 1596 to 1829, the Dutch traders sold 250,000 slaves in the Dutch Guianas, 142,000 in the Dutch Caribbean, and 28,000 in Dutch Brazil.[354] In addition, tens of thousands of slaves, mostly from India and some from Africa, were carried to the Dutch East Indies.[355] The Netherlands abolished slavery in 1863. Although the decision was made in 1848, it took many years for the law to be implemented. Furthermore, slaves in Suriname would be fully free only in 1873, since the law stipulated that there was to be a mandatory 10-year transition.
Barbary Corsairs continued to trade in European slaves into the Modern time-period.[305] Muslim pirates, primarily Algerians with the support of the Ottoman Empire, raided European coasts and shipping from the 16th to the 19th centuries, and took thousands of captives, whom they sold or enslaved. Many were held for ransom, and European communities raised funds such as Malta's Monte della Redenzione degli Schiavi to buy back their citizens. The raids gradually ended with the naval decline of the Ottoman Empire in the late 16th and 17th centuries, as well as the European conquest of North Africa throughout the 19th century.[305]
From 1609 to 1616, England lost 466 merchant ships to Barbary pirates. 160 English ships were captured by Algerians between 1677 and 1680.[357] Many of the captured sailors were made into slaves and held for ransom. The corsairs were no strangers to the South West of England where raids were known in a number of coastal communities. In 1627 Barbary Pirates under command of the Dutch renegade Jan Janszoon (Murat Reis), operating from the Moroccan port of Salé, occupied the island of Lundy.[358] During this time there were reports of captured slaves being sent to Algiers.[359][360]
Ireland, despite its northern position, was not immune from attacks by the corsairs. In June 1631 Janszoon, with pirates from Algiers and armed troops of the Ottoman Empire, stormed ashore at the little harbor village of Baltimore, County Cork. They captured almost all the villagers and took them away to a life of slavery in North Africa.[361] The prisoners were destined for a variety of fates—some lived out their days chained to the oars as galley slaves, while others would spend long years in the scented seclusion of the harem or within the walls of the sultan's palace. Only two of them ever saw Ireland again.
The Congress of Vienna (1814–15), which ended the Napoleonic Wars, led to increased European consensus on the need to end Barbary raiding.[361] The sacking of Palma on the island of Sardinia by a Tunisian squadron, which carried off 158 inhabitants, roused widespread indignation. Britain had by this time banned the slave trade and was seeking to induce other countries to do likewise. States that were more vulnerable to the corsairs complained that Britain cared more for ending the trade in African slaves than stopping the enslavement of Europeans and Americans by the Barbary States.
In order to neutralise this objection and further the anti-slavery campaign, in 1816 Britain sent Lord Exmouth to secure new concessions from Tripoli, Tunis, and Algiers, including a pledge to treat Christian captives in any future conflict as prisoners of war rather than slaves. He imposed peace between Algiers and the kingdoms of Sardinia and Sicily. On his first visit, Lord Exmouth negotiated satisfactory treaties and sailed for home. While he was negotiating, a number of Sardinian fishermen who had settled at Bona on the Tunisian coast were brutally treated without his knowledge.[361] As Sardinians they were technically under British protection, and the government sent Exmouth back to secure reparation. On 17 August, in combination with a Dutch squadron under Admiral Van de Capellen, Exmouth bombarded Algiers.[361] Both Algiers and Tunis made fresh concessions as a result.
The Barbary states had difficulty securing uniform compliance with a total prohibition of slave-raiding, as this had been traditionally of central importance to the North African economy. Slavers continued to take captives by preying on less well-protected peoples. Algiers subsequently renewed its slave-raiding, though on a smaller scale.[361] Europeans at the Congress of Aix-la-Chapelle in 1818 discussed possible retaliation. In 1820 a British fleet under Admiral Sir Harry Neal bombarded Algiers. Corsair activity based in Algiers did not entirely cease until France conquered the state in 1830.
[361]
The Crimeans frequently mounted raids into the Danubian principalities, Poland-Lithuania, and Muscovy to enslave people whom they could capture; for each captive, the khan received a fixed share (savğa) of 10% or 20%. These campaigns by Crimean forces were either sefers ("sojourns" – officially declared military operations led by the khans themselves), or çapuls ("despoiling" – raids undertaken by groups of noblemen, sometimes illegally because they contravened treaties concluded by the khans with neighbouring rulers).
For a long time, until the early 18th century, the Crimean Khanate maintained a massive slave trade with the Ottoman Empire and the Middle East, exporting about 2 million slaves from Russia and Poland-Lithuania over the period 1500–1700.[362] Caffa (modern Feodosia) became one of the best-known and significant trading ports and slave markets.[363] In 1769 the last major Tatar raid saw the capture of 20,000 Russian and Ruthenian slaves.[364]
Author and historian Brian Glyn Williams writes:
 Fisher estimates that in the sixteenth century the Polish–Lithuanian Commonwealth lost around 20,000 individuals a year and that from 1474 to 1694, as many as a million Commonwealth citizens were carried off into Crimean slavery.[365]Early modern sources are full of descriptions of sufferings of Christian slaves captured by the Crimean Tatars in the course of their raids:
It seems that the position and everyday conditions of a slave depended largely on his/her owner. Some slaves indeed could spend the rest of their days doing exhausting labor: as the Crimean vizir (minister) Sefer Gazi Aga mentions in one of his letters, the slaves were often "a plough and a scythe" of their owners. Most terrible, perhaps, was the fate of those who became galley-slaves, whose sufferings were poeticized in many Ukrainian dumas (songs). ... Both female and male slaves were often used for sexual purposes.[364]Britain played a prominent role in the Atlantic slave trade, especially after 1640, when sugar cane was introduced to the region. At first, most were white Britons, or Irish, enslaved as indentured labour – for a fixed period – in the West Indies. These people may have been criminals, political rebels, the poor with no prospects or others who were simply tricked or kidnapped. Slavery was a legal institution in all of the 13 American colonies and Canada (acquired by Britain in 1763). The profits of the slave trade and of West Indian plantations amounted to under 5% of the British economy at the time of the Industrial Revolution.[366]
A little-known incident in the career of Judge Jeffreys refers to an assize in Bristol in 1685 when he made the mayor of the city, then sitting fully robed beside him on the bench, go into the dock and be fined £1000 for being a "kidnapping knave"; some Bristol traders at the time were known to kidnap their own countrymen and ship them away as slaves.[367]
Somersett's case in 1772 was generally taken at the time to have decided that the condition of slavery did not exist under English law in England. In 1785, English poet William Cowper wrote: "We have no slaves at home – Then why abroad? Slaves cannot breathe in England; if their lungs receive our air, that moment they are free. They touch our country, and their shackles fall. That's noble, and bespeaks a nation proud. And jealous of the blessing. Spread it then, And let it circulate through every vein."[368] The decision proved to be a milestone in the British abolitionist movement, though slavery was not abolished in the British Empire until the passage of the 1833 Slavery Abolition Act.[369] In 1807, following many years of lobbying by the abolitionist movement, led primarily by William Wilberforce, the British Parliament voted to make the slave trade illegal anywhere in the Empire with the Slave Trade Act 1807. Thereafter Britain took a prominent role in combating the trade, and slavery itself was abolished in the British Empire (except for India) with the Slavery Abolition Act 1833. Between 1808 and 1860, the West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard.[370] Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade. Akitoye, the 11th Oba of Lagos, is famous for having used British involvement to regain his rule in return for suppressing slavery among the Yoruba people of Lagos in 1851. Anti-slavery treaties were signed with over 50 African rulers.[371] In 1839, the world's oldest international human rights organization, British and Foreign Anti-Slavery Society (now Anti-Slavery International), was formed in Britain as by Joseph Sturge, which worked to outlaw slavery in other countries.[372]
After 1833, the freed African slaves declined employment in the cane fields. This led to the importation of indentured labour again – mainly from India, and also China.
In 1811, Arthur William Hodge was executed for the murder of a slave in the British West Indies. He was not, however, as some[who?] have claimed, the first white person to have been lawfully executed for the murder of a slave.[373][374]
During World War II Nazi Germany operated several categories of Arbeitslager (Labor Camps) for different categories of inmates. The largest number of them held Polish gentiles and Jewish civilians forcibly abducted in occupied countries (see Łapanka) to provide labor in the German war industry, repair bombed railroads and bridges or work on farms. By 1944, 20% of all workers were foreigners, either civilians or prisoners of war.[375][376][377][378]
As agreed by the Allies at the Yalta conference, Germans were used as forced labor as part of the reparations to be extracted. By 1947, it is estimated that 400,000 Germans (both civilians and POWs) were being used as forced labor by the U.S., France, the UK and the Soviet Union. German prisoners were for example forced to clear minefields in France and the Low Countries. By December 1945, it was estimated by French authorities that 2,000 German prisoners were being killed or injured each month in accidents.[379] In Norway the last available casualty record, from 29 August 1945, shows that by that time a total of 275 German soldiers died while clearing mines, while 392 had been injured.[380]
The Soviet Union took over the already extensive katorga system and expanded it immensely, eventually organizing the Gulag to run the camps. In 1954, a year after Stalin's death, the new Soviet government of Nikita Khrushchev began to release political prisoners and close down the camps. By the end of the 1950s, virtually all "corrective labor camps" were reorganized, mostly into the system of corrective labor colonies. Officially, the Gulag was terminated by the MVD order 20 25 January 1960.[381][verification needed]
During the period of Stalinism, the Gulag labor camps in the Soviet Union were officially called "Corrective labor camps." The term "labor colony"; more exactly, "Corrective labor colony", (Russian: исправительно-трудовая колония, abbr. ИТК), was also in use, most notably the ones for underaged (16 years or younger) convicts and captured besprizorniki (street children, literally, "children without family care"). After the reformation of the camps into the Gulag, the term "corrective labor colony" essentially encompassed labor camps[citation needed].
The Soviet Union had about 14 million people working in Gulags during its existence.[382]
In the first half of the 19th century, small-scale slave raids took place across Polynesia to supply labor and sex workers for the whaling and sealing trades, with examples from both the westerly and easterly extremes of the Polynesian triangle.
By the 1860s this had grown to a larger scale operation with Peruvian slave raids in the South Sea Islands to collect labor for the guano industry.
Ancient Hawaii was a caste society. People were born into specific social classes. Kauwa were those of the outcast or slave class. They are believed to have been war captives or their descendants. Marriage between higher castes and the kauwa was strictly forbidden. The kauwa worked for the chiefs and were often used as human sacrifices at the luakini heiau. (They were not the only sacrifices; law-breakers of all castes or defeated political opponents were also acceptable as victims.)[383]
The kapu system was abolished during the ʻAi Noa in 1819, and with it the distinction between the kauwā slave class and the makaʻāinana (commoners).[384] The 1852 Constitution of the Kingdom of Hawaii officially made slavery illegal.[385]
Before the arrival of European settlers, each Maori tribe (iwi) considered itself a separate entity equivalent to a nation. In the traditional Maori society of Aotearoa, prisoners of war became taurekareka, slaves – unless released, ransomed or eaten.[386] With some exceptions, the child of a slave remained a slave.
As far as it is possible to tell, slavery seems to have increased in the early-19th century with increased numbers of prisoners being taken by Maori military leaders (such as Hongi Hika and Te Rauparaha) to satisfy the need for labor in the Musket Wars, to supply whalers and traders with food, flax and timber in return for western goods. The intertribal Musket Wars lasted from 1807 to 1843; northern tribes who had acquired muskets captured large numbers of slaves. About 20,000 Maori died in the wars. An unknown number of slaves were captured. Northern tribes used slaves (called mokai) to grow large areas of potatoes for trade with visiting ships. Chiefs started an extensive sex trade in the Bay of Islands in the 1830s, using mainly slave girls. By 1835 about 70 to 80 ships per year called into the port. One French captain described the impossibility of getting rid of the girls who swarmed over his ship, outnumbering his crew of 70 by 3 to 1. All payments to the girls were stolen by the chief.[387] By 1833 Christianity had become established in the north of New Zealand, and large numbers of slaves were freed.
Slavery was outlawed in 1840 via the Treaty of Waitangi, although it did not end completely until government was effectively extended over the whole of the country with the defeat of the King movement in the Wars of the mid-1860s.
One group of Polynesians who migrated to the Chatham Islands became the Moriori who developed a largely pacifist culture. It was originally speculated that they settled the Chathams direct from Polynesia, but it is now widely believed they were disaffected Maori who emigrated from the South Island of New Zealand.[388][389][390][391] Their pacifism left the Moriori unable to defend themselves when the islands were invaded by mainland Māori in the 1830s.
Two Taranaki tribes, Ngati Tama and Ngati Mutunga, displaced by the Musket Wars, carried out a carefully planned invasion of the Chatham Islands, 800 km east of Christchurch, in 1835. About 15% of the Polynesian Moriori natives who had migrated to the islands at about 1500 CE were killed, with many women being tortured to death. The remaining population was enslaved for the purpose of growing food, especially potatoes. The Moriori were treated in an inhumane and degrading manner for many years. Their culture was banned and they were forbidden to marry.[392]
Some 300 Moriori men, women and children were massacred and the remaining 1,200 to 1,300 survivors were enslaved.[393][394]
Some Maori took Moriori partners. The state of enslavement of Moriori lasted until the 1860s although it had been discouraged by CMS missionaries in northern New Zealand from the late 1820s. In 1870 Ngati Mutunga, one of the invading tribes, argued before the Native Land Court in New Zealand that their gross mistreatment of the Moriori was standard Maori practice or tikanga.[395]
The isolated island of Rapa Nui/Easter Island was inhabited by the Rapanui, who suffered a series of slave raids from 1805 or earlier, culminating in a near genocidal experience in the 1860s. The 1805 raid was by American sealers and was one of a series that changed the attitude of the islanders to outside visitors, with reports in the 1820s and 1830s that all visitors received a hostile reception. In December 1862, Peruvian slave raiders took between 1,400 and 2,000 islanders back to Peru to work in the guano industry; this was about a third of the island's population and included much of the island's leadership, the last ariki-mau and possibly the last who could read Rongorongo. After intervention by the French ambassador in Lima, the last 15 survivors were returned to the island, but brought with them smallpox, which further devastated the island.
Slavery has existed, in one form or another, throughout the whole of human history. So, too, have movements to free large or distinct groups of slaves. However, abolitionism should be distinguished from efforts to help a particular group of slaves, or to restrict one practice, such as the slave trade.
Drescher (2009) provides a model for the history of the abolition of slavery, emphasizing its origins in Western Europe. Around the year 1500, slavery had virtually died out in Western Europe, but was a normal phenomenon practically everywhere else. The imperial powers – the British, French, Spanish, Portuguese, Dutch, and Belgian empires, and a few others – built worldwide empires based primarily on plantation agriculture using slaves imported from Africa. However, the powers took care to minimize the presence of slavery in their homelands. In 1807 Britain and soon after, the United States also, both criminalized the international slave trade. The Royal Navy was increasingly effective in intercepting slave ships, freeing the captives and taking the crew for trial in courts.
Although there were numerous slave revolts in the Caribbean, the only successful uprising came in the French colony of Haiti in the 1790s, where the slaves rose up, killed the mulattoes and whites, and established the independent Republic of Haiti. 
The continuing profitability of slave-based plantations and the threats of race war slowed the development of abolition movements during the first half of the 19th century. These movements were strongest in Britain, and after 1840 in the United States. The Northern states of the United States abolished slavery, partly in response to the United States Declaration of Independence, between 1777 and 1804. Britain ended slavery in its empire in the 1830s. However, the plantation economies of the southern United States, based on cotton, and those in Brazil and Cuba, based on sugar, expanded and grew even more profitable. The bloody American Civil War ended slavery in the United States in 1865. The system ended in Cuba and Brazil in the 1880s because it was no longer profitable for the owners. Slavery continued to exist in Africa, where Arab slave traders raided black areas for new captives to be sold in the system. European colonial rule and diplomatic pressure slowly put an end to the trade, and eventually to the practice of slavery itself.[396]
In 1772, the Somersett Case (R. v. Knowles, ex parte Somersett)[398] of the English Court of King's Bench ruled that it was unlawful for a slave to be forcibly taken abroad. The case has since been misrepresented as finding that slavery was unlawful in England (although not elsewhere in the British Empire). A similar case, that of Joseph Knight, took place in Scotland five years later and ruled slavery to be contrary to the law of Scotland.
Following the work of campaigners in the United Kingdom, such as William Wilberforce, Henry Dundas, 1st Viscount Melville and Thomas Clarkson, who founded the Society for Effecting the Abolition of the Slave Trade (Abolition Society) in May 1787, the Act for the Abolition of the Slave Trade was passed by Parliament on 25 March 1807, coming into effect the following year. The act imposed a fine of £100 for every slave found aboard a British ship. The intention was to outlaw entirely the Atlantic slave trade within the whole British Empire.[citation needed]
The significance of the abolition of the British slave trade lay in the number of people hitherto sold and carried by British slave vessels. Britain shipped 2,532,300 Africans across the Atlantic, equalling 41% of the total transport of 6,132,900 individuals. This made the British empire the biggest slave-trade contributor in the world due to the magnitude of the empire, which made the abolition act all the more damaging to the global trade of slaves.[399] Britain used its diplomatic influence to press other nations into treaties to ban their slave trade and to give the Royal Navy the right to interdict slave ships sailing under their national flag.[400]
The Slavery Abolition Act, passed on 1 August 1833, outlawed slavery itself throughout the British Empire, with the exception of India. On 1 August 1834 slaves became indentured to their former owners in an apprenticeship system for six years. Full emancipation was granted ahead of schedule on 1 August 1838.[401] Britain abolished slavery in both Hindu and Muslim India with the Indian Slavery Act, 1843.[402]
The Society for the Mitigation and Gradual Abolition of Slavery Throughout the British Dominions (later London Anti-slavery Society ), was founded in 1823, and existed until 1838.[403]
Domestic slavery practised by the educated African coastal elites (as well as interior traditional rulers) in Sierra Leone was abolished in 1928. A study found practices of domestic slavery still widespread in rural areas in the 1970s.[404][405]
The British and Foreign Anti-Slavery Society, founded in 1839 and having gone several name changes since, still exists as Anti-Slavery International.[406]
There were slaves in Metropolitan France (especially in trade ports such as Nantes or Bordeaux).,[citation needed] but the institution was never officially authorized there. The legal case of Jean Boucaux in 1739 clarified the unclear legal position of possible slaves in France, and was followed by laws that established registers for slaves in mainland France, who were limited to a three-year stay, for visits or learning a trade. Unregistered "slaves" in France were regarded as free. However, slavery was of vital importance to the economy of France's Caribbean possessions, especially Saint-Domingue.
In 1793, influenced by the French Declaration of the Rights of Man and of the Citizen of August 1789 and alarmed as the massive slave revolt of August 1791 that had become the Haitian Revolution threatened to ally itself with the British, the Revolutionary French commissioners Léger-Félicité Sonthonax and Étienne Polverel declared general emancipation to reconcile them with France. In Paris, on 4 February 1794, Abbé Grégoire and the Convention ratified this action by officially abolishing slavery in all French territories outside mainland France, freeing all the slaves both for moral and security reasons.
Napoleon came to power in 1799 and soon had grandiose plans for the French sugar colonies; to achieve them he reintroduced slavery. Napoleon's major adventure into the Caribbean—sending 30,000 troops in 1802 to retake Saint Domingue (Haiti) from ex-slaves under Toussaint L'Ouverture who had revolted. Napoleon wanted to preserve France's financial benefits from the colony's sugar and coffee crops; he then planned to establish a major base at New Orleans. He therefore re-established slavery in Haiti and Guadeloupe, where it had been abolished after rebellions. Slaves and black freedmen fought the French for their freedom and independence. Revolutionary ideals played a central role in the fighting[citation needed] for it was the slaves and their allies who were fighting for the revolutionary ideals of freedom and equality, while the French troops under General Charles Leclerc fought to restore the order of the ancien régime. The goal of re-establishing slavery explicitly contradicted the ideals of the French Revolution. The French soldiers were unable to cope with tropical diseases, and most died of yellow fever. Slavery was reimposed in Guadeloupe but not in Haiti, which became an independent black republic.[407] Napoleon's vast colonial dreams for Egypt, India, the Caribbean, Louisiana, and even Australia were all doomed for lack of a fleet capable of matching Britain's Royal Navy. Realizing the fiasco Napoleon liquidated the Haiti project, brought home the survivors and sold off the huge Louisiana territory to the US in 1803.[408]
In 1794 slavery was abolished in the French Empire. After seizing Lower Egypt in 1798, Napoleon Bonaparte issued a proclamation in Arabic, declaring all men to be free and equal. However, the French bought males as soldiers and females as concubines. Napoleon personally opposed the abolition and restored colonial slavery in 1802, a year after the capitulation of his troops in Egypt.[409]
Napoleon decreed the abolition of the slave trade upon his returning from Elba in an attempt to appease Britain. His decision was confirmed by the Treaty of Paris on 20 November 1815 and by order of Louis XVIII on 8 January 1817. However, trafficking continued despite sanctions.[410]
Slavery in the French colonies was finally abolished in 1848, three months after the beginning of the revolution against the July Monarchy. It was in large part the result of the tireless 18-year campaign of Victor Schœlcher. On 3 March 1848, he had been appointed under-secretary of the navy, and caused a decree to be issued by the provisional government which acknowledged the principle of the enfranchisement of the slaves through the French possessions. He also wrote the decree of 27 April 1848 in which the French government announced that slavery was abolished in all of its colonies.[citation needed]
In 1688, four German Quakers in Germantown presented a protest against the institution of slavery to their local Quaker Meeting. It was ignored for 150 years but in 1844 it was rediscovered and was popularized by the abolitionist movement. The 1688 Petition was the first American public document of its kind to protest slavery, and in addition was one of the first public documents to define universal human rights.
The American Colonization Society, the primary vehicle for returning black Americans to greater freedom in Africa, established the colony of Liberia in 1821–23, on the premise that former American slaves would have greater freedom and equality there.[411] 
Various state colonization societies also had African colonies which were later merged with Liberia, including the Republic of Maryland, Mississippi-in-Africa, and Kentucky in Africa. These societies assisted in the movement of thousands of African Americans to Liberia, with ACS founder Henry Clay stating; "unconquerable prejudice resulting from their color, they never could amalgamate with the free whites of this country. It was desirable, therefore, as it respected them, and the residue of the population of the country, to drain them off". Abraham Lincoln, an enthusiastic supporter of Clay, adopted his position on returning the blacks to their own land.[412]
Slaves in the United States who escaped ownership would often make their way to Canada via the "Underground Railroad". The more famous of the African American abolitionists include former slaves Harriet Tubman, Sojourner Truth and Frederick Douglass. Many more people who opposed slavery and worked for abolition were northern whites, such as William Lloyd Garrison and John Brown. Slavery was legally abolished in 1865 by the Thirteenth Amendment to the United States Constitution.
While abolitionists agreed on the evils of slavery, there were differing opinions on what should happen after African Americans were freed. By the time of Emancipation, African-Americans were now native to the United States and did not want to leave. Most believed that their labor had made the land theirs as well as that of the whites.[413]
The Declaration of the Powers, on the Abolition of the Slave Trade, of 8 February 1815 (Which also formed ACT, No. XV. of the Final Act of the Congress of Vienna of the same year) included in its first sentence the concept of the "principles of humanity and universal morality" as justification for ending a trade that was "odious in its continuance".[414]
The 1926 Slavery Convention, an initiative of the League of Nations, was a turning point in banning global slavery. Article 4 of the Universal Declaration of Human Rights, adopted in 1948 by the UN General Assembly, explicitly banned slavery. The United Nations 1956 Supplementary Convention on the Abolition of Slavery was convened to outlaw and ban slavery worldwide, including child slavery. In December 1966, the UN General Assembly adopted the International Covenant on Civil and Political Rights, which was developed from the Universal Declaration of Human Rights. Article 4 of this international treaty bans slavery. The treaty came into force in March 1976 after it had been ratified by 35 nations.
As of November 2003, 104 nations had ratified the treaty. However, illegal forced labour involves millions of people in the 21st century, 43% for sexual exploitation and 32% for economic exploitation.[415]
In May 2004, the 22 members of the Arab League adopted the Arab Charter on Human Rights, which incorporated the 1990 Cairo Declaration on Human Rights in Islam,[416] which states:
Human beings are born free, and no one has the right to enslave, humiliate, oppress or exploit them, and there can be no subjugation but to God the Most-High.Currently, the Anti-trafficking Coordination Team Initiative (ACT Team Initiative), a coordinated effort between the U.S. Departments of Justice, Homeland Security, and Labor, addresses human trafficking.[417] The International Labour Organization estimates that there are 20.9 million victims of human trafficking globally, including 5.5 million children, of which 55% are women and girls.[418]
According to the Global Slavery Index, slavery continues into the 21st century. It claims that as of 2018, the countries with the most slaves were: India (8 million), China (3.86 million), Pakistan (3.19 million) and North Korea (2.64 million).[419] The countries with highest prevalence of slavery were North Korea (10.5%) and Eritrea (9.3%).[11]
The history of slavery originally was the history of the government's laws and policies toward slavery, and the political debates about it. Black history was promoted very largely at black colleges. The situation changed dramatically with the coming of the Civil Rights Movement of the 1950s. Attention shifted to the enslaved humans, the free blacks, and the struggles of the black community against adversity.[420]
Peter Kolchin described the state of historiography in the early 20th century as follows:
During the first half of the twentieth century, a major component of this approach was often simply racism, manifest in the belief that blacks were, at best, imitative of whites. Thus Ulrich B. Phillips, the era's most celebrated and influential expert on slavery, combined a sophisticated portrait of the white planters' life and behavior with crude passing generalizations about the life and behavior of their black slaves.[421]Historians James Oliver Horton and Lois E. Horton described Phillips' mindset, methodology and influence:
His portrayal of blacks as passive, inferior people, whose African origins made them uncivilized, seemed to provide historical evidence for the theories of racial inferiority that supported racial segregation. Drawing evidence exclusively from plantation records, letters, southern newspapers, and other sources reflecting the slaveholder's point of view, Phillips depicted slavemasters who provided for the welfare of their slaves and contended that true affection existed between master and slave.[422]The racist attitude concerning slaves carried over into the historiography of the Dunning School of Reconstruction era history, which dominated in the early 20th century. Writing in 2005, the historian Eric Foner states:
Their account of the era rested, as one member of the Dunning school put it, on the assumption of "negro incapacity." Finding it impossible to believe that blacks could ever be independent actors on the stage of history, with their own aspirations and motivations, Dunning et al. portrayed African Americans either as "children", ignorant dupes manipulated by unscrupulous whites, or as savages, their primal passions unleashed by the end of slavery.[423]Beginning in the 1950s, historiography moved away from the tone of the Phillips era. Historians still emphasized the slave as an object. Whereas Phillips presented the slave as the object of benign attention by the owners, historians such as Kenneth Stampp emphasized the mistreatment and abuse of the slave.[424]
In the portrayal of the slave as a victim, the historian Stanley M. Elkins in his 1959 work Slavery: A Problem in American Institutional and Intellectual Life compared the effects of United States slavery to that resulting from the brutality of the Nazi concentration camps. He stated the institution destroyed the will of the slave, creating an "emasculated, docile Sambo" who identified totally with the owner. Elkins' thesis was challenged by historians. Gradually historians recognized that in addition to the effects of the owner-slave relationship, slaves did not live in a "totally closed environment but rather in one that permitted the emergence of enormous variety and allowed slaves to pursue important relationships with persons other than their master, including those to be found in their families, churches and communities."[425]
Economic historians Robert W. Fogel and Stanley L. Engerman in the 1970s, through their work Time on the Cross, portrayed slaves as having internalized the Protestant work ethic of their owners.[426] In portraying the more benign version of slavery, they also argue in their 1974 book that the material conditions under which the slaves lived and worked compared favorably to those of free workers in the agriculture and industry of the time. (This was also an argument of Southerners during the 19th century.)
In the 1970s and 1980s, historians made use of sources such as black music and statistical census data to create a more detailed and nuanced picture of slave life. Relying also on 19th-century autobiographies of ex-slaves (known as slave narratives) and the WPA Slave Narrative Collection, a set of interviews conducted with former slaves in the 1930s by the Federal Writers' Project, historians described slavery as the slaves remembered it. Far from slaves' being strictly victims or content, historians showed slaves as both resilient and autonomous in many of their activities. Despite their exercise of autonomy and their efforts to make a life within slavery, current historians recognize the precariousness of the slave's situation. Slave children quickly learned that they were subject to the direction of both their parents and their owners. They saw their parents disciplined just as they came to realize that they also could be physically or verbally abused by their owners. Historians writing during this era include John Blassingame (Slave Community), Eugene Genovese (Roll, Jordan, Roll), Leslie Howard Owens (This Species of Property), and Herbert Gutman (The Black Family in Slavery and Freedom).[427]
Important work on slavery has continued; for instance, in 2003 Steven Hahn published the Pulitzer Prize-winning account, A Nation under Our Feet: Black Political Struggles in the Rural South from Slavery to the Great Migration, which examined how slaves built community and political understanding while enslaved, so they quickly began to form new associations and institutions when emancipated, including black churches separate from white control. In 2010, Robert E. Wright published a model that explains why slavery was more prevalent in some areas than others (e.g. southern than northern Delaware) and why some firms (individuals, corporations, plantation owners) chose slave labor while others used wage, indentured, or family labor instead.[428]
A national Marist Poll of Americans in 2015 asked, "Was slavery the main reason for the Civil War, or not?" 53% said yes and 41% said not. There were sharp cleavages along lines of region and party. In the South, 49% answered not. Nationwide 55 percent said students should be taught slavery was the reason for the Civil War.[429]
In 2018, a conference at the University of Virginia studied the history of slavery and recent views on it.[430]
One of the most controversial aspects of the British Empire is its role in first promoting and then ending slavery. In the 18th-century British merchant ships were the largest element in the "Middle Passage" which transported millions of slaves to the Western Hemisphere. Most of those who survived the journey wound up in the Caribbean, where the Empire had highly profitable sugar colonies, and the living conditions were bad (the plantation owners lived in Britain). Parliament ended the international transportation of slaves in 1807 and used the Royal Navy to enforce that ban. In 1833 it bought out the plantation owners and banned slavery. Historians before the 1940s argued that moralistic reformers such as William Wilberforce were primarily responsible.[431]
Historical revisionism arrived when West Indian historian Eric Williams, a Marxist, in Capitalism and Slavery (1944), rejected this moral explanation and argued that abolition was now more profitable, for a century of sugarcane raising had exhausted the soil of the islands, and the plantations had become unprofitable. It was more profitable to sell the slaves to the government than to keep up operations. The 1807 prohibition of the international trade, Williams argued, prevented French expansion on other islands. Meanwhile, British investors turned to Asia, where labor was so plentiful that slavery was unnecessary. Williams went on to argue that slavery played a major role in making Britain prosperous. The high profits from the slave trade, he said, helped finance the Industrial Revolution. Britain enjoyed prosperity because of the capital gained from the unpaid work of slaves.[432]
Since the 1970s numerous historians have challenged Williams from various angles and Gad Heuman has concluded, "More recent research has rejected this conclusion; it is now clear that the colonies of the British Caribbean profited considerably during the Revolutionary and Napoleonic Wars."[433][434] In his major attack on the Williams's thesis, Seymour Drescher argues that Britain's abolition of the slave trade in 1807 resulted not from the diminishing value of slavery for Britain but instead from the moral outrage of the British voting public.[435] Critics have also argued that slavery remained profitable in the 1830s because of innovations in agriculture so the profit motive was not central to abolition.[436] Richardson (1998) finds Williams's claims regarding the Industrial Revolution are exaggerated, for profits from the slave trade amounted to less than 1% of domestic investment in Britain. Richardson further challenges claims (by African scholars) that the slave trade caused widespread depopulation and economic distress in Africa—indeed that it caused the "underdevelopment" of Africa. Admitting the horrible suffering of slaves, he notes that many Africans benefited directly because the first stage of the trade was always firmly in the hands of Africans. European slave ships waited at ports to purchase cargoes of people who were captured in the hinterland by African dealers and tribal leaders. Richardson finds that the "terms of trade" (how much the ship owners paid for the slave cargo) moved heavily in favor of the Africans after about 1750. That is, indigenous elites inside West and Central Africa made large and growing profits from slavery, thus increasing their wealth and power.[437]
Economic historian Stanley Engerman finds that even without subtracting the associated costs of the slave trade (e.g., shipping costs, slave mortality, mortality of British people in Africa, defense costs) or reinvestment of profits back into the slave trade, the total profits from the slave trade and of West Indian plantations amounted to less than 5% of the British economy during any year of the Industrial Revolution.[438] Engerman's 5% figure gives as much as possible in terms of benefit of the doubt to the Williams argument, not solely because it does not take into account the associated costs of the slave trade to Britain, but also because it carries the full-employment assumption from economics and holds the gross value of slave trade profits as a direct contribution to Britain's national income.[439] Historian Richard Pares, in an article written before Williams's book, dismisses the influence of wealth generated from the West Indian plantations upon the financing of the Industrial Revolution, stating that whatever substantial flow of investment from West Indian profits into industry there was occurred after emancipation, not before.[440]


The history of Iran, which was commonly known until the mid-20th century as Persia in the Western world, is intertwined with the history of a larger region, also to an extent known as Greater Iran, comprising the area from Anatolia in the west to the borders of Ancient India and the Syr Darya in the east, and from the Caucasus and the Eurasian Steppe in the north to the Persian Gulf and the Gulf of Oman in the south.
Iran is home to one of the world's oldest continuous major civilizations, with historical and urban settlements dating back to 7000 BC.[1] The south-western and western part of the Iranian plateau participated in the traditional Ancient Near East with Elam (3200–539 BC), from the Early Bronze Age, and later with various other peoples, such as the Kassites, Mannaeans, and Gutians. Georg Wilhelm Friedrich Hegel calls the Persians the "first Historical People".[2] The Medes unified Iran as a nation and empire in 625 BC.[3] The Achaemenid Empire (550–330 BC), founded by Cyrus the Great, was the first true global superpower state[4] and it ruled from the Balkans to North Africa and also Central Asia, spanning three continents, from their seat of power in Persis (Persepolis). It was the largest empire yet seen and the first world empire.[5] The Achaemenid Empire was the only civilization in all of history to connect over 40% of the global population, accounting for approximately 49.4 million of the world's 112.4 million people in around 480 BC.[6] They were succeeded by the Seleucid, Parthian, and Sasanian Empires, who successively governed Iran for almost 1,000 years and made Iran once again as a leading power in the world. Persia's arch-rival was the Roman Empire and its successor, the Byzantine Empire.
The Iranian Empire proper begins in the Iron Age, following the influx of Iranian peoples. Iranian people gave rise to the Medes, the Achaemenid, Parthian, and Sasanian Empires of classical antiquity.
Once a major empire, Iran has endured invasions too, by the Macedonians, Arabs, Turks, and the Mongols. Iran has continually reasserted its national identity throughout the centuries and has developed as a distinct political and cultural entity.
The Muslim conquest of Persia (633–654) ended the Sasanian Empire and is a turning point in Iranian history. Islamization of Iran took place during the eighth to tenth centuries, leading to the eventual decline of Zoroastrianism in Iran as well as many of its dependencies. However, the achievements of the previous Persian civilizations were not lost but were to a great extent absorbed by the new Islamic polity and civilization.
Iran, with its long history of early cultures and empires, had suffered particularly hard during the late Middle Ages and the early modern period. Many invasions of nomadic tribes, whose leaders became rulers in this country, affected it negatively.[7]
Iran was reunified as an independent state in 1501 by the Safavid dynasty, which set Shia Islam as the empire's official religion,[8] marking one of the most important turning points in the history of Islam.[9] Functioning again as a leading world power, this time amongst the neighbouring Ottoman Empire, its arch-rival for centuries, Iran had been a monarchy ruled by an emperor almost without interruption from 1501 until the 1979 Iranian Revolution, when Iran officially became an Islamic republic on 1 April 1979.[10][11]
Over the course of the first half of the 19th century, Iran lost many of its territories in the Caucasus, which had been a part of Iran for centuries,[12] comprising modern-day Eastern Georgia, Dagestan, Republic of Azerbaijan, and Armenia, to its rapidly expanding and emerging rival neighbor, the Russian Empire, following the Russo-Persian Wars between 1804–1813 and 1826–1828.[13]
The earliest archaeological artifacts in Iran were found in the Kashafrud and Ganj Par sites that are thought to date back to 10,000 years ago in the Middle Paleolithic.[14] Mousterian stone tools made by Neanderthals have also been found.[15] There are more cultural remains of Neanderthals dating back to the Middle Paleolithic period, which mainly have been found in the Zagros region and fewer in central Iran at sites such as Kobeh, Kunji, Bisitun Cave, Tamtama, Warwasi, and Yafteh Cave.[16] In 1949, a Neanderthal radius was discovered by Carleton S. Coon in Bisitun Cave.[17] Evidence for Upper Paleolithic and Epipaleolithic periods are known mainly from the Zagros Mountains in the caves of Kermanshah and Khorramabad and a few number of sites in the Alborz and Central Iran. During this time, people began creating rock art.[citation needed]
Early agricultural communities such as Chogha Golan in 10,000 BC[18][19] along with settlements such as Chogha Bonut (the earliest village in Elam) in 8000 BC,[20][21] began to flourish in and around the Zagros Mountains region in western Iran.[22] Around about the same time, the earliest-known clay vessels and modelled human and animal terracotta figurines were produced at Ganj Dareh, also in western Iran.[22] There are also 10,000-year-old human and animal figurines from Tepe Sarab in Kermanshah Province among many other ancient artefacts.[15]
The south-western part of Iran was part of the Fertile Crescent where most of humanity's first major crops were grown, in villages such as Susa (where a settlement was first founded possibly as early as 4395 cal BC)[23] and settlements such as Chogha Mish, dating back to 6800 BC;[24][25] there are 7,000-year-old jars of wine excavated in the Zagros Mountains[26] (now on display at the University of Pennsylvania) and ruins of 7000-year-old settlements such as Tepe Sialk are further testament to that. The two main Neolithic Iranian settlements were the Zayandeh River Culture and Ganj Dareh.[citation needed]
Parts of what is modern-day northwestern Iran was part of the Kura–Araxes culture (circa 3400 BC—ca. 2000 BC), that stretched up into the neighbouring regions of the Caucasus and Anatolia.[27][28]
Susa is one of the oldest-known settlements of Iran and the world. Based on C14 dating, the time of the foundation of the city is as early as 4395 BC,[29] a time right after the establishment of the ancient Sumerian city of Uruk in 4500. The general perception among archaeologists is that Susa was an extension of the Sumerian city-state of Uruk, hence incorporating many aspects of Mesopotamian culture.[30][31] In its later history, Susa became the capital of Elam, which emerged as a state founded 4000.[29] There are also dozens of prehistoric sites across the Iranian plateau pointing to the existence of ancient cultures and urban settlements in the fourth millennium BC,[24] One of the earliest civilizations on the Iranian plateau was the Jiroft culture in southeastern Iran in the province of Kerman.
It is one of the most artefact-rich archaeological sites in the Middle East. Archaeological excavations in Jiroft led to the discovery of several objects belonging to the 4th millennium BC.[32] There is a large quantity of objects decorated with highly distinctive engravings of animals, mythological figures, and architectural motifs. The objects and their iconography are considered unique. Many are made from chlorite, a grey-green soft stone; others are in copper, bronze, terracotta, and even lapis lazuli. Recent excavations at the sites have produced the world's earliest inscription which pre-dates Mesopotamian inscriptions.[33][34]
There are records of numerous other ancient civilizations on the Iranian plateau before the emergence of Iranian peoples during the Early Iron Age. The Early Bronze Age saw the rise of urbanization into organized city-states and the invention of writing (the Uruk period) in the Near East. While Bronze Age Elam made use of writing from an early time, the Proto-Elamite script remains undeciphered, and records from Sumer pertaining to Elam are scarce.
Records become more tangible with the rise of the Neo-Assyrian Empire and its records of incursions from the Iranian plateau.
As early as the 20th century BC, tribes came to the Iranian plateau from the Pontic–Caspian steppe. The arrival of Iranians on the Iranian plateau forced the Elamites to relinquish one area of their empire after another and to take refuge in Elam, Khuzestan and the nearby area, which only then became coterminous with Elam.[35] Bahman Firuzmandi say that the southern Iranians might be intermixed with the Elamite peoples living in the plateau.[36]
By the mid-first millennium BC, Medes, Persians, and Parthians populated the Iranian plateau. Until the rise of the Medes, they all remained under Assyrian domination, like the rest of the Near East. In the first half of the first millennium BC, parts of what is now Iranian Azerbaijan were incorporated into Urartu.
The tomb of Cyrus the Great
Ruins of the Gate of All Nations, Persepolis
Ruins of the Apadana, Persepolis
Depiction of united Medes and Persians at the Apadana, Persepolis
Ruins of the Tachara, Persepolis
In 646 BC, Assyrian king Ashurbanipal sacked Susa, which ended Elamite supremacy in the region.[37] For over 150 years Assyrian kings of nearby Northern Mesopotamia had been wanting to conquer Median tribes of Western Iran.[38] Under pressure from Assyria, the small kingdoms of the western Iranian plateau coalesced into increasingly larger and more centralized states.[37]
In the second half of the seventh century BC, the Medes gained their independence and were united by Deioces. In 612 BC, Cyaxares, Deioces' grandson, and the Babylonian king Nabopolassar invaded Assyria and laid siege to and eventually destroyed Nineveh, the Assyrian capital, which led to the fall of the Neo-Assyrian Empire.[39] Urartu was later on conquered and dissolved as well by the Medes.[40][41] The Medes are credited with founding Iran as a nation and empire, and established the first Iranian empire, the largest of its day until Cyrus the Great established a unified empire of the Medes and Persians, leading to the Achaemenid Empire (c.550–330 BC).
Cyrus the Great overthrew, in turn, the Median, Lydian, and Neo-Babylonian Empires, creating an empire far larger than Assyria. He was better able, through more benign policies, to reconcile his subjects to Persian rule; the longevity of his empire was one result. The Persian king, like the Assyrian, was also "King of Kings", xšāyaθiya xšāyaθiyānām (shāhanshāh in modern Persian) – "great king", Megas Basileus, as known by the Greeks.
Cyrus's son, Cambyses II, conquered the last major power of the region, ancient Egypt, causing the collapse of the Twenty-sixth Dynasty of Egypt. Since he became ill and died before, or while, leaving Egypt, stories developed, as related by Herodotus, that he was struck down for impiety against the ancient Egyptian deities. The winner, Darius I, based his claim on membership in a collateral line of the Achaemenid Empire.
Darius' first capital was at Susa, and he started the building program at Persepolis. He rebuilt a canal between the Nile and the Red Sea, a forerunner of the modern Suez Canal. He improved the extensive road system, and it is during his reign that mentions are first made of the Royal Road (shown on map), a great highway stretching all the way from Susa to Sardis with posting stations at regular intervals. Major reforms took place under Darius. Coinage, in the form of the daric (gold coin) and the shekel (silver coin) was standardized (coinage had already been invented over a century before in Lydia c. 660 BC but not standardized),[42] and administrative efficiency increased.
The Old Persian language appears in royal inscriptions, written in a specially adapted version of the cuneiform script. Under Cyrus the Great and Darius I, the Persian Empire eventually became the largest empire in human history up until that point, ruling and administrating over most of the then known world,[43] as well as spanning the continents of Europe, Asia, and Africa. The greatest achievement was the empire itself. The Persian Empire represented the world's first superpower[44][45] that was based on a model of tolerance and respect for other cultures and religions.[46]
In the late sixth century BC, Darius launched his European campaign, in which he defeated the Paeonians, conquered Thrace, and subdued all coastal Greek cities, as well as defeating the European Scythians around the Danube river.[47] In 512/511, Macedon became a vassal kingdom of Persia.[47]
In 499 BC, Athens lent support to a revolt in Miletus, which resulted in the sacking of Sardis. This led to an Achaemenid campaign against mainland Greece known as the Greco-Persian Wars, which lasted the first half of the 5th century BC, and is known as one of the most important wars in European history. In the First Persian invasion of Greece, the Persian general Mardonius re-subjugated Thrace and made Macedon a full part of Persia.[47] The war eventually turned out in defeat, however. Darius' successor Xerxes I launched the Second Persian invasion of Greece. At a crucial moment in the war, about half of mainland Greece was overrun by the Persians, including all territories to the north of the Isthmus of Corinth,[48][49] however, this was also turned out in a Greek victory, following the battles of Plataea and Salamis, by which Persia lost its footholds in Europe, and eventually withdrew from it.[50] During the Greco-Persian wars, the Persians gained major territorial advantages. They captured and razed Athens twice, once in 480 BC and again in 479 BC. However, after a string of Greek victories the Persians were forced to withdraw, thus losing control of Macedonia, Thrace and Ionia. Fighting continued for several decades after the successful Greek repelling of the Second Invasion with numerous Greek city-states under the Athens' newly formed Delian League, which eventually ended with the peace of Callias in 449 BC, ending the Greco-Persian Wars. In 404 BC, following the death of Darius II, Egypt rebelled under Amyrtaeus. Later pharaohs successfully resisted Persian attempts to reconquer Egypt until 343 BC, when Egypt was reconquered by Artaxerxes III.
From 334 BC to 331 BC, Alexander the Great, defeated Darius III in the battles of Granicus, Issus and Gaugamela, swiftly conquering the Persian Empire by 331 BC. Alexander's empire broke up shortly after his death, and Alexander's general, Seleucus I Nicator, tried to take control of Iran, Mesopotamia, and later Syria and Anatolia. His empire was the Seleucid Empire. He was killed in 281 BC by Ptolemy Keraunos.
Greek language, philosophy, and art came with the colonists. During the Seleucid era, Greek became the common tongue of diplomacy and literature throughout the empire.
The Parthian Empire, ruled by the Parthians, a group of northwestern Iranian people, was the realm of the Arsacid dynasty, who reunited and governed the Iranian plateau after the Parni conquest of Parthia and defeating the Seleucid Empire in the later third century BC, and intermittently controlled Mesopotamia between ca 150 BC and 224 AD. The Parthian Empire quickly included Eastern Arabia.
Parthia was the eastern arch-enemy of the Roman Empire and it limited Rome's expansion beyond Cappadocia (central Anatolia). The Parthian armies included two types of cavalry: the heavily armed and armored cataphracts and the lightly-armed but highly-mobile mounted archers.
For the Romans, who relied on heavy infantry, the Parthians were too hard to defeat, as both types of cavalry were much faster and more mobile than foot soldiers. The Parthian shot used by the Parthian cavalry was most notably feared by the Roman soldiers, which proved pivotal in the crushing Roman defeat at the Battle of Carrhae. On the other hand, the Parthians found it difficult to occupy conquered areas as they were unskilled in siege warfare. Because of these weaknesses, neither the Romans nor the Parthians were able completely to annex each other's territory.
The Parthian empire subsisted for five centuries, longer than most Eastern Empires. The end of this empire came at last in 224 AD, when the empire's organization had loosened and the last king was defeated by one of the empire's vassal peoples, the Persians under the Sasanians. However, the Arsacid dynasty continued to exist for centuries onwards in Armenia, the Iberia, and the Caucasian Albania, which were all eponymous branches of the dynasty.
The first shah of the Sasanian Empire, Ardashir I, started reforming the country economically and militarily. For a period of more than 400 years, Iran was once again one of the leading powers in the world, alongside its neighbouring rival, the Roman and then Byzantine Empires.[51][52] The empire's territory, at its height, encompassed all of today's Iran, Iraq, Azerbaijan, Armenia, Georgia, Abkhazia, Dagestan, Lebanon, Jordan, Palestine, Israel, parts of Afghanistan, Turkey, Syria, parts of Pakistan, Central Asia, Eastern Arabia, and parts of Egypt.
Most of the Sassanian Empire's lifespan was overshadowed by the frequent Byzantine–Sasanian wars, a continuation of the Roman–Parthian Wars and the all-comprising Roman–Persian Wars; the last was the longest-lasting conflict in human history. Started in the first century BC by their predecessors, the Parthians, and Romans, the last Roman–Persian War was fought in the seventh century. The Persians defeated the Romans at the Battle of Edessa in 260 and took emperor Valerian prisoner for the remainder of his life.
Eastern Arabia was conquered early on. During Khosrow II's rule in 590–628, Egypt, Jordan, Palestine and Lebanon were also annexed to the Empire. The Sassanians called their empire Erânshahr ("Dominion of the Aryans", i.e., of Iranians).[53]
A chapter of Iran's history followed after roughly six hundred years of conflict with the Roman Empire. During this time, the Sassanian and Romano-Byzantine armies clashed for influence in Anatolia, the western Caucasus (mainly Lazica and the Kingdom of Iberia; modern-day Georgia and Abkhazia), Mesopotamia, Armenia and the Levant. Under Justinian I, the war came to an uneasy peace with payment of tribute to the Sassanians.
However, the Sasanians used the deposition of the Byzantine emperor Maurice as a casus belli to attack the Empire. After many gains, the Sassanians were defeated at Issus, Constantinople, and finally Nineveh, resulting in peace. With the conclusion of the over 700 years lasting Roman–Persian Wars through the climactic Byzantine–Sasanian War of 602–628, which included the very siege of the Byzantine capital of Constantinople, the war-exhausted Persians lost the Battle of al-Qādisiyyah (632) in Hilla (present-day Iraq) to the invading Muslim forces.
The Sasanian era, encompassing the length of Late Antiquity, is considered to be one of the most important and influential historical periods in Iran, and had a major impact on the world. In many ways, the Sassanian period witnessed the highest achievement of Persian civilization and constitutes the last great Iranian Empire before the adoption of Islam. Persia influenced Roman civilization considerably during Sassanian times,[54] their cultural influence extending far beyond the empire's territorial borders, reaching as far as Western Europe,[55] Africa,[56] China and India[57] and also playing a prominent role in the formation of both European and Asiatic medieval art.[58]
This influence carried forward to the Muslim world. The dynasty's unique and aristocratic culture transformed the Islamic conquest and destruction of Iran into a Persian Renaissance.[55] Much of what later became known as Islamic culture, architecture, writing, and other contributions to civilization, were taken from the Sassanian Persians into the broader Muslim world.[59]
In 633, when the Sasanian king Yazdegerd III was ruling over Iran, the Muslims under Umar invaded the country right after it had been in a bloody civil war. Several Iranian nobles and families such as king Dinar of the House of Karen, and later Kanarangiyans of Khorasan, mutinied against their Sasanian overlords. Although the House of Mihran had claimed the Sasanian throne under the two prominent generals Bahrām Chōbin and Shahrbaraz, it remained loyal to the Sasanians during their struggle against the Arabs, but the Mihrans were eventually betrayed and defeated by their own kinsmen, the House of Ispahbudhan, under their leader Farrukhzad, who had mutinied against Yazdegerd III.
Yazdegerd III, fled from one district to another until a local miller killed him for his purse at Merv in 651.[60] By 674, Muslims had conquered Greater Khorasan (which included modern Iranian Khorasan province and modern Afghanistan and parts of Transoxiana).
The Muslim conquest of Persia ended the Sasanian Empire and led to the eventual decline of the Zoroastrian religion in Persia. Over time, the majority of Iranians converted to Islam. Most of the aspects of the previous Persian civilizations were not discarded but were absorbed by the new Islamic polity. As Bernard Lewis has commented:
These events have been variously seen in Iran: by some as a blessing, the advent of the true faith, the end of the age of ignorance and heathenism; by others as a humiliating national defeat, the conquest and subjugation of the country by foreign invaders. Both perceptions are of course valid, depending on one's angle of vision.[61]After the fall of the Sasanian Empire in 651, the Arabs of the Umayyad Caliphate adopted many Persian customs, especially the administrative and the court mannerisms. Arab provincial governors were undoubtedly either Persianized Arameans or ethnic Persians; certainly Persian remained the language of official business of the caliphate until the adoption of Arabic toward the end of the seventh century,[62] when in 692 minting began at the capital, Damascus. The new Islamic coins evolved from imitations of Sasanian coins (as well as Byzantine), and the Pahlavi script on the coinage was replaced with Arabic alphabet.
During the Umayyad Caliphate, the Arab conquerors imposed Arabic as the primary language of the subject peoples throughout their empire. Al-Hajjaj ibn Yusuf, who was not happy with the prevalence of the Persian language in the divan, ordered the official language of the conquered lands to be replaced by Arabic, sometimes by force.[63] In al-Biruni's From The Remaining Signs of Past Centuries for example it is written:
When Qutaibah bin Muslim under the command of Al-Hajjaj bin Yousef was sent to Khwarazmia with a military expedition and conquered it for the second time, he swiftly killed whoever wrote the Khwarazmian native language that knew of the Khwarazmian heritage, history, and culture. He then killed all their Zoroastrian priests and burned and wasted their books, until gradually the illiterate only remained, who knew nothing of writing, and hence their history was mostly forgotten."[64]There are a number of historians who see the rule of the Umayyads as setting up the "dhimmah" to increase taxes from the dhimmis to benefit the Muslim Arab community financially and by discouraging conversion.[65] Governors lodged complaints with the caliph when he enacted laws that made conversion easier, depriving the provinces of revenues.
In the 7th century, when many non-Arabs such as Persians entered Islam, they were recognized as mawali ("clients") and treated as second-class citizens by the ruling Arab elite until the end of the Umayyad Caliphate. During this era, Islam was initially associated with the ethnic identity of the Arab and required formal association with an Arab tribe and the adoption of the client status of mawali.[65] The half-hearted policies of the late Umayyads to tolerate non-Arab Muslims and Shias had failed to quell unrest among these minorities.
However, all of Iran was still not under Arab control, and the region of Daylam was under the control of the Daylamites, while Tabaristan was under Dabuyid and Paduspanid control, and the Mount Damavand region under Masmughans of Damavand. The Arabs had invaded these regions several times but achieved no decisive result because of the inaccessible terrain of the regions. The most prominent ruler of the Dabuyids, known as Farrukhan the Great (r. 712–728), managed to hold his domains during his long struggle against the Arab general Yazid ibn al-Muhallab, who was defeated by a combined Dailamite-Dabuyid army, and was forced to retreat from Tabaristan.[66]
With the death of the Umayyad Caliph Hisham ibn Abd al-Malik in 743, the Islamic world was launched into civil war. Abu Muslim was sent to Khorasan by the Abbasid Caliphate initially as a propagandist and then to revolt on their behalf. He took Merv defeating the Umayyad governor there Nasr ibn Sayyar. He became the de facto Abbasid governor of Khurasan. During the same period, the Dabuyid ruler Khurshid declared independence from the Umayyads but was shortly forced to recognize Abbasid authority. In 750, Abu Muslim became the leader of the Abbasid army and defeated the Umayyads at the Battle of the Zab. Abu Muslim stormed Damascus, the capital of the Umayyad caliphate, later that year.
The Abbasid army consisted primarily of Khorasanians and was led by an Iranian general, Abu Muslim Khorasani. It contained both Iranian and Arab elements, and the Abbasids enjoyed both Iranian and Arab support. The Abbasids overthrew the Umayyads in 750.[67] According to Amir Arjomand, the Abbasid Revolution essentially marked the end of the Arab empire and the beginning of a more inclusive, multi-ethnic state in the Middle East.[68]
One of the first changes the Abbasids made after taking power from the Umayyads was to move the empire's capital from Damascus, in the Levant, to Iraq. The latter region was influenced by Persian history and culture, and moving the capital was part of the Persian mawali demand for Arab influence in the empire. The city of Baghdad was constructed on the Tigris River, in 762, to serve as the new Abbasid capital.[69]
The Abbasids established the position of vizier like Barmakids in their administration, which was the equivalent of a "vice-caliph", or second-in-command. Eventually, this change meant that many caliphs under the Abbasids ended up in a much more ceremonial role than ever before, with the vizier in real power. A new Persian bureaucracy began to replace the old Arab aristocracy, and the entire administration reflected these changes, demonstrating that the new dynasty was different in many ways from the Umayyads.[69]
By the 9th century, Abbasid control began to wane as regional leaders sprang up in the far corners of the empire to challenge the central authority of the Abbasid caliphate.[69] The Abbasid caliphs began enlisting mamluks, Turkic-speaking warriors, who had been moving out of Central Asia into Transoxiana as slave warriors as early as the 9th century. Shortly thereafter the real power of the Abbasid caliphs began to wane; eventually, they became religious figureheads while the warrior slaves ruled.[67]
The 9th century also saw the revolt by native Zoroastrians, known as the Khurramites, against oppressive Arab rule. The movement was led by a Persian freedom fighter Babak Khorramdin. Babak's Iranianizing[70] rebellion, from its base in Azerbaijan in northwestern Iran,[71] called for a return of the political glories of the Iranian[72] past. The Khorramdin rebellion of Babak spread to the Western and Central parts of Iran and lasted more than twenty years before it was defeated when Babak was betrayed by Afshin, a senior general of the Abbasid Caliphate.
As the power of the Abbasid caliphs diminished, a series of dynasties rose in various parts of Iran, some with considerable influence and power. Among the most important of these overlapping dynasties were the Tahirids in Khorasan (821–873); the Saffarids in Sistan (861–1003, their rule lasted as maliks of Sistan until 1537); and the Samanids (819–1005), originally at Bukhara. The Samanids eventually ruled an area from central Iran to Pakistan.[67]
By the early 10th century, the Abbasids almost lost control to the growing Persian faction known as the Buyid dynasty (934–1062). Since much of the Abbasid administration had been Persian anyway, the Buyids were quietly able to assume real power in Baghdad. The Buyids were defeated in the mid-11th century by the Seljuq Turks, who continued to exert influence over the Abbasids, while publicly pledging allegiance to them. The balance of power in Baghdad remained as such – with the Abbasids in power in name only – until the Mongol invasion of 1258 sacked the city and definitively ended the Abbasid dynasty.[69]

During the Abbassid period an enfranchisement was experienced by the mawali and a shift was made in political conception from that of a primarily Arab empire to one of a Muslim empire[73] and c. 930 a requirement was enacted that required all bureaucrats of the empire be Muslim.[65]Islamization was a long process by which Islam was gradually adopted by the majority population of Iran. Richard Bulliet's "conversion curve" indicates that only about 10% of Iran converted to Islam during the relatively Arab-centric Umayyad period. Beginning in the Abbasid period, with its mix of Persian as well as Arab rulers, the Muslim percentage of the population rose. As Persian Muslims consolidated their rule of the country, the Muslim population rose from approximately 40% in the mid-9th century to close to 100% by the end of the 11th century.[73] Seyyed Hossein Nasr suggests that the rapid increase in conversion was aided by the Persian nationality of the rulers.[74]
Although Persians adopted the religion of their conquerors, over the centuries they worked to protect and revive their distinctive language and culture, a process known as Persianization. Arabs and Turks participated in this attempt.[75][76][77]
In the 9th and 10th centuries, non-Arab subjects of the Ummah created a movement called Shu'ubiyyah in response to the privileged status of Arabs. Most of those behind the movement were Persian, but references to Egyptians, Berbers and Aramaeans are attested.[78] Citing as its basis Islamic notions of equality of races and nations, the movement was primarily concerned with preserving Persian culture and protecting Persian identity, though within a Muslim context.
The Samanid dynasty led the revival of Persian culture and the first important Persian poet after the arrival of Islam, Rudaki, was born during this era and was praised by Samanid kings. The Samanids also revived many ancient Persian festivals. Their successor, the Ghaznawids, who were of non-Iranian Turkic origin, also became instrumental in the revival of Persian culture.[79]
The culmination of the Persianization movement was the Shahnameh, the national epic of Iran, written almost entirely in Persian. This voluminous work, reflects Iran's ancient history, its unique cultural values, its pre-Islamic Zoroastrian religion, and its sense of nationhood. According to Bernard Lewis:[61]
"Iran was indeed Islamized, but it was not Arabized. Persians remained Persians. And after an interval of silence, Iran re-emerged as a separate, different and distinctive element within Islam, eventually adding a new element even to Islam itself. Culturally, politically, and most remarkable of all even religiously, the Iranian contribution to this new Islamic civilization is of immense importance. The work of Iranians can be seen in every field of cultural endeavour, including Arabic poetry, to which poets of Iranian origin composing their poems in Arabic made a very significant contribution. In a sense, Iranian Islam is a second advent of Islam itself, a new Islam sometimes referred to as Islam-i Ajam. It was this Persian Islam, rather than the original Arab Islam, that was brought to new areas and new peoples: to the Turks, first in Central Asia and then in the Middle East in the country which came to be called Turkey, and of course to India. The Ottoman Turks brought a form of Iranian civilization to the walls of Vienna..."The Islamization of Iran was to yield deep transformations within the cultural, scientific, and political structure of Iran's society: The blossoming of Persian literature, philosophy, medicine and art became major elements of the newly forming Muslim civilization. Inheriting a heritage of thousands of years of civilization, and being at the "crossroads of the major cultural highways",[82] contributed to Persia emerging as what culminated into the "Islamic Golden Age". During this period, hundreds of scholars and scientists vastly contributed to technology, science and medicine, later influencing the rise of European science during the Renaissance.[83]
The most important scholars of almost all of the Islamic sects and schools of thought were Persian or lived in Iran, including the most notable and reliable Hadith collectors of Shia and Sunni like Shaikh Saduq, Shaikh Kulainy, Hakim al-Nishaburi, Imam Muslim and Imam Bukhari, the greatest theologians of Shia and Sunni like Shaykh Tusi, Imam Ghazali, Imam Fakhr al-Razi and Al-Zamakhshari, the greatest physicians, astronomers, logicians, mathematicians, metaphysicians, philosophers and scientists like Avicenna, and Nasīr al-Dīn al-Tūsī, the greatest Shaykh of Sufism like Rumi, Abdul-Qadir Gilani.
In 977, a Turkic governor of the Samanids, Sabuktigin, conquered Ghazna (in present-day Afghanistan) and established a dynasty, the Ghaznavids, that lasted to 1186.[67] The Ghaznavid empire grew by taking all of the Samanid territories south of the Amu Darya in the last decade of the 10th century, and eventually occupied parts of Eastern Iran, Afghanistan, Pakistan and north-west India.[69]
The Ghaznavids are generally credited with launching Islam into a mainly Hindu India. The invasion of India was undertaken in 1000 by the Ghaznavid ruler, Mahmud, and continued for several years. They were unable to hold power for long, however, particularly after the death of Mahmud in 1030. By 1040 the Seljuqs had taken over the Ghaznavid lands in Iran.[69]
The Seljuqs, who like the Ghaznavids were Persianate in nature and of Turkic origin, slowly conquered Iran over the course of the 11th century.[67] The dynasty had its origins in the Turcoman tribal confederations of Central Asia and marked the beginning of Turkic power in the Middle East. They established a Sunni Muslim rule over parts of Central Asia and the Middle East from the 11th to 14th centuries. They set up an empire known as Great Seljuq Empire that stretched from Anatolia in the west to western Afghanistan in the east and the western borders of (modern-day) China in the north-east; and was the target of the First Crusade. Today they are regarded as the cultural ancestors of the Western Turks, the present-day inhabitants of Turkey and Turkmenistan, and they are remembered as great patrons of Persian culture, art, literature, and language.[84][85][86]
The founder of the dynasty, Tughril Beg, turned his army against the Ghaznavids in Khorasan. He moved south and then west, conquering but not wasting the cities in his path. In 1055 the caliph in Baghdad gave Tughril Beg robes, gifts, and the title King of the East. Under Tughril Beg's successor, Malik Shah (1072–1092), Iran enjoyed a cultural and scientific renaissance, largely attributed to his brilliant Iranian vizier, Nizam al Mulk. These leaders established the observatory where Omar Khayyám did much of his experimentation for a new calendar, and they built religious schools in all the major towns. They brought Abu Hamid Ghazali, one of the greatest Islamic theologians, and other eminent scholars to the Seljuq capital at Baghdad and encouraged and supported their work.[67]
When Malik Shah I died in 1092, the empire split as his brother and four sons quarreled over the apportioning of the empire among themselves. In Anatolia, Malik Shah I was succeeded by Kilij Arslan I who founded the Sultanate of Rûm and in Syria by his brother Tutush I. In Persia he was succeeded by his son Mahmud I whose reign was contested by his other three brothers Barkiyaruq in Iraq, Muhammad I in Baghdad and Ahmad Sanjar in Khorasan. As Seljuq power in Iran weakened, other dynasties began to step up in its place, including a resurgent Abbasid caliphate and the Khwarezmshahs. The Khwarezmid Empire was a Sunni Muslim Persianate dynasty, of East Turkic origin, that ruled in Central Asia. Originally vassals of the Seljuqs, they took advantage of the decline of the Seljuqs to expand into Iran.[87] In 1194 the Khwarezmshah Ala ad-Din Tekish defeated the Seljuq sultan Toghrul III in battle and the Seljuq empire in Iran collapsed. Of the former Seljuq Empire, only the Sultanate of Rum in Anatolia remained.

A serious internal threat to the Seljuqs during their reign came from the Nizari Ismailis, a secret sect with headquarters at Alamut Castle between Rasht and Tehran. They controlled the immediate area for more than 150 years and sporadically sent out adherents to strengthen their rule by murdering important officials. Several of the various theories on the etymology of the word assassin derive from these killers.[67]Parts of northwestern Iran were conquered in the early 13th century AD by the Kingdom of Georgia, led by Tamar the Great.[88]
The Khwarazmian dynasty only lasted for a few decades, until the arrival of the Mongols. Genghis Khan had unified the Mongols, and under him the Mongol Empire quickly expanded in several directions. In 1218, it bordered Khwarezm. At that time, the Khwarazmian Empire was ruled by Ala ad-Din Muhammad (1200–1220). Muhammad, like Genghis, was intent on expanding his lands and had gained the submission of most of Iran. He declared himself shah and demanded formal recognition from the Abbasid caliph Al-Nasir. When the caliph rejected his claim, Ala ad-Din Muhammad proclaimed one of his nobles caliph and unsuccessfully tried to depose an-Nasir.
The Mongol invasion of Iran began in 1219, after two diplomatic missions to Khwarezm sent by Genghis Khan had been massacred. During 1220–21 Bukhara, Samarkand, Herat, Tus and Nishapur were razed, and the whole populations were slaughtered. The Khwarezm-Shah fled, to die on an island off the Caspian coast.[89]
During the invasion of Transoxiana in 1219, along with the main Mongol force, Genghis Khan used a Chinese specialist catapult unit in battle, they were used again in 1220 in Transoxania. The Chinese may have used the catapults to hurl gunpowder bombs, since they already had them by this time.[90]
While Genghis Khan was conquering Transoxania and Persia, several Chinese who were familiar with gunpowder were serving in Genghis's army.[91] "Whole regiments" entirely made out of Chinese were used by the Mongols to command bomb hurling trebuchets during the invasion of Iran.[92] Historians have suggested that the Mongol invasion had brought Chinese gunpowder weapons to Central Asia. One of these was the huochong, a Chinese mortar.[93] Books written around the area afterward depicted gunpowder weapons which resembled those of China.[94]
Before his death in 1227, Genghis had reached western Azerbaijan, pillaging and burning many cities along the way after entering into Iran from its north east.
The Mongol invasion was by and large disastrous to the Iranians. Although the Mongol invaders eventually converted to Islam and accepted the culture of Iran, the Mongol destruction in Iran and other regions the Islamic heartland (particularly the historical Khorasan region, mainly in Central Asia) marked a major change of direction for the region. Much of the six centuries of Islamic scholarship, culture, and infrastructure was destroyed as the invaders leveled cities, burned libraries, and in some cases replaced mosques with Buddhist temples.[95][96][97]
The Mongols killed many Iranian civilians. Destruction of qanat irrigation systems in the north east of Iran destroyed the pattern of relatively continuous settlements, producing many abandoned towns which were relatively quite good with irrigation and agriculture.[98]
After Genghis's death, Iran was ruled by several Mongol commanders. Genghis' grandson, Hulagu Khan, was tasked with the westward expansion of Mongol dominion. However, by time he ascended to power, the Mongol Empire had already dissolved, dividing into different factions. Arriving with an army, he established himself in the region and founded the Ilkhanate, a breakaway state of the Mongol Empire, which would rule Iran for the next eighty years and become Persian in the process.
Hulagu Khan seized Baghdad in 1258 and put the last Abbasid caliph to death. The westward advance of his forces was stopped by the Mamelukes, however, at the Battle of Ain Jalut in Palestine in 1260. Hulagu's campaigns against the Muslims also enraged Berke, khan of the Golden Horde and a convert to Islam. Hulagu and Berke fought against each other, demonstrating the weakening unity of the Mongol empire.
The rule of Hulagu's great-grandson, Ghazan (1295–1304) saw the establishment of Islam as the state religion of the Ilkhanate. Ghazan and his famous Iranian vizier, Rashid al-Din, brought Iran a partial and brief economic revival. The Mongols lowered taxes for artisans, encouraged agriculture, rebuilt and extended irrigation works, and improved the safety of the trade routes. As a result, commerce increased dramatically.
Items from India, China, and Iran passed easily across the Asian steppes, and these contacts culturally enriched Iran. For example, Iranians developed a new style of painting based on a unique fusion of solid, two-dimensional Mesopotamian painting with the feathery, light brush strokes and other motifs characteristic of China. After Ghazan's nephew Abu Said died in 1335, however, the Ilkhanate lapsed into civil war and was divided between several petty dynasties – most prominently the Jalayirids, Muzaffarids, Sarbadars and Kartids.
The mid-14th-century Black Death killed about 30% of the country's population.[99]
Prior to the rise of the Safavid Empire, Sunni Islam was the dominant religion, accounting for around 90% of the population at the time. According to Mortaza Motahhari the majority of Iranian scholars and masses remained Sunni until the time of the Safavids.[100] The domination of Sunnis did not mean Shia were rootless in Iran. The writers of The Four Books of Shia were Iranian, as well as many other great Shia scholars.
The domination of the Sunni creed during the first nine Islamic centuries characterized the religious history of Iran during this period. There were however some exceptions to this general domination which emerged in the form of the Zaydīs of Tabaristan (see Alid dynasties of northern Iran), the Buyids, the Kakuyids, the rule of Sultan Muhammad Khudabandah (r. Shawwal 703-Shawwal 716/1304–1316) and the Sarbedaran.[101]
Apart from this domination there existed, firstly, throughout these nine centuries, Shia inclinations among many Sunnis of this land and, secondly, original Imami Shiism as well as Zaydī Shiism had prevalence in some parts of Iran. During this period, Shia in Iran were nourished from Kufah, Baghdad and later from Najaf and Hillah.[101] Shiism was the dominant sect in Tabaristan, Qom, Kashan, Avaj and Sabzevar. In many other areas merged population of Shia and Sunni lived together.
During the 10th and 11th centuries, Fatimids sent Ismailis Da'i (missioners) to Iran as well as other Muslim lands. When Ismailis divided into two sects, Nizaris established their base in Iran. Hassan-i Sabbah conquered fortresses and captured Alamut in 1090 AD. Nizaris used this fortress until a Mongol raid in 1256.
After the Mongol raid and fall of the Abbasids, Sunni hierarchies faltered. Not only did they lose the caliphate but also the status of official madhhab. Their loss was the gain of Shia, whose centre wasn't in Iran at that time. Several local Shia dynasties like Sarbadars were established during this time.
The main change occurred in the beginning of the 16th century, when Ismail I founded the Safavid dynasty and initiated a religious policy to recognize Shi'a Islam as the official religion of the Safavid Empire, and the fact that modern Iran remains an officially Shi'ite state is a direct result of Ismail's actions.
Iran remained divided until the arrival of Timur, an Turco-Mongol[102] belonging to the Timurid dynasty. Like its predecessors, the Timurid Empire was also part of the Persianate world. After establishing a power base in Transoxiana, Timur invaded Iran in 1381 and eventually conquered most of it. Timur's campaigns were known for their brutality; many people were slaughtered and several cities were destroyed.[103]
His regime was characterized by tyranny and bloodshed, but also by its inclusion of Iranians in administrative roles and its promotion of architecture and poetry. His successors, the Timurids, maintained a hold on most of Iran until 1452, when they lost the bulk of it to Black Sheep Turkmen. The Black Sheep Turkmen were conquered by the White Sheep Turkmen under Uzun Hasan in 1468; Uzun Hasan and his successors were the masters of Iran until the rise of the Safavids.[103]
Sufi poet Hafez's popularity became firmly established in the Timurid era that saw the compilation and widespread copying of his divan. Sufis were often persecuted by orthodox Muslims who considered their teachings blasphemous. Sufism developed a symbolic language rich with metaphors to obscure poetic references to provocative philosophical teachings. Hafez concealed his own Sufi faith, even as he employed the secret language of Sufism (developed over hundreds of years) in his own work, and he is sometimes credited with having "brought it to perfection".[104] His work was imitated by Jami, whose own popularity grew to spread across the full breadth of the Persianate world.[105]
The Kara Koyunlu were Turkmen[106][107][108][109] tribal federation that ruled over northwestern Iran and surrounding areas from 1374 to 1468 CE. The Kara Koyunlu expanded their conquest to Baghdad, however, internal fighting, defeats by the Timurids, rebellions by the Armenians in response to their persecution,[110] and failed struggles with the Ag Qoyunlu led to their eventual demise.[111]
[check quotation syntax]
Aq Qoyunlu were Turkmen[112][113] under the leadership of the Bayandur tribe,[114] tribal federation of Sunni Muslims who ruled over most of Iran and large parts of surrounding areas from 1378 to 1501 CE. Aq Qoyunlu emerged when Timur granted them all of Diyar Bakr in present-day Turkey. Afterward, they struggled with their rival Oghuz Turks, the Qara Qoyunlu. While the Aq Qoyunlu were successful in defeating Kara Koyunlu, their struggle with the emerging Safavid dynasty led to their downfall.[115]
Persia underwent a revival under the Safavid dynasty (1502–1736), the most prominent figure of which was Shah Abbas I. Some historians credit the Safavid dynasty for founding the modern nation-state of Iran. Iran's contemporary Shia character, and significant segments of Iran's current borders take their origin from this era (e.g. Treaty of Zuhab).
The Safavid dynasty was one of the most significant ruling dynasties of Persia (modern Iran), and "is often considered the beginning of modern Persian history".[116] They ruled one of the greatest Persian empires after the Muslim conquest of Persia[117][118][119][120] and established the Twelver school of Shi'a Islam[8] as the official religion of their empire, marking one of the most important turning points in Muslim history. The Safavids ruled from 1501 to 1722 (experiencing a brief restoration from 1729 to 1736) and at their height, they controlled all of modern Iran, Azerbaijan and Armenia, most of Georgia, the North Caucasus, Iraq, Kuwait and Afghanistan, as well as parts of Turkey, Syria, Pakistan, Turkmenistan and Uzbekistan. Safavid Iran was one of the Islamic "gunpowder empires", along with its neighbours, its archrival and principal enemy the Ottoman Empire, as well as the Mughal Empire.
The Safavid ruling dynasty was founded by Ismāil, who styled himself Shāh Ismāil I.[121] Practically worshipped by his Qizilbāsh followers, Ismāil invaded Shirvan to avenge the death of his father, Shaykh Haydar, who had been killed during his siege of Derbent, in Dagestan. Afterwards he went on a campaign of conquest, and following the capture of Tabriz in July 1501, he enthroned himself as the Shāh of Iran,[122][123][124] minted coins in this name, and proclaimed Shi'ism the official religion of his domain.[8]
Although initially the masters of Azerbaijan and southern Dagestan only, the Safavids had, in fact, won the struggle for power in Persia which had been going on for nearly a century between various dynasties and political forces following the fragmentation of the Kara Koyunlu and the Aq Qoyunlu. A year after his victory in Tabriz, Ismāil proclaimed most of Persia as his domain, and[8] quickly conquered and unified Iran under his rule. Soon afterwards, the new Safavid Empire rapidly conquered regions, nations, and peoples in all directions, including Armenia, Azerbaijan, parts of Georgia, Mesopotamia (Iraq), Kuwait, Syria, Dagestan, large parts of what is now Afghanistan, parts of Turkmenistan, and large chunks of Anatolia, laying the foundation of its multi-ethnic character which would heavily influence the empire itself (most notably the Caucasus and its peoples).
Tahmasp I, the son and successor of Ismail I, carried out multiple invasions in the Caucasus which had been incorporated in the Safavid empire since Shah Ismail I and for many centuries afterwards, and started with the trend of deporting and moving hundreds of thousands of Circassians, Georgians, and Armenians to Iran's heartlands. Initially only solely put in the royal harems, royal guards, and minor other sections of the Empire, Tahmasp believed he could eventually reduce the power of the Qizilbash, by creating and fully integrating a new layer in Iranian society. As Encyclopædia Iranica states, for Tahmasp, the problem circled around the military tribal elite of the empire, the Qizilbash, who believed that physical proximity to and control of a member of the immediate Safavid family guaranteed spiritual advantages, political fortune, and material advancement.[125] With this new Caucasian layer in Iranian society, the undisputed might of the Qizilbash (who functioned much like the ghazis of the neighbouring Ottoman Empire) would be questioned and fully diminished as society would become fully meritocratic.
Shah Abbas I and his successors would significantly expand this policy and plan initiated by Tahmasp, deporting during his reign alone around some 200,000 Georgians, 300,000 Armenians and 100,000–150,000 Circassians to Iran, completing the foundation of a new layer in Iranian society. With this, and the complete systematic disorganisation of the Qizilbash by his personal orders, he eventually fully succeeded in replacing the power of the Qizilbash, with that of the Caucasian ghulams. These new Caucasian elements (the so-called ghilman / غِلْمَان / "servants"), almost always after conversion to Shi'ism depending on given function would be, were unlike the Qizilbash, fully loyal only to the Shah. The other masses of Caucasians were deployed in all other possible functions and positions available in the empire, as well as in the harem, regular military, craftsmen, farmers, etc. This system of mass usage of Caucasian subjects remained to exist until the fall of the Qajar dynasty.
The greatest of the Safavid monarchs, Shah Abbas I the Great (1587–1629) came to power in 1587 aged 16. Abbas I first fought the Uzbeks, recapturing Herat and Mashhad in 1598, which had been lost by his predecessor Mohammad Khodabanda by the Ottoman–Safavid War (1578–1590). Then he turned against the Ottomans, the archrivals of the Safavids, recapturing Baghdad, eastern Iraq and the Caucasian provinces and beyond by 1618. Between 1616 and 1618, following the disobedience of his most loyal Georgian subjects Teimuraz I and Luarsab II, Abbas carried out a punitive campaign in his territories of Georgia, devastating Kakheti and Tbilisi and carrying away 130,000[126] – 200,000[127][128] Georgian captives towards mainland Iran. His new army, which had dramatically been improved with the advent of Robert Shirley and his brothers following the first diplomatic mission to Europe, pitted the first crushing victory over the Safavids' archrivals, the Ottomans in the above-mentioned 1603–1618 war and would surpass the Ottomans in military strength. He also used his new force to dislodge the Portuguese from Bahrain (1602) and Hormuz (1622) with aid of the English navy, in the Persian Gulf.
He expanded commercial links with the Dutch East India Company and established firm links with the European royal houses, which had been initiated by Ismail I earlier on by the Habsburg–Persian alliance. Thus Abbas I was able to break the dependence on the Qizilbash for military might and therefore was able to centralize control.
The Safavid dynasty had already established itself during Shah Ismail I, but under Abbas I it really became a major power in the world along with its archrival the Ottoman Empire, against whom it became able to compete with on equal foot. It also started the promotion of tourism in Iran. Under their rule Persian Architecture flourished again and saw many new monuments in various Iranian cities, of which Isfahan is the most notable example.
Except for Shah Abbas the Great, Shah Ismail I, Shah Tahmasp I, and Shah Abbas II, many of the Safavid rulers were ineffectual, often being more interested in their women, alcohol and other leisure activities. The end of Abbas II's reign in 1666, marked the beginning of the end of the Safavid dynasty. Despite falling revenues and military threats, many of the later shahs had lavish lifestyles. Shah Soltan Hosain (1694–1722) in particular was known for his love of wine and disinterest in governance.[129]
The declining country was repeatedly raided on its frontiers. Finally, Ghilzai Pashtun chieftain named Mir Wais Khan began a rebellion in Kandahar and defeated the Safavid army under the Iranian Georgian governor over the region, Gurgin Khan. In 1722, Peter the Great of neighbouring Imperial Russia launched the Russo-Persian War (1722–1723), capturing many of Iran's Caucasian territories, including Derbent, Shaki, Baku, but also Gilan, Mazandaran and Astrabad. At the mids of all chaos, in the same year of 1722, an Afghan army led by Mir Wais' son Mahmud marched across eastern Iran, besieged and took Isfahan. Mahmud proclaimed himself 'Shah' of Persia. Meanwhile, Persia's imperial rivals, the Ottomans and the Russians, took advantage of the chaos in the country to seize more territory for themselves.[130] By these events, the Safavid dynasty had effectively ended. In 1724, conform the Treaty of Constantinople, the Ottomans and the Russians agreed to divide the newly conquered territories of Iran amongst themselves.[131]
Iran's territorial integrity was restored by a native Iranian Turkic Afshar warlord from Khorasan, Nader Shah. He defeated and banished the Afghans, defeated the Ottomans, reinstalled the Safavids on the throne, and negotiated Russian withdrawal from Iran's Caucasian territories, with the Treaty of Resht and Treaty of Ganja. By 1736, Nader had become so powerful he was able to depose the Safavids and have himself crowned shah. Nader was one of the last great conquerors of Asia and briefly presided over what was probably the most powerful empire in the world. To financially support his wars against Persia's arch-rival, the Ottoman Empire, he fixed his sights on the weak but rich Mughal Empire to the east. In 1739, accompanied by his loyal Caucasian subjects including Erekle II,[132][133] he invaded Mughal India, defeated a numerically superior Mughal army in less than three hours, and completely sacked and looted Delhi, bringing back immense wealth to Persia. On his way back, he also conquered all the Uzbek khanates – except for Kokand – and made the Uzbeks his vassals. He also firmly re-established Persian rule over the entire Caucasus, Bahrain, as well as large parts of Anatolia and Mesopotamia. Undefeated for years, his defeat in Dagestan, following guerrilla rebellions by the Lezgins and the assassination attempt on him near Mazandaran is often considered the turning point in Nader's impressive career. To his frustration, the Dagestanis resorted to guerrilla warfare, and Nader with his conventional army could make little headway against them.[134] At the Battle of Andalal and the Battle of Avaria, Nader's army was crushingly defeated and he lost half of his entire force, as well forcing him to flee for the mountains.[135] Though Nader managed to take most of Dagestan during his campaign, the effective guerrilla warfare as deployed by the Lezgins, but also the Avars and Laks made the Iranian re-conquest of the particular North Caucasian region this time a short lived one; several years later, Nader was forced to withdraw. Around the same time, the assassination attempt was made on him near Mazandaran which accelerated the course of history; he slowly grew ill and megalomaniac, blinding his sons whom he suspected of the assassination attempts, and showing increasing cruelty against his subjects and officers. In his later years this eventually provoked multiple revolts and, ultimately, Nader's assassination in 1747.[136]
Nader's death was followed by a period of anarchy in Iran as rival army commanders fought for power. Nader's own family, the Afsharids, were soon reduced to holding on to a small domain in Khorasan. Many of the Caucasian territories broke away in various Caucasian khanates. Ottomans regained lost territories in Anatolia and Mesopotamia. Oman and the Uzbek khanates of Bukhara and Khiva regained independence. Ahmad Shah Durrani, one of Nader's officers, founded an independent state which eventually became modern Afghanistan. Erekle II and Teimuraz II, who, in 1744, had been made the kings of Kakheti and Kartli respectively by Nader himself for their loyal service,[137] capitalized on the eruption of instability, and declared de facto independence. Erekle II assumed control over Kartli after Teimuraz II's death, thus unifying the two as the Kingdom of Kartli-Kakheti, becoming the first Georgian ruler in three centuries to preside over a politically unified eastern Georgia,[138] and due to the frantic turn of events in mainland Iran he would be able to remain de facto autonomous through the Zand period.[139] From his capital Shiraz, Karim Khan of the Zand dynasty ruled "an island of relative calm and peace in an otherwise bloody and destructive period,"[140] however the extent of Zand power was confined to contemporary Iran and parts of the Caucasus. Karim Khan's death in 1779 led to yet another civil war in which the Qajar dynasty eventually triumphed and became kings of Iran. During the civil war, Iran permanently lost Basra in 1779 to the Ottomans, which had been captured during the Ottoman–Persian War (1775–76),[141] and Bahrain to Al Khalifa family after Bani Utbah invasion in 1783.[citation needed]
[check quotation syntax]
Mihr 'Ali (Iranian, active ca. 1800–1830). Portrait of Fath-Ali Shah Qajar. Brooklyn Museum.
Qajar era currency bill with depiction of Naser al-Din Shah Qajar.
A map of Iran under the Qajar dynasty in the 19th century.
A map showing the 19th-century northwestern borders of Iran, comprising modern-day eastern Georgia, Dagestan, Armenia, and the Republic of Azerbaijan, before being ceded to the neighboring Russian Empire by the Russo-Iranian wars.
Agha Mohammad Khan emerged victorious out of the civil war that commenced with the death of the last Zand king. His reign is noted for the reemergence of a centrally led and united Iran. After the death of Nader Shah and the last of the Zands, most of Iran's Caucasian territories had broken away into various Caucasian khanates. Agha Mohammad Khan, like the Safavid kings and Nader Shah before him, viewed the region as no different than the territories in mainland Iran. Therefore, his first objective after having secured mainland Iran, was to reincorpate the Caucasus region into Iran.[142] Georgia was seen as one of the most integral territories.[139] For Agha Mohammad Khan, the resubjugation and reintegration of Georgia into the Iranian Empire was part of the same process that had brought Shiraz, Isfahan, and Tabriz under his rule.[139] As the Cambridge History of Iran states, its permanent secession was inconceivable and had to be resisted in the same way as one would resist an attempt at the separation of Fars or Gilan.[139] It was therefore natural for Agha Mohammad Khan to perform whatever necessary means in the Caucasus in order to subdue and reincorporate the recently lost regions following Nader Shah's death and the demise of the Zands, including putting down what in Iranian eyes was seen as treason on the part of the wali (viceroy) of Georgia, namely the Georgian king Erekle II (Heraclius II) who was appointed viceroy of Georgia by Nader Shah himself.[139]
Agha Mohammad Khan subsequently demanded that Heraclius II renounce its 1783 treaty with Russia, and to submit again to Persian suzerainty,[142] in return for peace and the security of his kingdom. The Ottomans, Iran's neighboring rival, recognized the latter's rights over Kartli and Kakheti for the first time in four centuries.[143] Heraclius appealed then to his theoretical protector, Empress Catherine II of Russia, pleading for at least 3,000 Russian troops,[143] but he was ignored, leaving Georgia to fend off the Persian threat alone.[144] Nevertheless, Heraclius II still rejected the Khan's ultimatum.[145] As a response, Agha Mohammad Khan invaded the Caucasus region after crossing the Aras river, and, while on his way to Georgia, he re-subjugated Iran's territories of the Erivan Khanate, Shirvan, Nakhchivan Khanate, Ganja khanate, Derbent Khanate, Baku khanate, Talysh Khanate, Shaki Khanate, Karabakh Khanate, which comprise modern-day Armenia, Azerbaijan, Dagestan, and Igdir. Having reached Georgia with his large army, he prevailed in the Battle of Krtsanisi, which resulted in the capture and sack of Tbilisi, as well as the effective resubjugation of Georgia.[146][147] Upon his return from his successful campaign in Tbilisi and in effective control over Georgia, together with some 15,000 Georgian captives that were moved back to mainland Iran,[144] Agha Mohammad was formally crowned Shah in 1796 in the Mughan plain, just as his predecessor Nader Shah was about sixty years earlier.
Agha Mohammad Shah was later assassinated while preparing a second expedition against Georgia in 1797 in Shusha[148] (now part of the Republic of Azerbaijan) and the seasoned king Heraclius died early in 1798. The reassertion of Iranian hegemony over Georgia did not last long; in 1799 the Russians marched into Tbilisi.[149] The Russians were already actively occupied with an expansionist policy towards its neighboring empires to its south, namely the Ottoman Empire and the successive Iranian kingdoms since the late 17th/early 18th century. The next two years following Russia's entrance into Tbilisi were a time of confusion, and the weakened and devastated Georgian kingdom, with its capital half in ruins, was easily absorbed by Russia in 1801.[144][145] As Iran could not permit or allow the cession of Transcaucasia and Dagestan, which had been an integral part of Iran for centuries,[12] this would lead directly to the wars of several years later, namely the Russo-Persian Wars of 1804-1813 and 1826–1828. The outcome of these two wars (in the Treaty of Gulistan and the Treaty of Turkmenchay, respectively) proved for the irrevocable forced cession and loss of what is now eastern Georgia, Dagestan, Armenia, and Azerbaijan to Imperial Russia.[150][146]
The area to the north of the river Aras, among which the territory of the contemporary republic of Azerbaijan, eastern Georgia, Dagestan, and Armenia were Iranian territory until they were occupied by Russia in the course of the 19th century.[151][152][153][154][155][156][157]
Painting showing the Battle of Sultanabad, 13 February 1812. State Hermitage Museum.
Storming of Lankaran, 1812. Painted by Franz Roubaud.
Battle of Elisabethpol (Ganja), 1828. Franz Roubaud. Part of the collection of the Museum for History, Baku.
Following the official loss of vast territories in the Caucasus, major demographic shifts were bound to take place. Following the 1804–1814 war, but also per the 1826–1828 war which ceded the last territories, large migrations, so-called Caucasian Muhajirs, set off to migrate to mainland Iran. Some of these groups included the Ayrums, Qarapapaqs, Circassians, Shia Lezgins, and other Transcaucasian Muslims.[158]
After the Battle of Ganja of 1804 during the Russo-Persian War (1804–1813), many thousands of Ayrums and Qarapapaqs were settled in Tabriz. During the remaining part of the 1804–1813 war, as well as through the 1826–1828 war, a large number of the Ayrums and Qarapapaqs that were still remaining in newly conquered Russian territories were settled in and migrated to Solduz (in modern-day Iran's West Azerbaijan province).[159] As the Cambridge History of Iran states; "The steady encroachment of Russian troops along the frontier in the Caucasus, General Yermolov's brutal punitive expeditions and misgovernment, drove large numbers of Muslims, and even some Georgian Christians, into exile in Iran."[160]
From 1864 until the early 20th century, another mass expulsion took place of Caucasian Muslims as a result of the Russian victory in the Caucasian War. Others simply voluntarily refused to live under Christian Russian rule, and thus departed for Turkey or Iran. These migrations once again, towards Iran, included masses of Caucasian Azerbaijanis, other Transcaucasian Muslims, as well as many North Caucasian Muslims, such as Circassians, Shia Lezgins and Laks.[158][161]
Many of these migrants would prove to play a pivotal role in further Iranian history, as they formed most of the ranks of the Persian Cossack Brigade, which was established in the late 19th century.[162] The initial ranks of the brigade would be entirely composed of Circassians and other Caucasian Muhajirs.[162] This brigade would prove decisive in the following decades in Qajar history.
Furthermore, the 1828 Treaty of Turkmenchay included the official rights for the Russian Empire to encourage settling of Armenians from Iran in the newly conquered Russian territories.[163][164] Until the mid-fourteenth century, Armenians had constituted a majority in Eastern Armenia.[165] 
At the close of the fourteenth century, after Timur's campaigns, the Timurid Renaissance flourished, and Islam had become the dominant faith, and Armenians became a minority in Eastern Armenia. [165] After centuries of constant warfare on the Armenian Plateau, many Armenians chose to emigrate and settle elsewhere. Following Shah Abbas I's massive relocation of Armenians and Muslims in 1604–05,[166] their numbers dwindled even further.
At the time of the Russian invasion of Iran, some 80% of the population of Iranian Armenia were Muslims (Persians, Turkics, and Kurds) whereas Christian Armenians constituted a minority of about 20%.[167] As a result of the Treaty of Gulistan (1813) and the Treaty of Turkmenchay (1828), Iran was forced to cede Iranian Armenia (which also constituted the present-day Armenia), to the Russians.[168][169] After the Russian administration took hold of Iranian Armenia, the ethnic make-up shifted, and thus for the first time in more than four centuries, ethnic Armenians started to form a majority once again in one part of historic Armenia.[170] The new Russian administration encouraged the settling of ethnic Armenians from Iran proper and Ottoman Turkey. As a result, by 1832, the number of ethnic Armenians had matched that of the Muslims.[167] It would be only after the Crimean War and the Russo-Turkish War of 1877–1878, which brought another influx of Turkish Armenians, that ethnic Armenians once again established a solid majority in Eastern Armenia.[171] Nevertheless, the city of Erivan retained a Muslim majority up to the twentieth century.[171] According to the traveller H. F. B. Lynch, the city of Erivan was about 50% Armenian and 50% Muslim (Tatars[a] i.e. Azeris and Persians) in the early 1890s.[174]
Fath Ali Shah's reign saw increased diplomatic contacts with the West and the beginning of intense European diplomatic rivalries over Iran. His grandson Mohammad Shah, who succeeded him in 1834, fell under the Russian influence and made two unsuccessful attempts to capture Herat. When Mohammad Shah died in 1848 the succession passed to his son Naser al-Din Shah Qajar, who proved to be the ablest and most successful of the Qajar sovereigns. He founded the first modern hospital in Iran.[175]
The Great Persian Famine of 1870–1871 is believed to have caused the death of two million people.[176]
A new era in the history of Persia dawned with the Persian Constitutional Revolution against the Shah in the late 19th and early 20th centuries. The Shah managed to remain in power, granting a limited constitution in 1906 (making the country a constitutional monarchy). The first Majlis (parliament) was convened on October 7, 1906.
The discovery of petroleum in 1908 by the British in Khuzestan spawned intense renewed interest in Persia by the British Empire (see William Knox D'Arcy and Anglo-Iranian Oil Company, now BP). Control of Persia remained contested between the United Kingdom and Russia, in what became known as The Great Game, and codified in the Anglo-Russian Convention of 1907, which divided Persia into spheres of influence, regardless of her national sovereignty.
During World War I, the country was occupied by British, Ottoman and Russian forces but was essentially neutral (see Persian Campaign). In 1919, after the Russian Revolution and their withdrawal, Britain attempted to establish a protectorate in Persia, which was unsuccessful.
Finally, the Constitutionalist movement of Gilan and the central power vacuum caused by the instability of the Qajar government resulted in the rise of Reza Khan, who was later to become Reza Shah Pahlavi, and the subsequent establishment of the Pahlavi dynasty in 1925. In 1921, a military coup established Reza Khan, an officer of the Persian Cossack Brigade, as the dominant figure for the next 20 years. Seyyed Zia'eddin Tabatabai was also a leader and important figure in the perpetration of the coup. The coup was not actually directed at the Qajar monarchy; according to Encyclopædia Iranica, it was targeted at officials who were in power and actually had a role in controlling the government — the cabinet and others who had a role in governing Persia.[177] In 1925, after being prime minister for two years, Reza Khan became the first shah of the Pahlavi dynasty.
Reza Shah ruled for almost 16 years until September 16, 1941, when he was forced to abdicate by the Anglo-Soviet invasion of Iran. He established an authoritarian government that valued nationalism, militarism, secularism and anti-communism combined with strict censorship and state propaganda.[178] Reza Shah introduced many socio-economic reforms, reorganizing the army, government administration, and finances.[179]
To his supporters, his reign brought "law and order, discipline, central authority, and modern amenities – schools, trains, buses, radios, cinemas, and telephones".[180] However, his attempts of modernisation have been criticised for being "too fast"[181] and "superficial",[182] and his reign a time of "oppression, corruption, taxation, lack of authenticity" with "security typical of police states."[180]
Many of the new laws and regulations created resentment among devout Muslims and the clergy. For example, mosques were required to use chairs; most men were required to wear western clothing, including a hat with a brim; women were encouraged to discard the hijab; men and women were allowed to congregate freely, violating Islamic mixing of the sexes. Tensions boiled over in 1935, when bazaaris and villagers rose up in rebellion at the Imam Reza shrine in Mashhad, chanting slogans such as 'The Shah is a new Yezid.' Dozens were killed and hundreds were injured when troops finally quelled the unrest.[183]
While German armies were highly successful against the Soviet Union, the Iranian government expected Germany to win the war and establish a powerful force on its borders. It rejected British and Soviet demands to expel German residents from Iran. In response, the two Allies invaded in August 1941 and easily overwhelmed the weak Iranian army in Operation Countenance. Iran became the major conduit of Allied Lend-Lease aid to the Soviet Union. The purpose was to secure Iranian oil fields and ensure Allied supply lines (see Persian Corridor). Iran remained officially neutral. Its monarch Rezā Shāh was deposed during the subsequent occupation and replaced with his young son Mohammad Reza Pahlavi.[184]
At the Tehran Conference of 1943, the Allies issued the Tehran Declaration which guaranteed the post-war independence and boundaries of Iran. However, when the war actually ended, Soviet troops stationed in northwestern Iran not only refused to withdraw but backed revolts that established short-lived, pro-Soviet separatist national states in the northern regions of Azerbaijan and Iranian Kurdistan, the Azerbaijan People's Government and the Republic of Kurdistan respectively, in late 1945. Soviet troops did not withdraw from Iran proper until May 1946 after receiving a promise of oil concessions. The Soviet republics in the north were soon overthrown and the oil concessions were revoked.[185][186]
Initially there were hopes that post-occupation Iran could become a constitutional monarchy. The new, young Shah Mohammad Reza Shah Pahlavi initially took a very hands-off role in government, and allowed parliament to hold a lot of power. Some elections were held in the first shaky years, although they remained mired in corruption. Parliament became chronically unstable, and from the 1947 to 1951 period Iran saw the rise and fall of six different prime ministers. Pahlavi increased his political power by convening the Iran Constituent Assembly, 1949, which finally formed the Senate of Iran—a legislative upper house allowed for in the 1906 constitution but never brought into being. The new senators were largely supportive of Pahlavi, as he had intended.
In 1951 Prime Minister Mohammed Mosaddeq received the vote required from the parliament to nationalize the British-owned oil industry, in a situation known as the Abadan Crisis. Despite British pressure, including an economic blockade, the nationalization continued. Mosaddeq was briefly removed from power in 1952 but was quickly re-appointed by the Shah, due to a popular uprising in support of the premier, and he, in turn, forced the Shah into a brief exile in August 1953 after a failed military coup by Imperial Guard Colonel Nematollah Nassiri.
Shortly thereafter on August 19 a successful coup was headed by retired army general Fazlollah Zahedi, organized by the United States (CIA)[187] with the active support of the British (MI6) (known as Operation Ajax and Operation Boot to the respective agencies).[188] The coup—with a black propaganda campaign designed to turn the population against Mosaddeq [189] — forced Mosaddeq from office. Mosaddeq was arrested and tried for treason. Found guilty, his sentence was reduced to house arrest on his family estate while his foreign minister, Hossein Fatemi, was executed. Zahedi succeeded him as prime minister, and suppressed opposition to the Shah, specifically the National Front and Communist Tudeh Party.
Iran was ruled as an autocracy under the Shah with American support from that time until the revolution. The Iranian government entered into agreement with an international consortium of foreign companies which ran the Iranian oil facilities for the next 25 years, splitting profits fifty-fifty with Iran but not allowing Iran to audit their accounts or have members on their board of directors. In 1957 martial law was ended after 16 years and Iran became closer to the West, joining the Baghdad Pact and receiving military and economic aid from the US. In 1961, Iran initiated a series of economic, social, agrarian and administrative reforms to modernize the country that became known as the Shah's White Revolution.
The core of this program was land reform. Modernization and economic growth proceeded at an unprecedented rate, fueled by Iran's vast petroleum reserves, the third-largest in the world. However, the reforms, including the White Revolution, did not greatly improve economic conditions and the liberal pro-Western policies alienated certain Islamic religious and political groups. In early June 1963 several days of massive rioting occurred in support of Ayatollah Ruhollah Khomeini following the cleric's arrest for a speech attacking the Shah.
Two years later, premier Hassan Ali Mansur was assassinated and the internal security service, SAVAK, became more violently active. In the 1970s, leftist guerilla groups such as Mujaheddin-e-Khalq (MEK), emerged and attacked regime and foreign targets.
Nearly a hundred Iran political prisoners were killed by the SAVAK during the decade before the revolution and many more were arrested and tortured.[190] The Islamic clergy, headed by the Ayatollah Ruhollah Khomeini (who had been exiled in 1964), were becoming increasingly vociferous.
Iran greatly increased its defense budget and by the early 1970s was the region's strongest military power. Bilateral relations with its neighbor Iraq were not good, mainly due to a dispute over the Shatt al-Arab waterway. In November 1971, Iranian forces seized control of three islands at the mouth of the Persian Gulf; in response, Iraq expelled thousands of Iranian nationals. Following a number of clashes in April 1969, Iran abrogated the 1937 accord and demanded a renegotiation.
In mid-1973, the Shah returned the oil industry to national control. Following the Arab-Israeli War of October 1973, Iran did not join the Arab oil embargo against the West and Israel. Instead, it used the situation to raise oil prices, using the money gained for modernization and to increase defense spending.
A border dispute between Iraq and Iran was resolved with the signing of the Algiers Accord on March 6, 1975.
The Iranian Revolution, also known as the Islamic Revolution,[191] was the revolution that transformed Iran from an absolute monarchy under Shah Mohammad Reza Pahlavi to an Islamic republic under Ayatollah Ruhollah Khomeini, one of the leaders of the revolution and founder of the Islamic Republic.[11] Its time span can be said to have begun in January 1978 with the first major demonstrations,[192] and concluded with the approval of the new theocratic Constitution—whereby Ayatollah Khomeini became Supreme Leader of the country—in December 1979.[193]
In between, Mohammad Reza Pahlavi left the country for exile in January 1979 after strikes and demonstrations paralyzed the country, and on February 1, 1979 Ayatollah Khomeini returned to Tehran.[193] The final collapse of the Pahlavi dynasty occurred shortly after on February 11 when Iran's military declared itself "neutral" after guerrillas and rebel troops overwhelmed troops loyal to the Shah in armed street fighting. Iran officially became an Islamic Republic on April 1, 1979, when Iranians overwhelmingly approved a national referendum to make it so.[194]
The ideology of the revolutionary government was populist, nationalist and most of all Shi'a Islamic. Its unique constitution is based on the concept of velayat-e faqih the idea advanced by Khomeini that Muslims – in fact everyone – requires "guardianship", in the form of rule or supervision by the leading Islamic jurist or jurists.[195] Khomeini served as this ruling jurist, or supreme leader, until his death in 1989.
Iran's rapidly modernising, capitalist economy was replaced by populist and Islamic economic and cultural policies. Much industry was nationalized, laws and schools Islamicized, and Western influences banned.
The Islamic revolution also created great impact around the world. In the non-Muslim world it has changed the image of Islam, generating much interest in the politics and spirituality of Islam,[196] along with "fear and distrust towards Islam" and particularly the Islamic Republic and its founder.[197]
Khomeini served as leader of the revolution or as Supreme Leader of Iran from 1979 to his death on June 3, 1989. This era was dominated by the consolidation of the revolution into a theocratic republic under Khomeini, and by the costly and bloody war with Iraq.
The consolidation lasted until 1982–3,[198][199] as Iran coped with the damage to its economy, military, and apparatus of government, and protests and uprisings by secularists, leftists, and more traditional Muslims—formerly ally revolutionaries but now rivals—were effectively suppressed. Many political opponents were executed by the new regimes. Following the events of the revolution, Marxist guerrillas and federalist parties revolted in regions comprising Khuzistan, Kurdistan and Gonbad-e Qabus, resulting in severe fighting between rebels and revolutionary forces. These revolts began in April 1979 and lasted between several months to over a year, depending on the region. The Kurdish uprising, led by the KDPI, was the most violent, lasting until 1983 and resulting in 10,000 casualties.
In the summer of 1979 a new constitution giving Khomeini a powerful post as guardian jurist Supreme Leader[200] and a clerical Council of Guardians power over legislation and elections, was drawn up by an Assembly of Experts for Constitution. The new constitution was approved by referendum in December 1979.
An early event in the history of the Islamic republic that had a long-term impact was the Iran hostage crisis. Following the admitting of the former Shah of Iran into the United States for cancer treatment, on November 4, 1979, Iranian students seized US embassy personnel, labeling the embassy a "den of spies."[201] Fifty-two hostages were held for 444 days until January 1981.[202] An American military attempt to rescue the hostages failed.[203]
The takeover was enormously popular in Iran, where thousands gathered in support of the hostage takers, and it is thought to have strengthened the prestige of the Ayatollah Khomeini and consolidated the hold of anti-Americanism. It was at this time that Khomeini began referring to America as the "Great Satan." In America, where it was considered a violation of the long-standing principle of international law that diplomats may be expelled but not held captive, it created a powerful anti-Iranian backlash. Relations between the two countries have remained deeply antagonistic and American international sanctions have hurt Iran's economy.[204]
During this political and social crisis, Iraqi leader Saddam Hussein attempted to take advantage of the disorder of the Revolution, the weakness of the Iranian military and the revolution's antagonism with Western governments. The once-strong Iranian military had been disbanded during the revolution, and with the Shah ousted, Hussein had ambitions to position himself as the new strong man of the Middle East. He sought to expand Iraq's access to the Persian Gulf by acquiring territories that Iraq had claimed earlier from Iran during the Shah's rule.
Of chief importance to Iraq was Khuzestan which not only boasted a substantial Arab population, but rich oil fields as well. On the unilateral behalf of the United Arab Emirates, the islands of Abu Musa and the Greater and Lesser Tunbs became objectives as well. With these ambitions in mind, Hussein planned a full-scale assault on Iran, boasting that his forces could reach the capital within three days. On September 22, 1980, the Iraqi army invaded Iran at Khuzestan, precipitating the Iran–Iraq War. The attack took revolutionary Iran completely by surprise.
Although Saddam Hussein's forces made several early advances, Iranian forces had pushed the Iraqi army back into Iraq by 1982. Khomeini sought to export his Islamic revolution westward into Iraq, especially on the majority Shi'a Arabs living in the country. The war then continued for six more years until 1988, when Khomeini, in his words, "drank the cup of poison" and accepted a truce mediated by the United Nations.
Tens of thousands of Iranian civilians and military personnel were killed when Iraq used chemical weapons in its warfare. Iraq was financially backed by Egypt, the Arab countries of the Persian Gulf, the Soviet Union and the Warsaw Pact states, the United States (beginning in 1983), France, the United Kingdom, Germany, Brazil, and the People's Republic of China (which also sold weapons to Iran).
There were more than 182,000 Kurdish victims[205] of Iraq's chemical weapons during the eight-year war. The total Iranian casualties of the war were estimated to be between 500,000 and 1,000,000. Almost all relevant international agencies have confirmed that Saddam engaged in chemical warfare to blunt Iranian human wave attacks; these agencies unanimously confirmed that Iran never used chemical weapons during the war.[206][207][208][209]
Starting on 19 July 1988 and lasting for about five months the government systematically executed thousands of political prisoners across Iran. This is commonly referred to as the 1988 executions of Iranian political prisoners or the 1988 Iranian Massacre. The main target was the membership of the People's Mojahedin Organization of Iran (PMOI), although a lesser number of political prisoners from other leftist groups were also included such as the Tudeh Party of Iran (Communist Party).[210][211] Estimates of the number executed vary from 1,400[212] to 30,000.[213][214]
On his deathbed in 1989, Khomeini appointed a 25-man Constitutional Reform Council which named then president Ali Khamenei as the next Supreme Leader, and made a number of changes to Iran's constitution.[215] A smooth transition followed Khomeini's death on June 3, 1989. While Khamenei lacked Khomeini's "charisma and clerical standing", he developed a network of supporters within Iran's armed forces and its economically powerful religious foundations.[216] Under his reign Iran's regime is said – by at least one observer – to resemble more "a clerical oligarchy ... than an autocracy."[216]
Succeeding Khamenei as president on August 3, 1989 was pragmatic conservative Ali-Akbar Hashemi Rafsanjani, who served two four-year terms and focused his efforts on rebuilding Iran's economy and war-damaged infrastructure, though low oil prices hampered this endeavor. He sought to restore confidence in the government among the general population by privatizing the companies that had been nationalized in the first few years of the Islamic Republic, as well as by bringing in qualified technocrats to manage the economy. The state of their economy also influenced the government to move towards ending their diplomatic isolation. This was achieved through the reestablishment of normalized relations with neighbors such as Saudi Arabia and an attempt to improve its reputation in the region with assertions that its revolution was not exportable to other states.[217] During the Persian Gulf War in 1991 the country remained neutral, restricting its action to the condemnation of the U.S. and allowing fleeing Iraqi aircraft and refugees into the country.
Iran in the 1990s had a greater secular behavior and admiration for Western popular culture than in the previous decades. This admiration had become a way in which the urban population expressed their resentment at the invasive Islamic policies of the government.[218] The pressures from the population placed on the new Supreme Leader Ayatollah Ali Khamenei led to an uneasy alliance between him and President Akbar Hashemi Rafsanjani. Through this alliance they attempted to hinder the ulama's ability to gain further control of the state. In 1989, they created a sequence of constitutional amendments that removed the office of prime minister and increased the scope of presidential power. However, these new amendments did not curtail the powers of the Supreme Leader of Iran in any way; this position still contained the ultimate authority over the armed forces, the making of war and peace, the final say in foreign policy, and the right to intervene in the legislative process whenever he deemed it necessary.[218]
President Rafsanjani's economic policies led to stronger relations with the outside world. But his government's relaxation of the enforcement of certain regulations on social behavior were met with some responses of widespread disenchantment among the general population with the ulama as rulers of the country.[218] This led to the defeat of the government's candidate for president in 1997, who had the backing of the supreme Islamic jurist. He was beaten by an independent candidate from the Reformists, Mohammad Khatami. He received 69% of the vote and enjoyed particular support from two groups of the population that had felt ostracized by the practices of the state: women and youth. The younger generations in the country had been too young to experience the shah's regime or the revolution that ended it, and now they resented the restrictions placed on their daily lives under the Islamic Republic. Mohammad Khatami's presidency was soon marked by tensions between the reform-minded government and an increasingly conservative and vocal clergy. This rift reached a climax in July 1999 when massive anti-government protests erupted in the streets of Tehran. The disturbances lasted over a week before police and pro-government vigilantes dispersed the crowds.
Khatami was re-elected in June 2001 but his efforts were repeatedly blocked by the conservatives in the parliament. Conservative elements within Iran's government moved to undermine the reformist movement, banning liberal newspapers and disqualifying candidates for parliamentary elections. This clampdown on dissent, combined with the failure of Khatami to reform the government, led to growing political apathy among Iran's youth.
In June 2003, anti-government protests by several thousand students took place in Tehran.[219][220] Several human rights protests also occurred in 2006.
In the 2005 Iranian presidential election, Mahmoud Ahmadinejad, mayor of Tehran, became the sixth president of Iran, after winning 62 percent of the vote in the run-off poll, against former president Ali-Akbar Hashemi Rafsanjani.[221] During the authorization ceremony he kissed Khamenei's hand in demonstration of his loyalty to him.[222][223]
During this time, the American invasion of Iraq, the overthrow of Saddam Hussein's regime and empowerment of its Shi'a majority, all strengthened Iran's position in the region particularly in the mainly Shi'a south of Iraq, where a top Shia leader in the week of September 3, 2006 renewed demands for an autonomous Shi'a region.[224] At least one commentator (former U.S. Defense Secretary William S. Cohen) has stated that as of 2009 Iran's growing power has eclipsed anti-Zionism as the major foreign policy issue in the Middle East.[225]
During 2005 and 2006, there were claims that the United States and Israel were planning to attack Iran, with the most cited reason being Iran's civilian nuclear energy program which the United States and some other states fear could lead to a nuclear weapons program. China and Russia opposed military action of any sort and opposed economic sanctions. Supreme Leader Ali Khamenei issued a fatwa forbidding the production, stockpiling and use of nuclear weapons. The fatwa was cited in an official statement by the Iranian government at an August 2005 meeting of the International Atomic Energy Agency (IAEA) in Vienna.[226][227]
In 2009, Ahmadinejad's reelection was hotly disputed and marred by large protests that formed the "greatest domestic challenge" to the leadership of the Islamic Republic "in 30 years". The resulting social unrest is widely known as the Iranian Green Movement.[228] Reformist opponent Mir-Hossein Mousavi and his supporters alleged voting irregularities and by 1 July 2009, 1000 people had been arrested and 20 killed in street demonstrations.[229] Supreme Leader Ali Khamenei and other Islamic officials blamed foreign powers for fomenting the protest.[230]
On 15 June 2013, Hassan Rouhani won the presidential election in Iran, with a total number of 36,704,156 ballots cast; Rouhani won 18,613,329 votes. In his press conference one day after election day, Rouhani reiterated his promise to recalibrate Iran's relations with the world.
On April 2, 2015, following eight days of tortuous discussions in Switzerland, which lasted through the night, Iran and six world powers (United States, United Kingdom, France, China and Russia plus Germany) agreed on the outlines of an understanding to limit Iran's nuclear programs, negotiators indicated, as both sides prepared for announcements. Iranian Foreign Minister Mohammad Javad Zarif tweeted: "Found solutions. Ready to start drafting immediately." European Union foreign policy chief Federica Mogherini tweeted that she would meet the press with Zarif after a final meeting of the seven nations in the nuclear talks. She wrote: "Good news."
Reading out a joint statement, European Union foreign policy chief Federica Mogherini hailed what she called a "decisive step" after more than a decade of work. Iranian Foreign Minister Mohammad Javad Zarif followed with the same statement in Persian. U.S. Secretary of State John Kerry and the top diplomats of Britain, France and Germany also briefly took the stage behind them. The deal is intended to be a provisional framework for a comprehensive agreement and was signed in 2015, and marked a significant breakthrough in the 12-year history of negotiations with Iran over its nuclear programme.
When Donald Trump was campaigning to become President of the US, he repeatedly said he would abandon the Iran nuclear deal. After he was elected president, the USA announced its withdrawal from the agreement on 8 May 2018.
The Iranian-backed group known as Kataib Hezbollah attacked the United States embassy in Baghdad on December 31, 2019.
On January 3, 2020, the United States military executed a drone strike at Baghdad Airport, killing Qasem Soleimani, the leader of the Quds Force, an elite branch of the Islamic Revolutionary Guard Corps.
On 3 August 2021 Ebrahim Raisi was elected 8th President of Iran.
Beginning on 16 September 2022 protests started against the government of Iran following the death of Mahsa Amini(Jina).[231][232][233]
The history of mathematics deals with the origin of discoveries in mathematics and the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, followed closely by Ancient Egypt and the Levantine state of Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the patterns in nature, the field of astronomy and to record time and formulate calendars.
The earliest mathematical texts available are from Mesopotamia and Egypt – Plimpton 322 (Babylonian c. 2000 – 1900 BC),[2] the Rhind Mathematical Papyrus (Egyptian c. 1800 BC)[3] and the Moscow Mathematical Papyrus (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples, so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.
The study of mathematics as a "demonstrative discipline" began in the 6th century BC with the Pythagoreans, who coined the term "mathematics" from the ancient Greek μάθημα (mathema), meaning "subject of instruction".[4] Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics.[5] Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers.[6][7] The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī.[8][9] Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations.[10] Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.
Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century.
The origins of mathematical thought lie in the concepts of number, patterns in nature, magnitude, and form.[11] Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the "number" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between "one", "two", and "many", but not of numbers larger than two.[11]
The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a tally of the earliest known demonstration of sequences of prime numbers[12] or a six-month lunar calendar.[13] Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that "no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10."[14] The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.[15]
Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design.[16] All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.[17]
Babylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity.[18] The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period).[19] It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.
In contrast to the sparsity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s.[20] Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.[21]
The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onward, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.[22]
Babylonian mathematics were written using a sexagesimal (base-60) numeral system.[20] From this derives the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 × 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30.[20] Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a place-value system, where digits written in the left column represented larger values, much as in the decimal system.[19] The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different from multiplying integers, similar to modern notation.[19] The notational system of the Babylonians was the best of any civilization until the Renaissance,[23] and its power allowed it to achieve remarkable computational accuracy; for example, the Babylonian tablet YBC 7289 gives an approximation of √2 accurate to five decimal places.[23] The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.[19] By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions.[19] This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.[19]
Other topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular numbers, and their reciprocal pairs.[24] The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time.[25] Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem.[26] However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.[21]
Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars. Archaeological evidence has suggested that the Ancient Egyptian counting system had origins in Sub-Saharan Africa.[27] Also, fractal geometry designs which are widespread among Sub-Saharan African cultures are also found in Egyptian architecture and cosmological signs.[28]
The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC.[29] It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge,[30] including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6).[31] It also shows how to solve first order linear equations[32] as well as arithmetic and geometric series.[33]
Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC.[34] It consists of what are today called word problems or story problems, which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).
Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.[35]
Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD.[36] Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.[37]
Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.[38]
Greek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.
Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed.[39] Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number".[40] It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem,[41] though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers.[42][43] Although he was preceded by the Babylonians, Indians and the Chinese,[44] the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum).[45] The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the mensa Pythagorica.[46]
Plato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others.[47] His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came.[48] Plato also discussed the foundations of mathematics,[49] clarified some of the definitions (e.g. that of a line as "breadthless length"), and reorganized the assumptions.[50] The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.[48]
Eudoxus (408–c. 355 BC) developed the method of exhaustion, a precursor of modern integration[51] and a theory of ratios that avoided the problem of incommensurable magnitudes.[52] The former allowed the calculations of areas and volumes of curvilinear figures,[53] while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.[54]
In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria.[56] It was there that Euclid (c. 300 BC) taught, and wrote the Elements, widely considered the most successful and influential textbook of all time.[1] The Elements introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[57] The Elements was known to all educated people in the West up through the middle of the 20th century and its contents are still taught in geometry classes today.[58] In addition to the familiar theorems of Euclidean geometry, the Elements was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry,[57] including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.[59]
Archimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity,[60] used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[61] He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 310/71 < π < 310/70.[62] He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid),[61] and an ingenious method of exponentiation for expressing very large numbers.[63] While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles.[64] He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.[65]
Apollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone.[66] He also coined the terminology in use today for conic sections, namely parabola ("place beside" or "comparison"), "ellipse" ("deficiency"), and "hyperbola" ("a throw beyond").[67] His work Conics is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton.[68] While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.[69]
Around the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers.[70] The 3rd century BC is generally regarded as the "Golden Age" of Greek mathematics, with advances in pure mathematics henceforth in relative decline.[71] Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers.[71] Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle.[72] Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots.[73] Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem.[74] The most complete and influential trigonometric work of antiquity is the Almagest of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years.[75] Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.[76]
Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the "Silver Age" of Greek mathematics.[77] During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as "Diophantine analysis".[78] The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the Arithmetica, a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations.[79] The Arithmetica had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the Arithmetica (that of dividing a square into two squares).[80] Diophantus also made significant advances in notation, the Arithmetica being the first instance of algebraic symbolism and syncopation.[79]
Among the last great Greek mathematicians is Pappus of Alexandria (4th  century AD).  He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His Collection is a major source of knowledge on Greek mathematics as most of it has survived.[81] Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library[citation needed] and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed.[82] Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius.[83]  Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics.  The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Hagia Sophia.[84]  Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.[85]
Although ethnic Greek mathematicians continued under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison.[86][87] Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks.[88] It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.[89]
Using calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury.[90] Siculus Flaccus, one of the Roman gromatici (i.e. land surveyor), wrote the Categories of Fields, which aided Roman surveyors in measuring the surface areas of allotted lands and territories.[91] Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns.[92] Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.[93][94]
The creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year.[95] In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February.[96] This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle.[97] This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (r. 1572–1585), virtually the same solar calendar used in modern times as the international standard calendar.[98]
At roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC – c. 15 BC).[99] The device was used at least until the reign of emperor Commodus (r. 177 – 192 AD), but its design seems to have been lost until experiments were made during the 15th century in Western Europe.[100] Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.[101]
An analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development.[102] The oldest extant mathematical text from China is the Zhoubi Suanjing, variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable.[103] However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.[44]
Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called "rod numerals" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten.[104] Thus, the number 123 would be written using the symbol for "1", followed by the symbol for "100", then the symbol for "2" followed by the symbol for "10", followed by the symbol for "3". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system.[105] Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the suan pan, or Chinese abacus. The date of the invention of the suan pan is not certain, but the earliest written mention dates from AD 190, in Xu Yue's Supplementary Notes on the Art of Figures.
The oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The Mo Jing described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well.[106] It also defined the concepts of circumference, diameter, radius, and volume.[107]
In 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is The Nine Chapters on the Mathematical Art, the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles.[103] It created mathematical proof for the Pythagorean theorem,[108] and a mathematical formula for Gaussian elimination.[109] The treatise also provides values of π,[103] which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724,[110] as well as 3.162 by taking the square root of 10.[111][112] Liu Hui commented on the Nine Chapters in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159).[113][114] Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (between 3.1415926 and 3.1415927), which remained the most accurate value of π for almost the next 1000 years.[113][115] He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.[116]
The high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the Precious Mirror of the Four Elements by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method.[113] The Precious Mirror also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100.[117] The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).[117]
Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.[117]
Japanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere.[118] Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644).[119] For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers.[120] Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.[121]
The earliest civilization on the Indian subcontinent is the Indus Valley civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.[123]
The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD),[124] appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others.[125] As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual.[124] The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π.[126][127][a] In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem.[127] All of these results are present in Babylonian mathematics, indicating Mesopotamian influence.[124] It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.[124]
Pāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar.[128] His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion.[129] Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system.[130][131] His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called mātrāmeru).[132]
The next significant mathematical documents from India after the Sulba Sutras are the Siddhantas, astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence.[133] They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry.[134] Through a series of translation errors, the words "sine" and "cosine" derive from the Sanskrit "jiya" and "kojiya".[134]
Around 500 AD, Aryabhata wrote the Aryabhatiya, a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology.[135] Though about half of the entries are wrong, it is in the Aryabhatiya that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the Aryabhatiya as a "mix of common pebbles and costly crystals".[136]
In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in Brahma-sphuta-siddhanta, he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system.[137] It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.[citation needed]
In the 12th century, Bhāskara II[138] lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.[139]
In the 14th century, Madhava of Sangamagrama, the founder of the  Kerala School of Mathematics, found the Madhava–Leibniz series and obtained from it a transformed series, whose first 21 terms he used to compute the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions.[140] In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the Yukti-bhāṣā.[141]
[142] It has been argued that the advances of the Kerala school, which laid the foundations of the calculus, were transmitted to Europe in the 16th century[143] via Jesuit missionaries and traders who were active around the ancient port of Muziris at the time and, as a result, directly influenced later European developments in analysis and calculus.[144] However, other scholars argue that the Kerala School did not formulate a systematic theory of differentiation and integration, and that there is not any direct evidence of their results being transmitted outside Kerala.[145][146][147][148]
The Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.
In the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote an important book on the Hindu–Arabic numerals and one on methods for solving equations. His book On the Calculation with Hindu Numerals, written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word algorithm is derived from the Latinization of his name, Algoritmi, and the word algebra from the title of one of his works, Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala (The Compendious Book on Calculation by Completion and Balancing). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots,[149] and he was the first to teach algebra in an elementary form and for its own sake.[150] He also discussed the fundamental method of "reduction" and "balancing", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as al-jabr.[151] His algebra was also no longer concerned "with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study." He also studied an equation for its own sake and "in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems."[152]
In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions.[153] His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.
Further developments in algebra were made by Al-Karaji in his treatise al-Fakhri, where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes.[154] The historian of mathematics, F. Woepcke,[155] praised Al-Karaji for being "the first who introduced the theory of algebraic calculus." Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.[156]
In the late 11th century, Omar Khayyam wrote Discussions of the Difficulties in Euclid, a book about what he perceived as flaws in Euclid's Elements, especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.[157]
In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating nth roots, which was a special case of the methods given many centuries later by Ruffini and Horner.
Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.[158]
During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.
In the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics.[159] Maya numerals utilized a base of twenty, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures.[159] The Maya used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy.[159] While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Maya developed a standard symbol for it.[159]
Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's Timaeus and the biblical passage (in the Book of Wisdom) that God had ordered all things in measure, and number, and weight.[160]
Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term quadrivium to describe the study of arithmetic, geometry, astronomy, and music. He wrote De institutione arithmetica, a free translation from the Greek of Nicomachus's Introduction to Arithmetic; De institutione musica, also derived from Greek sources; and a series of excerpts from Euclid's Elements. His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.[161][162]
In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's The Compendious Book on Calculation by Completion and Balancing, translated into Latin by Robert of Chester, and the complete text of Euclid's Elements, translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona.[163][164]  These and other new sources sparked a renewal of mathematics.
Leonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father.  (Europe was still using Roman numerals.)  There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce.  Leonardo wrote Liber Abaci in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it.  The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.
The 14th century saw the development of new mathematical concepts to investigate a wide range of problems.[165] One important contribution was development of mathematics of local motion.
Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R).[166] Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.[167]
One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed "by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant".[169]
Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that "a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]".[170]
Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled.[171] In a later mathematical commentary on Euclid's Elements, Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.[172]
During the Renaissance, the development of mathematics and of accounting were intertwined.[173] While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as abbaco in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.
Piero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including De Prospectiva Pingendi (On Perspective for Painting), Trattato d’Abaco (Abacus Treatise), and De quinque corporibus regularibus (On the Five Regular Solids).[174][175][176]
Luca Pacioli's Summa de Arithmetica, Geometria, Proportioni et Proportionalità (Italian: "Review of Arithmetic, Geometry, Ratio and Proportion") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, "Particularis de Computis et Scripturis" (Italian: "Details of Calculation and Recording"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons.[177] In Summa Arithmetica, Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. Summa Arithmetica was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.
In Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book Ars Magna, together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his L'Algebra in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.
Simon Stevin's book De Thiende ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.
Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Regiomontanus's table of sines and cosines was published in 1533.[178]
During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.[179]
The 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.[180]
The analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.
Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.[181]
In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.
The most influential mathematician of the 18th century was arguably Leonhard Euler (1707–1783). His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol i, and he popularized the use of the Greek letter 



π


{\displaystyle \pi }

 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.
Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.
Throughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.
This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.
The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in electrical engineering and computer science.
Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.
Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.
Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.
In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.
The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.
In 1897, Kurt Hensel introduced p-adic numbers.
The 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.
In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.
Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.
Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the "enormous theorem"), whose proof between 1955 and 2004 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym "Nicolas Bourbaki", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.[182]
Differential geometry came into its own when Albert Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.
Non-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.
The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the fast Fourier transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.
At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus either addition or multiplication (but not both), was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.
One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.
Paul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the "collaborative distance" between a person and Erdős, as measured by joint authorship of mathematical papers.
Emmy Noether has been described by many as the most important woman in the history of mathematics.[183] She studied the theories of rings, fields, and algebras.
As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long.[184] More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.
In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).
Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive toward open access publishing, first popularized by arXiv.
There are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is expanding exponentially.[citation needed]

Muammar Gaddafi became the de facto leader of Libya on 1 September 1969 after leading a group of young Libyan Army officers against King Idris I in a bloodless coup d'état. After the king had fled the country, the Revolutionary Command Council (RCC) headed by Gaddafi abolished the monarchy and the old constitution and established the Libyan Arab Republic, with the motto "freedom, socialism and unity".[1]
After coming to power, the RCC government initiated a process of directing funds toward providing education, health care and housing for all. Public education in the country became free and primary education compulsory for both sexes. Medical care became available to the public at no cost, but providing housing for all was a task the RCC government was unable to complete.[2] Under Gaddafi, per capita income in the country rose to more than US$11,000, the 5th highest in Africa.[3] The increase in prosperity was accompanied by a controversial foreign policy, and there was increased domestic political repression.[1][4]
During the 1980s and 1990s, Gaddafi, in alliance with the Eastern Bloc and Fidel Castro's Cuba, openly supported rebel movements like Nelson Mandela's African National Congress, Yasser Arafat's Palestine Liberation Organization, the Provisional Irish Republican Army and the Polisario Front (Western Sahara). Gaddafi's government was either known to be or suspected of participating in or aiding attacks by these and other proxy forces. Additionally, Gaddafi undertook several invasions of neighboring states in Africa, notably Chad in the 1970s and 1980s. All of his actions led to a deterioration of Libya's foreign relations with several countries, mostly Western states,[5] and culminated in the 1986 United States bombing of Libya. Gaddafi defended his government's actions by citing the need to support anti-imperialist and anti-colonial movements around the world. Notably, Gaddafi supported anti-Zionist, pan-Arab, pan-Africanist and Arab and black civil rights movements. Gaddafi's behavior, often erratic, led some outsiders to conclude that he was not mentally sound, a claim disputed by the Libyan authorities and other observers close to Gaddafi. Despite receiving extensive aid and technical assistance from the Soviet Union and its allies, Gaddafi retained close ties to pro-American governments in Western Europe, largely by courting Western oil companies with promises of access to lucrative Libyan energy sectors. After the 9/11 attacks, strained relations between Libya and the West were mostly normalised, and sanctions against the country relaxed, in exchange for nuclear disarmament.
In early 2011, a civil war broke out in the context of the wider "Arab Spring". The rebel anti-Gaddafi forces formed a committee named the National Transitional Council on 27 February 2011. It was meant to act as an interim authority in the rebel-controlled areas. After killings by government forces[6] in addition to those by the rebel forces,[7] a multinational coalition led by NATO forces intervened on 21 March 2011 in support of the rebels.[8][9][10] The International Criminal Court issued an arrest warrant against Gaddafi and his entourage on 27 June 2011. Gaddafi's government was overthrown in the wake of the fall of Tripoli to the rebel forces on 20 August 2011, although pockets of resistance held by forces in support of Gaddafi's government held out for another two months, especially in Gaddafi's hometown of Sirte, which he declared the new capital of Libya on 1 September 2011.[11] The fall of the last remaining sites in Sirte under pro-Gaddafi control on 20 October 2011, followed by the subsequent killing of Gaddafi, marked the end of the Libyan Arab Jamahiriya.
The name of Libya was changed several times during Gaddafi's tenure as leader. From 1969 to 1977, the name was the Libyan Arab Republic. In 1977, the name was changed to Socialist People's Libyan Arab Jamahiriya.[12] Jamahiriya was a term coined by Gaddafi,[12] usually translated as "state of the masses". The country was renamed again in 1986 as the Great Socialist People's Libyan Arab Jamahiriya, after the 1986 United States bombing of Libya.
The discovery of significant oil reserves in 1959 and the subsequent income from petroleum sales enabled the Kingdom of Libya to transition from one of the world's poorest nations to a wealthy state. Although oil drastically improved the Libyan government's finances, resentment began to build over the increased concentration of the nation's wealth in the hands of King Idris. This discontent mounted with the rise of Nasserism and Arab nationalism/socialism throughout North Africa and the Middle East.
On 1 September 1969, a group of about 70 young army officers known as the Free Officers Movement and enlisted men mostly assigned to the Signal Corps, seized control of the government and in a stroke abolished the Libyan monarchy. The coup was launched at Benghazi, and within two hours the takeover was completed. Army units quickly rallied in support of the coup, and within a few days firmly established military control in Tripoli and elsewhere throughout the country. Popular reception of the coup, especially by younger people in the urban areas, was enthusiastic. Fears of resistance in Cyrenaica and Fezzan proved unfounded. No deaths or violent incidents related to the coup were reported.[13]
The Free Officers Movement, which claimed credit for carrying out the coup, was headed by a twelve-member directorate that designated itself the Revolutionary Command Council (RCC). This body constituted the Libyan government after the coup. In its initial proclamation on 1 September,[14] the RCC declared the country to be a free and sovereign state called the Libyan Arab Republic, which would proceed "in the path of freedom, unity, and social justice, guaranteeing the right of equality to its citizens, and opening before them the doors of honorable work." The rule of the Turks and Italians and the "reactionary" government just overthrown were characterized as belonging to "dark ages", from which the Libyan people were called to move forward as "free brothers" to a new age of prosperity, equality, and honor.
The RCC advised diplomatic representatives in Libya that the revolutionary changes had not been directed from outside the country, that existing treaties and agreements would remain in effect, and that foreign lives and property would be protected. Diplomatic recognition of the new government came quickly from countries throughout the world. United States recognition was officially extended on 6 September.
In view of the lack of internal resistance, it appeared that the chief danger to the new government lay in the possibility of a reaction inspired by the absent King Idris or his designated heir, Hasan Al-Rida, who had been taken into custody at the time of the coup along with other senior civil and military officials of the royal government. Within days of the coup, however, Hasan publicly renounced all rights to the throne, stated his support for the new government, and called on the people to accept it without violence.
Idris, in an exchange of messages with the RCC through Egypt's President Nasser, dissociated himself from reported attempts to secure British intervention and disclaimed any intention of coming back to Libya. In return, he was assured by the RCC of the safety of his family still in the country. At his own request and with Nasser's approval, Idris took up residence once again in Egypt, where he had spent his first exile and where he remained until his death in 1983.
On 7 September 1969, the RCC announced that it had appointed a cabinet to conduct the government of the new republic. An American-educated technician, Mahmud Sulayman al-Maghribi, who had been imprisoned since 1967 for his political activities, was designated prime minister. He presided over the eight-member Council of Ministers, of whom six, like Maghrabi, were civilians and two – Adam Said Hawwaz and Musa Ahmad – were military officers. Neither of the officers was a member of the RCC.
The Council of Ministers was instructed to "implement the state's general policy as drawn up by the RCC", leaving no doubt where ultimate authority rested. The next day the RCC decided to promote Captain Gaddafi to colonel and to appoint him commander in chief of the Libyan Armed Forces. Although RCC spokesmen declined until January 1970 to reveal any other names of RCC members, it was apparent from that date onward that the head of the RCC and new de facto head of state was Gaddafi.
Analysts were quick to point out the striking similarities between the Libyan military coup of 1969 and that in Egypt under Nasser in 1952, and it became clear that the Egyptian experience and the charismatic figure of Nasser had formed the model for the Free Officers Movement. As the RCC in the last months of 1969 moved vigorously to institute domestic reforms, it proclaimed neutrality in the confrontation between the superpowers and opposition to all forms of colonialism and imperialism. It also made clear Libya's dedication to Arab unity and to the support of the Palestinian cause against Israel.
The RCC reaffirmed the country's identity as part of the "Arab nation" and its state religion as Islam. It abolished parliamentary institutions, all legislative functions being assumed by the RCC, and continued the prohibition against political parties, in effect since 1952. The new government categorically rejected communism – in large part because it was atheist – and officially espoused an Arab interpretation of socialism that integrated Islamic principles with social, economic, and political reform. Libya had shifted, virtually overnight, from the camp of conservative Arab traditionalist states to that of the radical nationalist states.
Following the formation of the Libyan Arab Republic, Gaddafi and his associates insisted that their government would not rest on individual leadership, but rather on collegial decision making.
The first major cabinet change occurred soon after the first challenge to the government. In December 1969, Adam Said Hawwaz, the minister of defense, and Musa Ahmad, the minister of interior, were arrested and accused of planning a coup. In the new cabinet formed after the crisis, Gaddafi, retaining his post as chairman of the RCC, also became prime minister and defense minister.[15]
Major Abdel Salam Jallud, generally regarded as second only to Gaddafi in the RCC, became deputy prime minister and minister of interior.[15] This cabinet totaled thirteen members, of whom five were RCC officers.[15] The government was challenged a second time in July 1970 when Abdullah Abid Sanusi and Ahmed al-Senussi, distant cousins of former King Idris, and members of the Sayf an Nasr clan of Fezzan were accused of plotting to seize power for themselves.[15] After the plot was foiled, a substantial cabinet change occurred, RCC officers for the first time forming a majority among new ministers.[15]
From the start, RCC spokesmen had indicated a serious intent to bring the "defunct regime" to account. In 1971 and 1972 more than 200 former government officials—including seven prime ministers and numerous cabinet ministers—as well as former King Idris and members of the royal family, were brought to trial on charges of treason and corruption in the Libyan People's Court.
Many, who like Idris lived in exile, were tried in absentia. Although a large percentage of those charged were acquitted, sentences of up to fifteen years in prison and heavy fines were imposed on others. Five death sentences, all but one of them in absentia, were pronounced, among them, one against Idris. Fatima, the former queen, and Hasan ar Rida were sentenced to five and three years in prison, respectively.
Meanwhile, Gaddafi and the RCC had disbanded the Sanusi order and officially downgraded its historical role in achieving Libya's independence. He also attacked regional and tribal differences as obstructions in the path of social advancement and Arab unity, dismissing traditional leaders and drawing administrative boundaries across tribal groupings.
The Free Officers Movement was renamed "Arab Socialist Union" (ASU) in 1971, modeled after Egypt's Arab Socialist Union, and made the sole legal party in Gaddafi's Libya. It acted as a "vehicle of national expression", purporting to "raise the political consciousness of Libyans" and to "aid the RCC in formulating public policy through debate in open forums".[16] Trade unions were incorporated into the ASU and strikes outlawed. The press, already subject to censorship, was officially conscripted in 1972 as an agent of the revolution. Italians and what remained of the Jewish community were expelled from the country and their property confiscated in October 1970.
In 1972, Libya joined the Federation of Arab Republics with Egypt and Syria but the intended union of pan-Arabic states never had the intended success, and was effectively dormant after 1973.
As months passed, Gaddafi, caught up in his apocalyptic visions of revolutionary Pan-Arabism and Islam locked in mortal struggle with what he termed the encircling, demonic forces of reaction, imperialism, and Zionism, increasingly devoted attention to international rather than internal affairs. As a result, routine administrative tasks fell to Major Jallud, who in 1972 became prime minister in place of Gaddafi. Two years later Jallud assumed Gaddafi's remaining administrative and protocol duties to allow Gaddafi to devote his time to revolutionary theorizing. Gaddafi remained commander in chief of the armed forces and effective head of state. The foreign press speculated about an eclipse of his authority and personality within the RCC, but Gaddafi soon dispelled such theories by his measures to restructure Libyan society.
After the September coup, U.S. forces proceeded deliberately with the planned withdrawal from Wheelus Air Base under the agreement made with the previous government. The foreign minister, Salah Busir, played an important role in negotiating the British and American military withdrawal from the new republic. The last of the American contingent turned the facility over to the Libyans on 11 June 1970, a date thereafter celebrated in Libya as a national holiday. On 27 March 1970, the British air base in El Adem and the naval base in Tobruk were abandoned.[17]
As relations with the U.S. steadily deteriorated, Gaddafi forged close links with the Soviet Union and other Eastern Bloc countries, all the while maintaining Libya's stance as a nonaligned country and opposing the spread of communism in the Arab world. Libya's army—sharply increased from the 6,000-man pre-revolutionary force that had been trained and equipped by the British—was armed with Soviet-built armor and missiles.
The economic base for Libya's revolution has been its oil revenues. However, Libya's petroleum reserves were small compared with those of other major Arab petroleum-producing states. As a consequence, Libya was more ready to ration output in order to conserve its natural wealth and less responsive to moderating its price-rise demands than the other countries. Petroleum was seen both as a means of financing the economic and social development of a woefully underdeveloped country and as a political weapon to brandish in the Arab struggle against Israel.
The increase in production that followed the 1969 revolution was accompanied by Libyan demands for higher petroleum prices, a greater share of revenues, and more control over the development of the country's petroleum industry. Foreign petroleum companies agreed to a price hike of more than three times the going rate (from US$0.90 to US$3.45 per barrel) early in 1971. In December, the Libyan government suddenly nationalized the holdings of British Petroleum in Libya and withdrew funds amounting to approximately US$550 million invested in British banks as a result of a foreign policy dispute. British Petroleum rejected as inadequate a Libyan offer of compensation, and the British treasury banned Libya from participation in the Sterling Area.
In 1973, the Libyan government announced the nationalization of a controlling interest in all other petroleum companies operating in the country. This step gave Libya control of about 60 percent of its domestic oil production by early 1974, a figure that subsequently rose to 70 percent. Total nationalization was out of the question, given the need for foreign expertise and funds in oil exploration, production, and distribution.
Insisting on the continued use of petroleum as leverage against Israel and its supporters in the West, Libya strongly urged the Organization of Petroleum Exporting Countries (OPEC) to take action in 1973, and Libyan militancy was partially responsible for OPEC measures to raise oil prices, impose embargoes, and gain control of production. On 19 October 1973, Libya was the first Arab nation to issue an oil embargo against the United States after US President Richard Nixon announced the US would provide Israel with a $2.2 billion military aid program during the Yom Kippur War.[18] Saudi Arabia and other Arab oil producing nations in OPEC would follow suit the next day.[18]
While the other Arab nations lifted their oil embargoes on 18 March 1974,[18] the Gaddafi regime refused to do so.[citation needed] As a consequence of such policies, Libya's oil production declined by half between 1970 and 1974, while revenues from oil exports more than quadrupled. Production continued to fall, bottoming out at an eleven-year low in 1975 at a time when the government was preparing to invest large amounts of petroleum revenues in other sectors of the economy. Thereafter, output stabilized at about two million barrels per day. Production and hence income declined yet again in the early 1980s because of the high price of Libyan crude and because recession in the industrialized world reduced demand for oil from all sources.
Libya's Five-Year Economic and Social Transformation Plan (1976–80), announced in 1975, was programmed to pump US$20 billion into the development of a broad range of economic activities that would continue to provide income after Libya's petroleum reserves had been exhausted. Agriculture was slated to receive the largest share of aid in an effort to make Libya self-sufficient in food and to help keep the rural population on the land. Industry, of which there was little before the revolution, also received a significant amount of funding in the first development plan as well as in the second, launched in 1981.
The "remaking of Libyan society" contained in Gaddafi's ideological visions began to be put into practice formally in 1973, with a cultural revolution. This revolution was designed to create bureaucratic efficiency, public interest and participation in the subnational governmental system, and national political coordination. In an attempt to instill revolutionary fervor into his compatriots and to involve large numbers of them in political affairs, Gaddafi urged them to challenge traditional authority and to take over and run government organs themselves. The instrument for doing this was the people's committee. Within a few months, such committees were found all across Libya. They were functionally and geographically based, and eventually became responsible for local and regional administration.
People's committees were established in such widely divergent organizations as universities, private business firms, government bureaucracies, and the broadcast media. Geographically based committees were formed at the governorate, municipal, and zone (lowest) levels. Seats on the people's committees at the zone level were filled by direct popular election; members so elected could then be selected for service at higher levels. By mid-1973 estimates of the number of people's committees ranged above 2,000. In the scope of their administrative and regulatory tasks and the method of their members' selection, the people's committees purportedly embodied the concept of direct democracy that Gaddafi propounded in the first volume of The Green Book, which appeared in 1976. The same concept lay behind proposals to create a new political structure composed of "people's congresses." The centerpiece of the new system was the General People's Congress (GPC), a national representative body intended to replace the RCC.
During this transition, on 7 April 1976, students in universities in Tripoli and Benghazi protested against human rights violations and military control "over all aspects of life in Libya" and called for free and fair elections and for power to be transferred to a civilian government. Violent counter demonstrations took place and many students were imprisoned. On 7 April 1977 anniversary of the event, students including Omar Dabob and Muhammed Ben Saoud were publicly executed in Benghazi and anti-Gaddafi military officers were executed later in the week. Friends of the executees were forced to participate or observe the executions. Regular executions continued annually on 7 April until the late 1980s.[19]
On 21 July 1977, there were first gun battles between troops on the border, followed by land and air strikes. Relations between the Libyan and Egyptian governments had been deteriorating ever since the end of the Yom Kippur War from October 1973, due to Libyan opposition to President Anwar Sadat's peace policy as well as the breakdown of unification talks between the two governments. There is some proof that the Egyptian government was considering a war against Libya as early as 1974. On 28 February 1974, during Henry Kissinger's visit to Egypt, President Sadat told him about such intentions and requested that pressure be put on the Israeli government not to launch an attack on Egypt in the event of its forces being occupied in war with Libya.[20] In addition, the Egyptian government had broken its military ties with Moscow, while the Libyan government kept that cooperation going. The Egyptian government also gave assistance to former RCC members Major Abd al Munim al Huni and Omar Muhayshi, who unsuccessfully tried to overthrow Gaddafi in 1975, and allowed them to reside in Egypt. During 1976 relations were ebbing, as the Egyptian government claimed to have discovered a Libyan plot to overthrow the government in Cairo. On 26 January 1976, Egyptian Vice President Hosni Mubarak indicated in a talk with the US Ambassador Hermann Eilts that the Egyptian government intended to exploit internal problems in Libya to promote actions against Libya, but did not elaborate.[21] On 22 July 1976, the Libyan government made a public threat to break diplomatic relations with Cairo if Egyptian subversive actions continued.[22] On 8 August 1976, an explosion occurred in the bathroom of a government office in Tahrir Square in Cairo, injuring 14, and the Egyptian government and media claimed this was done by Libyan agents.[23] The Egyptian government also claimed to have arrested two Egyptian citizens trained by Libyan intelligence to perform sabotage within Egypt.[24] On 23 August, an Egyptian passenger plane was hijacked by persons who reportedly worked with Libyan intelligence. They were captured by Egyptian authorities in an operation that ended without any casualties. In retaliation for accusations by the Egyptian government of Libyan complicity in the hijacking, the Libyan government ordered the closure of the Egyptian Consulate in Benghazi.[25] On 24 July, the combatants agreed to a ceasefire under the mediation of the President of Algeria Houari Boumediène and the Palestine Liberation Organization leader Yasser Arafat.
On 2 March 1977, the General People's Congress (GPC), at Gaddafi's behest, adopted the "Declaration of the Establishment of the People's Authority"[30][31] and proclaimed the Socialist People's Libyan Arab Jamahiriya (Arabic: الجماهيرية العربية الليبية الشعبية الاشتراكية[32] al-Jamāhīrīyah al-'Arabīyah al-Lībīyah ash-Sha'bīyah al-Ishtirākīyah). In the official political philosophy of Gaddafi's state, the "Jamahiriya" system was unique to the country, although it was presented as the materialization of the Third International Theory, proposed by Gaddafi to be applied to the entire Third World. The GPC also created the General Secretariat of the GPC, comprising the remaining members of the defunct Revolutionary Command Council, with Gaddafi as general secretary, and also appointed the General People's Committee, which replaced the Council of Ministers, its members now called secretaries rather than ministers.
The Libyan government claimed that the Jamahiriya was a direct democracy without any political parties, governed by its populace through local popular councils and communes (named Basic People's Congresses). Official rhetoric disdained the idea of a nation state, tribal bonds remaining primary, even within the ranks of the national army.[33]

Jamahiriya (Arabic: جماهيرية jamāhīrīyah) is an Arabic term generally translated as "state of the masses"; Lisa Anderson[34] has suggested "peopledom" or "state of the masses" as a reasonable approximations of the meaning of the term as intended by Gaddafi. The term does not occur in this sense in Muammar Gaddafi's Green Book of 1975. The nisba-adjective jamāhīrīyah ("mass-, "of the masses") occurs only in the third part, published in 1981, in the phrase إن الحركات التاريخية هي الحركات الجماهيرية (Inna al-ḥarakāt at-tārīkhīyah hiya al-ḥarakāt al-jamāhīrīyah), translated in the English edition as "Historic movements are mass movements".
The word jamāhīrīyah was derived from jumhūrīyah, which is the usual Arabic translation of "republic". It was coined by changing the component jumhūr—"public"—to its plural form, jamāhīr—"the masses". Thus, it is similar to the term People's Republic. It is often left untranslated in English, with the long-form name thus rendered as Great Socialist People's Libyan Arab Jamahiriya. However, in Hebrew, for instance, jamāhīrīyah is translated as "קהילייה" (qehiliyáh), a word also used to translate the term "Commonwealth" when referring to the designation of a country.
After weathering the 1986 U.S. bombing by the Reagan administration, Gaddafi added the specifier "Great" (العظمى al-'Uẓmá) to the official name of the country.
The changes in Libyan leadership since 1976 culminated in March 1979, when the General People's Congress declared that the "vesting of power in the masses" and the "separation of the state from the revolution" were complete. The government was divided into two parts, the "Jamahiriya sector" and the "revolutionary sector". The "Jamahiriya sector" was composed of the General People's Congress, the General People's Committee, and the local Basic People's Congresses. Gaddafi relinquished his position as general secretary of the General People's Congress, as which he was succeeded by Abdul Ati al-Obeidi, who had been prime minister since 1977.
The "Jamahiriya sector" was overseen by the "revolutionary sector", headed by Gaddafi as "Leader of the Revolution" (Qā'id)A and the surviving members of the Revolutionary Command Council. The leaders of the revolutionary sector were not subject to election, as they owed office to their role in the 1969 coup. They oversaw the "revolutionary committees", which were nominally grass-roots organizations that helped keep the people engaged. As a result, although Gaddafi held no formal government office after 1979, he retained control of the government and the country.[citation needed] Gaddafi also remained supreme commander of the armed forces.
All legislative and executive authority was vested in the GPC. This body, however, delegated most of its important authority to its general secretary and General Secretariat and to the General People's Committee. Gaddafi, as general secretary of the GPC, remained the primary decision maker, just as he had been when chairman of the RCC. In turn, all adults had the right and duty to participate in the deliberation of their local Basic People's Congress (BPC), whose decisions were passed up to the GPC for consideration and implementation as national policy. The BPCs were in theory the repository of ultimate political authority and decision making, embodying what Gaddafi termed direct "people's power". The 1977 declaration and its accompanying resolutions amounted to a fundamental revision of the 1969 constitutional proclamation, especially with respect to the structure and organization of the government at both national and subnational levels.
Continuing to revamp Libya's political and administrative structure, Gaddafi introduced yet another element into the body politic. Beginning in 1977, "revolutionary committees" were organized and assigned the task of "absolute revolutionary supervision of people's power"; that is, they were to guide the people's committees, "raise the general level of political consciousness and devotion to revolutionary ideals". In reality, the revolutionary committees were used to survey the population and repress any political opposition to Gaddafi's autocratic rule. Reportedly 10% to 20% of Libyans worked in surveillance for these committees, a proportion of informants on par with Ba'athist Iraq and Juche Korea.[35]
Filled with politically astute zealots, the ubiquitous revolutionary committees in 1979 assumed control of BPC elections. Although they were not official government organs, the revolutionary committees became another mainstay of the domestic political scene. As with the people's committees and other administrative innovations since the revolution, the revolutionary committees fit the pattern of imposing a new element on the existing subnational system of government rather than eliminating or consolidating already existing structures. By the late 1970s, the result was an unnecessarily complex system of overlapping jurisdictions in which cooperation and coordination among different elements were compromised by ill-defined authority and responsibility. The ambiguity may have helped serve Gaddafi's aim to remain the prime mover behind Libyan governance, while minimizing his visibility at a time when internal opposition to political repression was rising.
The RCC was formally dissolved and the government was again reorganized into people's committees. A new General People's Committee (cabinet) was selected, each of its "secretaries" becoming head of a specialized people's committee; the exceptions were the "secretariats" of petroleum, foreign affairs, and heavy industry, where there were no people's committees. A proposal was also made to establish a "people's army" by substituting a national militia, being formed in the late 1970s, for the national army. Although the idea surfaced again in early 1982, it did not appear to be close to implementation.
Gaddafi also wanted to combat the strict social restrictions that had been imposed on women by the previous regime, establishing the Revolutionary Women's Formation to encourage reform. In 1970, a law was introduced affirming equality of the sexes and insisting on wage parity. In 1971, Gaddafi sponsored the creation of a Libyan General Women's Federation. In 1972, a law was passed criminalizing the marriage of any females under the age of sixteen and ensuring that a woman's consent was a necessary prerequisite for a marriage.[36]
Remaking of the economy was parallel with the attempt to remold political and social institutions. Until the late 1970s, Libya's economy was mixed, with a large role for private enterprise except in the fields of oil production and distribution, banking, and insurance. But according to volume two of Gaddafi's Green Book, which appeared in 1978, private retail trade, rent, and wages were forms of exploitation that should be abolished. Instead, workers' self-management committees and profit participation partnerships were to function in public and private enterprises.
A property law was passed that forbade ownership of more than one private dwelling, and Libyan workers took control of a large number of companies, turning them into state-run enterprises. Retail and wholesale trading operations were replaced by state-owned "people's supermarkets", where Libyans in theory could purchase whatever they needed at low prices. By 1981 the state had also restricted access to individual bank accounts to draw upon privately held funds for government projects. The measures created resentment and opposition among the newly dispossessed. The latter joined those already alienated, some of whom had begun to leave the country. By 1982, perhaps 50,000 to 100,000 Libyans had gone abroad; because many of the emigrants were among the enterprising and better educated Libyans, they represented a significant loss of managerial and technical expertise.
The government also built a trans-Sahara water pipeline from major aquifers to both a network of reservoirs and the towns of Tripoli, Sirte and Benghazi in 2006–2007.[37] It is part of the Great Manmade River project, started in 1984. It is pumping large resources of water from the Nubian Sandstone Aquifer System to both urban populations and new irrigation projects around the country.[38]
Libya continued to be plagued with a shortage of skilled labor, which had to be imported along with a broad range of consumer goods, both paid for with petroleum income. The country consistently ranked as the African nation with the highest HDI, standing at 0.755 in 2010, which was 0.041 higher than the next highest African HDI that same year.[39] Gender equality was a major achievement under Gaddafi's rule. According to Lisa Anderson, president of the American University in Cairo and an expert on Libya, said that under Gaddafi more women attended university and had "dramatically" more employment opportunities than most Arab nations.[40]
As early as 1969, Gaddafi waged a campaign against Chad. Scholar Gerard Prunier claims part of his hostility was apparently because Chadian President François Tombalbaye was Christian.[41] Libya was also involved in a sometimes violent territorial dispute with neighbouring Chad over the Aouzou Strip, which Libya occupied in 1973. This dispute eventually led to the Libyan invasion of Chad. The prolonged foray of Libyan troops into the Aozou Strip in northern Chad, was finally repulsed in 1987, when extensive US and French help to Chadian rebel forces and the government headed by former Defence Minister Hissein Habré finally led to a Chadian victory in the so-called Toyota War. The conflict ended in a ceasefire in 1987. After a judgement of the International Court of Justice on 13 February 1994, Libya withdrew troops from Chad the same year and the dispute was settled.[42]
In 1977, Gaddafi dispatched his military across the border to Egypt, but Egyptian forces fought back in the Libyan–Egyptian War. Both nations agreed to a ceasefire under the mediation of the President of Algeria Houari Boumediène.[43]
In 1972, Gaddafi created the Islamic Legion as a tool to unify and Arabize the region. The priority of the Legion was first Chad, and then Sudan. In Darfur, a western province of Sudan, Gaddafi supported the creation of the Arab Gathering (Tajammu al-Arabi), which according to Gérard Prunier was "a militantly racist and pan-Arabist organization which stressed the 'Arab' character of the province."[44] The two organizations shared members and a source of support, and the distinction between them is often ambiguous.
This Islamic Legion was mostly composed of immigrants from poorer Sahelian countries,[45] but also, according to a source, thousands of Pakistanis who had been recruited in 1981 with the false promise of civilian jobs once in Libya.[46] Generally speaking, the Legion's members were immigrants who had gone to Libya with no thought of fighting wars, and had been provided with inadequate military training and had sparse commitment. A French journalist, speaking of the Legion's forces in Chad, observed that they were "foreigners, Arabs or Africans, mercenaries in spite of themselves, wretches who had come to Libya hoping for a civilian job, but found themselves signed up more or less by force to go and fight in an unknown desert."[45]
At the beginning of the 1987 Libyan offensive in Chad, it maintained a force of 2,000 in Darfur. The nearly continuous cross-border raids that resulted greatly contributed to a separate ethnic conflict within Darfur that killed about 9,000 people between 1985 and 1988.[47]
Janjaweed, a group accused by the US of carrying out a genocide in Darfur in the 2000s, emerged in 1988 and some of its leaders are former legionnaires.[48]
In 1972, Gaddafi tried to buy a nuclear bomb from the People's Republic of China. He then tried to get a bomb from Pakistan, but Pakistan severed its ties before it succeeded in building a bomb.[49] In 1978, Gaddafi turned to Pakistan's rival, India, for help building its own nuclear bomb.[50] In July 1978, Libya and India signed a memorandum of understanding to cooperate in peaceful applications of nuclear energy as part of India's Atom of Peace policy.[50] In 1991, then Prime Minister Navaz Sharif paid a state visit to Libya to hold talks on the promotion of a Free Trade Agreement between Pakistan and Libya.[51] However, Gaddafi focused on demanding Pakistan's Prime Minister sell him a nuclear weapon, which surprised many of the Prime Minister's delegation members and journalists.[51] When Prime minister Sharif refused Gaddafi's demand, Gaddafi disrespected him, calling him a "Corrupt politician", a term which insulted and surprised Sharif.[51] The Prime minister cancelled the talks, returned to Pakistan and expelled the Libyan Ambassador from Pakistan.[51]
Thailand reported its citizens had helped build storage facilities for nerve gas.[52] Germany sentenced a businessman, Jurgen Hippenstiel-Imhausen, to five years in prison for involvement in Libyan chemical weapons.[49][53] Inspectors from the Chemical Weapons Convention (CWC) verified in 2004 that Libya owned a stockpile of 23 metric tons of mustard gas and more than 1,300 metric tons of precursor chemicals.[54]
When Libya was under pressure from international disputes, on 19 August 1981, a naval dogfight occurred over the Gulf of Sirte in the Mediterranean Sea. US F-14 Tomcat jets fired anti-aircraft missiles against a formation of Libyan fighter jets in this dogfight and shot down two Libyan Su-22 Fitter attack aircraft. This naval action was a result of claiming the territory and losses from the previous incident. A second dogfight occurred on 4 January 1989; US carrier-based jets also shot down two Libyan MiG-23 Flogger-Es in the same place.
A similar action occurred on 23 March 1986; while patrolling the Gulf, US naval forces attacked a sizable naval force and various SAM sites defending Libyan territory. US fighter jets and fighter-bombers destroyed SAM launching facilities and sank various naval vessels, killing 35 seamen. This was a reprisal for terrorist hijackings between June and December 1985.
On 5 April 1986, Libyan agents bombed "La Belle" nightclub in West Berlin, killing three and injuring 229. Gaddafi's plan was intercepted by several national intelligence agencies and more detailed information was retrieved four years later from Stasi archives. The Libyan agents who had carried out the operation, from the Libyan embassy in East Germany, were prosecuted by the reunited Germany in the 1990s.[55]
In response to the discotheque bombing, joint US Air Force, Navy and Marine Corps air-strikes took place against Libya on 15 April 1986 and code-named Operation El Dorado Canyon and known as the 1986 bombing of Libya. Air defenses, three army bases, and two airfields in Tripoli and Benghazi were bombed. The surgical strikes failed to kill Gaddafi but he lost a few dozen military officers. Gaddafi spread propaganda how it had killed his "adopted daughter" and how victims had been all "civilians". Despite the variations of the stories, the campaign was successful, and a large proportion of the Western press reported the government's stories as facts.[56]: 141 
Following the 1986 bombing of Libya, Gaddafi intensified his support for anti-American government organizations. He financed Jeff Fort's Al-Rukn faction of the Chicago Black P. Stones gang, in their emergence as an indigenous anti-American armed revolutionary movement.[57] Al-Rukn members were arrested in 1986 for preparing strikes on behalf of Libya, including blowing up US government buildings and bringing down an airplane; the Al-Rukn defendants were convicted in 1987 of "offering to commit bombings and assassinations on US soil for Libyan payment."[57] In 1986, Libyan state television announced that Libya was training suicide squads to attack American and European interests. He began financing the IRA again in 1986, to retaliate against the British for harboring American fighter planes.[58]
Gaddafi announced that he had won a spectacular military victory over the US and the country was officially renamed the "Great Socialist People's Libyan Arab Jamahiriyah".[56]: 183  However, his speech appeared devoid of passion and even the "victory" celebrations appeared unusual. Criticism of Gaddafi by ordinary Libyan citizens became more bold, such as defacing of Gaddafi posters.[56]: 183  The raids against Libyan military had brought the government to its weakest point in 17 years.[56]: 183 
Gaddafi was a close supporter of Ugandan President Idi Amin.[59]
Gaddafi sent thousands of troops to fight against Tanzania on behalf of Idi Amin. About 600 Libyan soldiers lost their lives attempting to defend the collapsing regime of Amin. After the fall of Kampala, Amin was eventually exiled from Uganda to Libya before settling in Saudi Arabia.[60]
Gaddafi also aided Jean-Bédel Bokassa, the Emperor of the Central African Empire.[60][56]: 16  He also intervened militarily in the restored Central African Republic during the 2001 coup attempt, to protect his ally Ange-Félix Patassé. Patassé signed a deal giving Libya a 99-year lease to exploit all of that country's natural resources, including uranium, copper, diamonds, and oil.[61]
Gaddafi supported Soviet protégé Mengistu Haile Mariam of Ethiopia.[56]: 16  He also supported the Somali rebel groups, SNM and SSDF in their fight to overthrow the dictatorship of Siad Barre.
Gaddafi was a strong opponent of apartheid in South Africa and forged a friendship with Nelson Mandela.[62] One of Mandela's grandsons is named Gaddafi, an indication of the latter's support in South Africa.[63] Gaddafi funded Mandela's 1994 election campaign, and after taking office as the country's first democratically elected president in 1994, Mandela rejected entreaties from U.S. President Bill Clinton and others to cut ties with Gaddafi.[63] Mandela later played a key role in helping Gaddafi gain mainstream acceptance in the Western world later in the 1990s.[63][64] Over the years, Gaddafi came to be seen as a hero in much of Africa due to his revolutionary image.[65]
Gaddafi was a strong supporter of Zimbabwean President Robert Mugabe.[66]
Gaddafi's World Revolutionary Center (WRC) near Benghazi became a training center for groups backed by Gaddafi.[61] Graduates in power as of 2011 include Blaise Compaoré of Burkina Faso and Idriss Déby of Chad.[67]
Gaddafi trained and supported Liberian warlord-president Charles Taylor, who was indicted by the Special Court for Sierra Leone for war crimes and crimes against humanity committed during the conflict in Sierra Leone.[68] Foday Sankoh, the founder of Revolutionary United Front, was also Gaddafi's graduate. According to Douglas Farah, "The amputation of the arms and legs of men, women, and children as part of a scorched-earth campaign was designed to take over the region's rich diamond fields and was backed by Gaddafi, who routinely reviewed their progress and supplied weapons".[67]
Gaddafi's strong military support and finances gained him allies across the continent. He had himself crowned with the title "King of Kings of Africa" in 2008, in the presence of over 200 African traditional rulers and kings, although his views on African political and military unification received a lukewarm response from their governments.[69][70][71] His 2009 forum for African kings was canceled by the Ugandan hosts, who believed that traditional rulers discussing politics would lead to instability.[72] On 1 February 2009, a 'coronation ceremony' in Addis Ababa, Ethiopia, was held to coincide with the 53rd African Union Summit, at which he was elected head of the African Union for the year.[73] Gaddafi told the assembled African leaders: "I shall continue to insist that our sovereign countries work to achieve the United States of Africa."[74]
In 1971 Gaddafi warned that if France opposes Libyan military occupation of Chad, he will use all weapons in the war against France including the "revolutionary weapon".[56]: 183  On 11 June 1972, Gaddafi announced that any Arab wishing to volunteer for Palestinian militant groups "can register his name at any Libyan embassy will be given adequate training for combat". He also promised financial support for attacks.[56]: 182  On 7 October 1972, Gaddafi praised the Lod Airport massacre, executed by the communist Japanese Red Army, and demanded Palestinian terrorist groups to carry out similar attacks.[56]: 182 
Reportedly, Gaddafi was a major financier of the "Black September Movement" which perpetrated the Munich massacre at the 1972 Summer Olympics.[75] In 1973 the Irish Naval Service intercepted the vessel Claudia in Irish territorial waters, which carried Soviet arms from Libya to the Provisional IRA.[76][77] In 1976 after a series of terror activities by the Provisional IRA, Gaddafi announced that "the bombs which are convulsing Britain and breaking its spirit are the bombs of Libyan people. We have sent them to the Irish revolutionaries so that the British will pay the price for their past deeds".[56]: 182 
In the Philippines, Libya backed the Moro Islamic Liberation Front, which continues to carry out acts of violence in an effort to establish a separatist Islamic state in the southern Philippines.[78] Libya has also supported the New People's Army[79] and Libyan agents were seen meeting with the Communist Party of the Philippines.[80] Islamist terrorist group Abu Sayyaf has also been suspected of receiving Libyan funding.[81]
Gaddafi also became a strong supporter of the Palestine Liberation Organization, which support ultimately harmed Libya's relations with Egypt, when in 1979 Egypt pursued a peace agreement with Israel. As Libya's relations with Egypt worsened, Gaddafi sought closer relations with the Soviet Union. Libya became the first country outside the Soviet bloc to receive the supersonic MiG-25 combat fighters, but Soviet-Libyan relations remained relatively distant. Gaddafi also sought to increase Libyan influence, especially in states with an Islamic population, by calling for the creation of a Saharan Islamic state and supporting anti-government forces in sub-Saharan Africa.
In the 1970s and the 1980s, this support was sometimes so freely given that even the most unsympathetic groups could obtain Libyan support; often the groups represented ideologies far removed from Gaddafi's own. Gaddafi's approach often tended to confuse international opinion.
In October 1981 Egypt's President Anwar Sadat was assassinated. Gaddafi applauded the murder and remarked that it was a "punishment".[82]
In December 1981, the US State Department invalidated US passports for travel to Libya, and in March 1982, the U.S. declared a ban on the import of Libyan oil.[83]
Gaddafi reportedly spent hundreds of millions of the government's money on training and arming Sandinistas in Nicaragua.[84] Daniel Ortega, the President of Nicaragua, was his ally.
In April 1984, Libyan refugees in London protested against execution of two dissidents. Communications intercepted by MI5 show that Tripoli ordered its diplomats to direct violence against the demonstrators. Libyan diplomats shot at 11 people and killed British policewoman Yvonne Fletcher. The incident led to the breaking off of diplomatic relations between the United Kingdom and Libya for over a decade.[85]
After December 1985 Rome and Vienna airport attacks, which killed 19 and wounded around 140, Gaddafi indicated that he would continue to support the Red Army Faction, the Red Brigades, and the Irish Republican Army as long as European countries support anti-Gaddafi Libyans.[86] The Foreign Minister of Libya also called the massacres "heroic acts".[87]
In 1986, Libyan state television announced that Libya was training suicide squads to attack American and European interests.[88]
On 5 April 1986, Libyan agents were alleged with bombing the "La Belle" nightclub in West Berlin, killing three people and injuring 229 people who were spending evening there. Gaddafi's plan was intercepted by Western intelligence. More-detailed information was retrieved years later when Stasi archives were investigated by the reunited Germany. Libyan agents who had carried out the operation from the Libyan embassy in East Germany were prosecuted by reunited Germany in the 1990s.[89]
In May 1987, Australia broke off relations with Libya because of its role in fueling violence in Oceania.[79][90][91]
Under Gaddafi, Libya had a long history of supporting the Irish Republican Army. In late 1987 French authorities stopped a merchant vessel, the MV Eksund, which was delivering a 150-ton Libyan arms shipment to the IRA.[92] In Britain, Gaddafi's best-known political subsidiary is the Workers Revolutionary Party.[91][93]
Gaddafi fueled a number of Islamist and communist groups in the Philippines, including the New People's Army of the Communist Party of the Philippines and the Moro Islamic Liberation Front.[35][78][79][81][86]
In Indonesia, the Free Aceh Movement was a Libyan-backed militant group.[94] Vanuatu's ruling party enjoyed Libyan support.[79]
In New Zealand, Libya attempted to radicalize Māoris.[79]
In Australia, there were several cases of attempted radicalisation of Australian Aborigines, with individuals receiving paramilitary training in Libya. Libya put several left-wing unions on the Libyan payroll, such as the Food Preservers Union (FPU) and the Federated Confectioners Association of Australia (FCA)[citation needed]. Labour Party politician Bill Hartley, the secretary of Libya-Australia friendship society, was long-term supporter of Gaddafi and Saddam Hussein.[79][90][91]
In the 1980s, the Libyan government purchased advertisements in Arabic-language newspapers in Australia asking for Australian Arabs to join the military units of his worldwide struggle against imperialism. In part, because of this, Australia banned recruitment of foreign mercenaries in Australia.[91]
Gaddafi developed a relationship with the Revolutionary Armed Forces of Colombia, becoming acquainted with its leaders in meetings of revolutionary groups regularly hosted in Libya.[61][67]
Some publications were financed by Gaddafi. The Socialist Labour League's Workers News was one such publication: "in among the routine denunciations of uranium mining and calls for greater trade union militancy would be a couple of pages extolling Gaddafi's fatuous and incoherent green book and the Libyan revolution."[91]
Gaddafi was a lifelong supporter of Kurdish independence. In 2011, Jawad Mella, the president of the Kurdistan National Congress referred to Gaddafi as the "only world leader who truly supports the Kurds".[95]
Libya was accused in the 1988 bombing of Pan Am Flight 103 over Lockerbie, Scotland; UN sanctions were imposed in 1992. UN Security Council resolutions (UNSCRs) passed in 1992 and 1993 obliged Libya to fulfill requirements related to the Pan Am 103 bombing before sanctions could be lifted, leading to Libya's political and economic isolation for most of the 1990s. The UN sanctions cut airline connections with the outer world, reduced diplomatic representation and prohibited the sale of military equipment. Oil-related sanctions were assessed by some as equally significant for their exceptions: thus sanctions froze Libya's foreign assets (but excluded revenue from oil and natural gas and agricultural commodities) and banned the sale to Libya of refinery or pipeline equipment (but excluded oil production equipment).
Under the sanctions Libya's refining capacity eroded. Libya's role on the international stage grew less provocative after UN sanctions were imposed. In 1999, Libya fulfilled one of the UNSCR requirements by surrendering two Libyans suspected in connection with the bombing for trial before a Scottish court in the Netherlands. One of these suspects, Abdel Basset al-Megrahi, was found guilty; the other was acquitted. UN sanctions against Libya were subsequently suspended. The full lifting of the sanctions, contingent on Libya's compliance with the remaining UNSCRs, including acceptance of responsibility for the actions of its officials and payment of appropriate compensation, was passed 12 September 2003, explicitly linked to the release of up to $2.7 billion in Libyan funds to the families of the 1988 attack's 270 victims.
In 2002, Gaddafi paid a ransom reportedly worth tens of millions of dollars to Abu Sayyaf, a Filipino Islamist militancy, to release a number of kidnapped tourists. He presented it as an act of goodwill to Western countries; nevertheless the money helped the group to expand its operation.[35]
In December 2003, Libya announced that it had agreed to reveal and end its programs to develop weapons of mass destruction and to renounce terrorism, and Gaddafi made significant strides in normalizing relations with western nations. He received various Western European leaders as well as many working-level and commercial delegations, and made his first trip to Western Europe in 15 years when he traveled to Brussels in April 2004. Libya responded in good faith to legal cases brought against it in U.S. courts for terrorist acts that predate its renunciation of violence. Claims for compensation in the Lockerbie bombing, LaBelle disco bombing, and UTA 772 bombing cases are ongoing. The U.S. rescinded Libya's designation as a state sponsor of terrorism in June 2006. In late 2007, Libya was elected by the General Assembly to a nonpermanent seat on the United Nations Security Council for the 2008–2009 term. In the intercession between normalization and the Libyan Civil War in 2011, Operation Enduring Freedom – Trans Sahara was fought in Libya's portion of the Sahara Desert. This involved usage of American military assets, such as C-130s in combination with Libyan military infrastructure, namely the Al-Watiya Air Base.[96]
In 1994, the General People's Congress approved the introduction of "purification laws" to be put into effect, punishing theft by the amputation of limbs, and fornication and adultery by flogging.[97] Under the Libyan constitution, homosexual relations are punishable by up to five years in jail.[98]
Throughout his long rule, Gaddafi had to defend his position against opposition and coup attempts, emerging both from the military and from the general population. He reacted to these threats on one hand by maintaining a careful balance of power between the forces in the country, and by brutal repression on the other. Gaddafi successfully balanced the various tribes of Libya one against the other by distributing his favours. To forestall a military coup, he deliberately weakened the Libyan Armed Forces by regularly rotating officers, relying instead on loyal elite troops such as his Revolutionary Guard Corps, the special-forces Khamis Brigade and his personal Amazonian Guard, even though emphasis on political loyalty tended, over the long run, to weaken the professionalism of his personal forces. This trend made the country vulnerable to dissension at a time of crisis, as happened during early 2011.
The term "Green Terror" is used to describe campaigns of violence and intimidation against opponents of Gaddafi, particularly in reference to wave of oppression during Libya's cultural revolution, or to the wave of highly publicized hangings of regime opponents that began with the Execution of Al-Sadek Hamed Al-Shuwehdy. Dissent was illegal under Law 75 of 1973.[35] Reportedly 10 to 20 percent of Libyans worked in surveillance for Gaddafi's Revolutionary Committees,[citation needed] a proportion of informants on par with Saddam Hussein's Iraq or Kim Jong Il's North Korea. The surveillance took place in government, in factories, and in the education sector.[35]
Following an abortive attempt to replace English foreign language education with Russian,[99] in recent years English has been taught in Libyan schools from a primary level, and students have access to English-language media.[100] However, one protester in 2011 described the situation as: "None of us can speak English or French. He kept us ignorant and blindfolded".[101][102]
According to the 2009 Freedom of the Press Index, Libya was the most censored country in the Middle East and North Africa.[103] Prisons were run with little or no documentation of inmate population, and often neglected even such basic data as a prisoner's crime and sentence.[35]
During the late 1970s, some exiled Libyans[who?] formed active opposition groups. In early 1979, Gaddafi warned opposition leaders to return home immediately or face "liquidation." When caught, they could face being sentenced and hanged in public.[104]
It is the Libyan people's responsibility to liquidate such scums who are distorting Libya's image abroad.Gaddafi employed his network of diplomats and recruits to assassinate dozens of his critics around the world. Amnesty International listed at least twenty-five assassinations between 1980 and 1987.[35][90]
Gaddafi's agents were active in the UK, where many Libyans had sought asylum. After Libyan diplomats shot at 15 anti-Gaddafi protesters from inside the Libyan embassy's first floor and killed a British policewoman, the UK broke off relations with Gaddafi's government as a result of the incident.
Even the U.S. could not protect dissidents from Libya. In 1980, a Libyan agent attempted to assassinate dissident Faisal Zagallai, a doctoral student at the University of Colorado at Boulder. The bullets left Zagallai partially blinded.[105] A defector was kidnapped and executed in 1990 just before he was about to receive U.S. citizenship.[35]
Gaddafi asserted in June 1984 that killings could be carried out even when the dissidents were on pilgrimage in the holy city of Mecca. In August 1984, one Libyan plot was thwarted in Mecca.[56]: 183 
As of 2004, Libya still provided bounties for heads of critics, including 1 million dollars for Ashur Shamis, a Libyan-British journalist.[106]
There is indication that between the years of 2002 and 2007, Libya's Gaddafi-era intelligence service had a partnership with western spy organizations including MI6 and the CIA, who voluntarily provided information on Libyan dissidents in the United States and Canada in exchange for using Libya as a base for extraordinary renditions. This was done despite Libya's history of murdering dissidents abroad, and with full knowledge of Libya's brutal mistreatment of detainees.[107][108][109]
In the 1990s, Gaddafi's rule was threatened by militant Islamism. In October 1993, there was an unsuccessful assassination attempt on Gaddafi by elements of the Libyan army. In response, Gaddafi used repressive measures, using his personal Revolutionary Guard Corps to crush riots and Islamist activism during the 1990s. Nevertheless, Cyrenaica between 1995 and 1998 was politically unstable, due to the tribal allegiances of the local troops.[110]
A renewed serious threat to the Libyan Arab Jamahiriya came in February 2011, with the Libyan Civil War. Inspiration for the unrest is attributed to the uprisings in Tunisia and Egypt, connecting it with the wider Arab Spring.[111] In the east, the National Transitional Council was established in Benghazi. The novelist Idris Al-Mesmari was arrested hours after giving an interview with Al Jazeera about the police reaction to protests in Benghazi on 15 February. 
Some Libyan officials had sided with the protesters and requested help from the international community to bring an end to the massacres of civilians. The government in Tripoli had lost control of half of Libya by the end of February,[112][113] but as of mid-September Gaddafi remained in control of several parts of Fezzan. On 21 September, the forces of NTC captured Sabha, the largest city of Fezzan, reducing the control of Gaddafi to limited and isolated areas.
Many nations condemned Gaddafi's government over its use of force against civilians. Several other nations allied with Gaddafi called the uprising and intervention a "plot" by Western powers to loot Libya's resources.[114] The United Nations Security Council passed a resolution to enforce a no-fly zone over Libyan airspace on 17 March 2011.[115]
The UN resolution authorised air-strikes against Libyan ground troops and warships that appeared to threaten civilians.[116] On 19 March, the no-fly zone enforcement began, with French aircraft undertaking sorties across Libya and a naval blockade by the British Royal Navy.[117] Eventually, the aircraft carriers USS Enterprise and Charles de Gaulle arrived off the coast and provided the enforcers with a rapid-response capability. U.S. forces named their part of the enforcement action Operation Odyssey Dawn, meant to "deny the Libyan regime from using force against its own people" [118] according to U.S. Vice Admiral William E. Gortney. More than 110 "Tomahawk" cruise missiles were fired in an initial assault by U.S. warships and a British submarine against Libyan air defences.[119]
The last government holdouts in Sirte finally fell to anti-Gaddafi fighters on 20 October 2011, and, following the controversial death of Muammar Gaddafi, Libya was officially declared "liberated" on 23 October 2011, ending 42 years of Gaddafi's leadership in Libya.[120]
Political scientist Riadh Sidaoui suggested in October 2011 that Gaddafi "has created a great void for his exercise of power: there is no institution, no army, no electoral tradition in the country", and as a result, the period of transition would be difficult in Libya.[121]
State flag (ratio: 1:2)
Unofficial state flag (ratio: 2:3)
Coat of arms
State flag (ratio: 2:3)
Unofficial state flag (ratio: 2:1)
Coat of arms
Naval Ensign
State flag (ratio: 1:2)
Unofficial state flag (ratio: 2:3)
Coat of arms
Naval ensign
Military aircraft roundel
Military aircraft fin flash
